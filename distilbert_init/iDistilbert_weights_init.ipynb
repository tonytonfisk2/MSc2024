{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c51b6c7-3f1e-40c2-9858-096d412800bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "import torch\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8245442a-5180-4bae-b1fe-156b6d70049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = load_from_disk('/mnt/tony/MSc2024/data/tokenized_preprocessed_wikitext103.hf') #20231101\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "val_dataset = tokenized_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf56d9e-2717-4142-85b8-de0c121c59de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 85965\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#comet_ml.init(project_name=\"distilbert_dotprod\")\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "num_samples = len(train_dataset)//10\n",
    "train_subset = train_dataset.select(range(num_samples))\n",
    "print(train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f30480-ea0f-403a-8111-8f79ea96df03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 13:11:24.283517: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-19 13:11:24.307890: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-19 13:11:24.307913: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-19 13:11:24.330230: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "class distillTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model = None, temperature = None, alpha_ce = None, **kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.teacher.eval()\n",
    "        self.layer_groups = [f\"transformer.layer.{i}\" for i in range(6)] \n",
    "        self.current_layer_group = 0\n",
    "        self.unfrozen_layers = set()\n",
    "        self.layer_logs = []\n",
    "        self.context_loss_stats = []\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs = False):\n",
    "        student_outputs = model(**inputs)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher(**inputs)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "\n",
    "        #student_contexts = torch.stack(student_outputs.contexts[:self.current_layer_group + 1])\n",
    "        #teacher_contexts = torch.stack(teacher_outputs.contexts[:self.current_layer_group + 1])\n",
    "        student_contexts = student_outputs.contexts[self.current_layer_group]\n",
    "        teacher_contexts = teacher_outputs.contexts[self.current_layer_group]\n",
    "        #l_ce = self.distillation_loss(student_logits, teacher_logits)  \n",
    "    \n",
    "        context_loss = F.mse_loss(student_contexts, teacher_contexts)\n",
    "\n",
    "        student_stats = {\n",
    "            'mean': student_contexts.mean().item(),\n",
    "            'max': student_contexts.max().item(),\n",
    "            'min': student_contexts.min().item()\n",
    "        }\n",
    "        teacher_stats = {\n",
    "            'mean': teacher_contexts.mean().item(),\n",
    "            'max': teacher_contexts.max().item(),\n",
    "            'min': teacher_contexts.min().item()\n",
    "        }\n",
    "\n",
    "        self.context_loss_stats.append({\n",
    "            'layer': self.current_layer_group,\n",
    "            'student': student_stats,\n",
    "            'teacher': teacher_stats,\n",
    "        })\n",
    "        \n",
    "        return (context_loss, student_outputs) if return_outputs else context_loss\n",
    "        \n",
    "    def train(self, resume_from_checkpoint=None, **kwargs):\n",
    "        layer_plots = []\n",
    "        for layer_group in self.layer_groups:\n",
    "            print(f\"Training layer group: {layer_group}\")\n",
    "            self.switch_to_next_layer_group()\n",
    "            print(self.get_num_trainable_parameters())\n",
    "            res = super().train(resume_from_checkpoint=resume_from_checkpoint, **kwargs)\n",
    "            self.layer_logs.append(self.state.log_history.copy())\n",
    "            self.current_layer_group += 1\n",
    "            #self.save_model(f\"./results/layer_{layer_group}\")\n",
    "        self.plot_layer_losses()\n",
    "        return res\n",
    "\n",
    "    def freeze_all_layers(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def switch_to_next_layer_group(self):\n",
    "        self.freeze_all_layers()\n",
    "        print(\"Current layer\", self.current_layer_group)\n",
    "        \n",
    "        if self.current_layer_group < len(self.layer_groups):\n",
    "            current_layer = self.layer_groups[self.current_layer_group]\n",
    "            newly_unfrozen_params = []\n",
    "            \n",
    "            for name, param in self.model.named_parameters():\n",
    "                if current_layer in name and any(qkv in name for qkv in ['sa_layer_norm']): #v_lin q_lin k_lin\n",
    "                    if name not in self.unfrozen_layers:\n",
    "                        param.requires_grad = True\n",
    "                        newly_unfrozen_params.append(param)\n",
    "                        \n",
    "            print(f\"Unfrozen parameters for layer {self.current_layer_group}:\")\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    print(f\"  - {name}\")\n",
    "\n",
    "            optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": newly_unfrozen_params,\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "            }\n",
    "            ]\n",
    "            \n",
    "            self.optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate)\n",
    "            \n",
    "            num_training_steps = len(self.train_dataset) // self.args.train_batch_size * self.args.num_train_epochs\n",
    "            warmup_rate = 0.1  # 10% \n",
    "            warmup_steps = int(num_training_steps * warmup_rate)\n",
    "            self.lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "                self.optimizer,\n",
    "                num_warmup_steps=warmup_steps, \n",
    "                num_training_steps=num_training_steps\n",
    "            )\n",
    "\n",
    "    def plot_layer_losses(self):\n",
    "        fig, axs = plt.subplots(2, 3, figsize=(20, 15))\n",
    "        for layer, (ax, layer_logs) in enumerate(zip(axs.flatten(), self.layer_logs)):\n",
    "            train_data = [(log['step'], log['loss']) for log in layer_logs if 'loss' in log]\n",
    "            eval_data = [(log['step'], log['eval_loss']) for log in layer_logs if 'eval_loss' in log]\n",
    "            \n",
    "            if train_data:\n",
    "                steps, losses = zip(*train_data)\n",
    "                ax.plot(steps, losses, label='Train Loss')\n",
    "            if eval_data:\n",
    "                steps, losses = zip(*eval_data)\n",
    "                ax.plot(steps, losses, label='Validation Loss')\n",
    "            \n",
    "            ax.set_title(f'Layer {layer} Loss')\n",
    "            ax.set_xlabel('Steps')\n",
    "            ax.set_ylabel('Loss')\n",
    "            ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('sa_layernorm.png')\n",
    "        plt.close()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f795d8f-164d-42eb-b25a-2b70de7b6106",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#labels = tokenized_datasets['train'].features['label'].names\n",
    "#num_labels = len(labels)\n",
    "#label2id, id2label = {}, {}\n",
    "\n",
    "#for idx, lbl in enumerate(labels):\n",
    "#    label2id[lbl] = idx\n",
    "#    id2label[idx] = lbl\n",
    "#print(label2id)\n",
    "#print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1700fea5-f8c0-48c3-9dea-af25c76927e8",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iDistilBertForMaskedLM(\n",
       "  (activation): GELUActivation()\n",
       "  (distilbert): iDistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): iTransformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x iTransformerBlock(\n",
       "          (attention): iMultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (vocab_projector): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  (mlm_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification, DistilBertConfig, DistilBertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from iDistilbert import iDistilBertForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "student_id = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(student_id)\n",
    "#Load Models\n",
    "\n",
    "teacher_id = \"distilbert/distilbert-base-uncased\"\n",
    "teacher_config = DistilBertConfig(    \n",
    "    distance_metric = \"cosine_distance\",\n",
    "    activation_function = \"softmax\",\n",
    "    signed_inhibitor =  False,\n",
    "    alpha = 0,\n",
    "    center = False,\n",
    "    output_contexts = True,\n",
    ")\n",
    "    \n",
    "teacher_model = iDistilBertForMaskedLM.from_pretrained(\n",
    "        teacher_id,\n",
    "        config=teacher_config,\n",
    "    )\n",
    "\n",
    "student_config = DistilBertConfig(\n",
    "    distance_metric = \"manhattan_distance\",\n",
    "    activation_function = \"relu\",\n",
    "    signed_inhibitor =  True,\n",
    "    alpha = 0,\n",
    "    center = True,\n",
    "    output_contexts = True,\n",
    "    )\n",
    "\n",
    "student_model = iDistilBertForMaskedLM(student_config)\n",
    "\n",
    "initialized_weights = torch.load('/mnt/tony/MSc2024/distilbert_init/models/qkv_inhibbert_layerwise.pth')\n",
    "student_model.load_state_dict(initialized_weights, strict=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b398599a-3e82-4dd3-8f1d-4b75d5956719",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-4\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    num_train_epochs = EPOCHS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    logging_dir = './logs',\n",
    "    load_best_model_at_end= True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps = 134, \n",
    "    logging_steps = 100,\n",
    "    save_steps=134,\n",
    "    save_total_limit=2,\n",
    "    seed = 42,\n",
    "    #report_to=['comet_ml', 'tensorboard'],\n",
    "    report_to=['tensorboard'],\n",
    "    warmup_ratio=0.05,\n",
    "    gradient_accumulation_steps=4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "trainer = distillTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    model=student_model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_subset,         \n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "115d2d2a-781a-4558-bfda-0dcbd7ad386a",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer group: transformer.layer.0\n",
      "Current layer 0\n",
      "Unfrozen parameters for layer 0:\n",
      "  - distilbert.transformer.layer.0.sa_layer_norm.weight\n",
      "  - distilbert.transformer.layer.0.sa_layer_norm.bias\n",
      "1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1343' max='1343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1343/1343 1:31:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.128100</td>\n",
       "      <td>1.125783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>1.111500</td>\n",
       "      <td>1.076816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>1.025900</td>\n",
       "      <td>1.003825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.968300</td>\n",
       "      <td>0.914770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.904900</td>\n",
       "      <td>0.829828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.756859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>0.742400</td>\n",
       "      <td>0.693611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>0.697200</td>\n",
       "      <td>0.637619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1206</td>\n",
       "      <td>0.623000</td>\n",
       "      <td>0.588621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.591900</td>\n",
       "      <td>0.545143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer group: transformer.layer.1\n",
      "Current layer 1\n",
      "Unfrozen parameters for layer 1:\n",
      "  - distilbert.transformer.layer.1.sa_layer_norm.weight\n",
      "  - distilbert.transformer.layer.1.sa_layer_norm.bias\n",
      "1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1343' max='1343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1343/1343 1:32:12, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.499100</td>\n",
       "      <td>0.479578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.487600</td>\n",
       "      <td>0.448053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>0.435300</td>\n",
       "      <td>0.402907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.399000</td>\n",
       "      <td>0.354078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.367500</td>\n",
       "      <td>0.313847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>0.284411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>0.299400</td>\n",
       "      <td>0.263147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.248562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1206</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.237139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.229037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer group: transformer.layer.2\n",
      "Current layer 2\n",
      "Unfrozen parameters for layer 2:\n",
      "  - distilbert.transformer.layer.2.sa_layer_norm.weight\n",
      "  - distilbert.transformer.layer.2.sa_layer_norm.bias\n",
      "1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1343' max='1343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1343/1343 1:32:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.451300</td>\n",
       "      <td>0.384777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.442300</td>\n",
       "      <td>0.358499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>0.398600</td>\n",
       "      <td>0.324732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>0.290578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.344900</td>\n",
       "      <td>0.265519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>0.250153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>0.297800</td>\n",
       "      <td>0.240065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>0.287300</td>\n",
       "      <td>0.234125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1206</td>\n",
       "      <td>0.279100</td>\n",
       "      <td>0.230669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.274600</td>\n",
       "      <td>0.227879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer group: transformer.layer.3\n",
      "Current layer 3\n",
      "Unfrozen parameters for layer 3:\n",
      "  - distilbert.transformer.layer.3.sa_layer_norm.weight\n",
      "  - distilbert.transformer.layer.3.sa_layer_norm.bias\n",
      "1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1343' max='1343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1343/1343 1:32:15, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.896500</td>\n",
       "      <td>0.880644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.882800</td>\n",
       "      <td>0.845246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.791371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.781900</td>\n",
       "      <td>0.734992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.737800</td>\n",
       "      <td>0.681875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>0.662300</td>\n",
       "      <td>0.640704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>0.636800</td>\n",
       "      <td>0.605380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>0.606600</td>\n",
       "      <td>0.574882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1206</td>\n",
       "      <td>0.561700</td>\n",
       "      <td>0.549938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.545900</td>\n",
       "      <td>0.528006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer group: transformer.layer.4\n",
      "Current layer 4\n",
      "Unfrozen parameters for layer 4:\n",
      "  - distilbert.transformer.layer.4.sa_layer_norm.weight\n",
      "  - distilbert.transformer.layer.4.sa_layer_norm.bias\n",
      "1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1343' max='1343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1343/1343 1:32:15, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.624900</td>\n",
       "      <td>0.611603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.610900</td>\n",
       "      <td>0.580167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>0.556400</td>\n",
       "      <td>0.533075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.520900</td>\n",
       "      <td>0.482560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.481900</td>\n",
       "      <td>0.438069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>0.420400</td>\n",
       "      <td>0.403719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>0.375459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>0.378200</td>\n",
       "      <td>0.355001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1206</td>\n",
       "      <td>0.347300</td>\n",
       "      <td>0.337386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.335500</td>\n",
       "      <td>0.322995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer group: transformer.layer.5\n",
      "Current layer 5\n",
      "Unfrozen parameters for layer 5:\n",
      "  - distilbert.transformer.layer.5.sa_layer_norm.weight\n",
      "  - distilbert.transformer.layer.5.sa_layer_norm.bias\n",
      "1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1343' max='1343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1343/1343 1:32:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.652700</td>\n",
       "      <td>0.575702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.635800</td>\n",
       "      <td>0.545682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>0.588100</td>\n",
       "      <td>0.507656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>0.549400</td>\n",
       "      <td>0.465155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>0.430650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>0.404166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>0.456000</td>\n",
       "      <td>0.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.367983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1206</td>\n",
       "      <td>0.409700</td>\n",
       "      <td>0.358137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.347814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1343, training_loss=0.5080961027166689, metrics={'train_runtime': 5537.3659, 'train_samples_per_second': 15.525, 'train_steps_per_second': 0.243, 'total_flos': 1.1393896488763392e+16, 'train_loss': 0.5080961027166689, 'epoch': 0.9998138842359948})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120bd01e-2dad-4e19-8488-0da8fd2c08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder = 'models/'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "torch.save(student_model.state_dict(), os.path.join(folder, 'layernorm_inhibit.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900413b1-4a20-46c7-a198-1b4a5fbf965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    output = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    print(\"Forward pass successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during forward pass: {e}\")\n",
    "\n",
    "print(output.contexts[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a4f5247-027c-45e0-8e0d-0eb591ee765f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def smooth_data(data, window_size):\n",
    "    \"\"\"Apply moving average smoothing to the data.\"\"\"\n",
    "    cumsum = np.cumsum(np.insert(data, 0, 0)) \n",
    "    return (cumsum[window_size:] - cumsum[:-window_size]) / window_size\n",
    "\n",
    "def organize_data_by_layer(data):\n",
    "    organized_data = {}\n",
    "    for entry in data:\n",
    "        layer = entry['layer']\n",
    "        if layer not in organized_data:\n",
    "            organized_data[layer] = {'student': {'mean': [], 'max': [], 'min': []},\n",
    "                                     'teacher': {'mean': [], 'max': [], 'min': []}}\n",
    "        for model in ['student', 'teacher']:\n",
    "            for stat in ['mean', 'max', 'min']:\n",
    "                organized_data[layer][model][stat].append(entry[model][stat])\n",
    "    return organized_data\n",
    "\n",
    "def plot_all_layers(data, smooth_window=100):\n",
    "    organized_data = organize_data_by_layer(data)\n",
    "    num_layers = len(organized_data)\n",
    "    \n",
    "    fig, axs = plt.subplots(num_layers, 3, figsize=(20, 8 * num_layers))\n",
    "    fig.suptitle('Layer Statistics Comparison (Smoothed)', fontsize=16)\n",
    "    \n",
    "    if num_layers == 1:\n",
    "        axs = axs.reshape(1, -1)\n",
    "    \n",
    "    colors = {'student': 'blue', 'teacher': 'red'}\n",
    "    stats = ['mean', 'max', 'min']\n",
    "    \n",
    "    for layer, (layer_num, layer_data) in enumerate(sorted(organized_data.items())):\n",
    "        for col, stat in enumerate(stats):\n",
    "            ax = axs[layer, col]\n",
    "            for model in ['student', 'teacher']:\n",
    "                original_data = layer_data[model][stat]\n",
    "                \n",
    "                # Plot original data with low alpha\n",
    "                ax.plot(original_data, color=colors[model], alpha=0.3, linewidth=1)\n",
    "                \n",
    "                # Smooth and plot the data\n",
    "                if len(original_data) > smooth_window:\n",
    "                    smoothed_data = smooth_data(original_data, smooth_window)\n",
    "                    ax.plot(range(smooth_window-1, len(original_data)), smoothed_data, \n",
    "                            color=colors[model], label=f'{model.capitalize()} (Smoothed)')\n",
    "                else:\n",
    "                    ax.plot(original_data, color=colors[model], label=model.capitalize())\n",
    "            \n",
    "            ax.set_title(f'Layer {layer_num} - {stat.capitalize()}', fontsize=12)\n",
    "            ax.set_xlabel('Steps', fontsize=10)\n",
    "            ax.set_ylabel('Value', fontsize=10)\n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_layers_stats_smoothed2.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee949909-b4a7-469d-bd3c-5cf1d5e62a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_layers(trainer.context_loss_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132937f0-fc18-43ea-94c7-d60b1d2b1204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
