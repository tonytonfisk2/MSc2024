{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517eb148-1fad-438b-a634-788bad7d6869",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-20 04:34:15.647354: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-20 04:34:15.671179: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-20 04:34:15.671202: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-20 04:34:15.686979: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef647f9d-1fac-471a-a0b6-415e9bfda542",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(student_id)\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "def pre_process(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation = True, max_length = 512)\n",
    "\n",
    "tokenized_data = dataset.map(pre_process, batched = True)\n",
    "\n",
    "labels = tokenized_data['train'].features['label'].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = {}, {}\n",
    "\n",
    "for idx, lbl in enumerate(labels):\n",
    "    label2id[lbl] = idx\n",
    "    id2label[idx] = lbl\n",
    "\n",
    "train_subset = tokenized_data[\"train\"].select(range(1000))\n",
    "val_subset = tokenized_data[\"test\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ab3194-b789-4a76-8c99-3018b3c08bc6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iDistilBertForSequenceClassification(\n",
       "  (distilbert): iDistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): iTransformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x iTransformerBlock(\n",
       "          (attention): iMultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification, DistilBertConfig, DataCollatorWithPadding\n",
    "from iDistilbert import iDistilBertForSequenceClassification\n",
    "student_config = DistilBertConfig(\n",
    "    output_hidden_states = False,\n",
    "    distance_metric = \"manhattan_distance\",\n",
    "    activation_function = \"relu\",\n",
    "    signed_inhibitor =  True,\n",
    "    alpha = 0,\n",
    "    center = False,\n",
    "    num_labels = num_labels,\n",
    "    label2id = label2id,\n",
    "    id2label = id2label,\n",
    "    )\n",
    "\n",
    "#student_model = iDistilBertForSequenceClassification.from_pretrained('/mnt/tony/MSc2024/results/checkpoint-2582', config = student_config)\n",
    "student_model = iDistilBertForSequenceClassification(\n",
    "        config=student_config,\n",
    "    )\n",
    "initialized_weights = torch.load('/mnt/tony/MSc2024/distilbert_init/models/distilbert_init.pth')\n",
    "student_model.load_state_dict(initialized_weights, strict=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69824ec2-c219-4baa-a9f4-d2da5610bc8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification, DistilBertConfig, DataCollatorWithPadding\n",
    "from iDistilbert import iDistilBertForSequenceClassification\n",
    "student_id = \"distilbert/distilbert-base-uncased\"\n",
    "student_config = DistilBertConfig(    \n",
    "    distance_metric = \"cosine_distance\",\n",
    "    activation_function = \"softmax\",\n",
    "    signed_inhibitor =  False,\n",
    "    alpha = 0,\n",
    "    center = False,\n",
    "    output_contexts = False,\n",
    ")\n",
    "    \n",
    "student_model = iDistilBertForSequenceClassification(\n",
    "        config=student_config,\n",
    "    )\n",
    "\n",
    "initialized_weights = torch.load('/mnt/tony/MSc2024/distilbert_init/models/q_k_distilbert_layerwise.pth')\n",
    "student_model.load_state_dict(initialized_weights, strict=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "student_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367e46ad-52f4-4fff-b7bc-ab41f4a5c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "#experiment = comet_ml.get_global_experiment()\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Preprocess the logits to ensure they are in the correct format for metric computation.\n",
    "    This function will be called during the evaluation process.\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple):  \n",
    "        logits = logits[0]  # get logit tensors\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    return pred_ids, labels\n",
    "    \n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    return accuracy.compute(predictions=predictions[0], references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640aa6d2-4bfa-43db-9d11-97e96b9ac5dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 4e-5\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "            \n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    num_train_epochs = EPOCHS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    logging_dir = './logs',\n",
    "    load_best_model_at_end= True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps = 390,\n",
    "    save_steps=390,\n",
    "    logging_steps = 20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    seed = 42,\n",
    "    #report_to=['comet_ml', 'tensorboard'],\n",
    "    report_to=['tensorboard'],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16 = True,\n",
    "    weight_decay = 0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=student_model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=tokenized_data['train'],         \n",
    "    eval_dataset=tokenized_data['test'],\n",
    "    compute_metrics = compute_metrics,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0804c1bf-c4d6-462c-b6a5-2d60b377f640",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1560' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1560/1560 2:53:00, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.404600</td>\n",
       "      <td>0.367775</td>\n",
       "      <td>0.837360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.301900</td>\n",
       "      <td>0.305658</td>\n",
       "      <td>0.874720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.220600</td>\n",
       "      <td>0.297910</td>\n",
       "      <td>0.880440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>0.315318</td>\n",
       "      <td>0.880680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1560, training_loss=0.32507384022076924, metrics={'train_runtime': 10386.978, 'train_samples_per_second': 9.627, 'train_steps_per_second': 0.15, 'total_flos': 1.308404299261008e+16, 'train_loss': 0.32507384022076924, 'epoch': 3.9923224568138194})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa10bbd-83d0-4449-8a18-718d43d61041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 02:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2468193769454956,\n",
       " 'eval_accuracy': 0.904,\n",
       " 'eval_runtime': 168.4103,\n",
       " 'eval_samples_per_second': 148.447,\n",
       " 'eval_steps_per_second': 9.281,\n",
       " 'epoch': 1.9961612284069097}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71911519-cb48-4f68-8a26-0e9b77a93a72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "iDistilBertForSequenceClassification                    [4, 512, 768]             --\n",
       "├─iDistilBertModel: 1-1                                 [4, 512, 768]             --\n",
       "│    └─Embeddings: 2-1                                  [4, 512, 768]             --\n",
       "│    │    └─Embedding: 3-1                              [4, 512, 768]             23,440,896\n",
       "│    │    └─Embedding: 3-2                              [1, 512, 768]             393,216\n",
       "│    │    └─LayerNorm: 3-3                              [4, 512, 768]             1,536\n",
       "│    │    └─Dropout: 3-4                                [4, 512, 768]             --\n",
       "│    └─iTransformer: 2-2                                [4, 512, 768]             --\n",
       "│    │    └─ModuleList: 3-5                             --                        42,527,232\n",
       "├─Linear: 1-2                                           [4, 768]                  590,592\n",
       "├─Dropout: 1-3                                          [4, 768]                  --\n",
       "├─Linear: 1-4                                           [4, 2]                    1,538\n",
       "=========================================================================================================\n",
       "Total params: 66,955,010\n",
       "Trainable params: 66,955,010\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 266.64\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 858.81\n",
       "Params size (MB): 267.82\n",
       "Estimated Total Size (MB): 1126.66\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "input_ids = torch.randint(0, 30522, (4, 512)).long().to(device)  # Assuming vocab size 30522\n",
    "\n",
    "# Attention mask (optional, but typically used)\n",
    "attention_mask = torch.ones((4, 512)).long().to(device)\n",
    "\n",
    "# Generate summary, note that input size should match what the model expects\n",
    "summary(student_model, input_data={'input_ids': input_ids, 'attention_mask': attention_mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee86655-ff2c-4a44-b926-0d32ef769bec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
