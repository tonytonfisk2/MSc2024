{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c51b6c7-3f1e-40c2-9858-096d412800bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8245442a-5180-4bae-b1fe-156b6d70049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "student_id = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(student_id)\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "def pre_process(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation = True, max_length = 512)\n",
    "\n",
    "tokenized_data = dataset.map(pre_process, batched = True)\n",
    "\n",
    "#test_valid = tokenized_data['test'].train_test_split(test_size=0.5)\n",
    "#tokenized_data = DatasetDict({\n",
    "#    'train': tokenized_data['train'],\n",
    "#    'test': test_valid['train'],\n",
    "#    'validation': test_valid['test']\n",
    "#})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf56d9e-2717-4142-85b8-de0c121c59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comet_ml.init(project_name=\"distilbert_dotprod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab886cc4-95e0-484b-94f7-19ca4579b5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 10:31:59.271676: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-04 10:31:59.294572: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-04 10:31:59.294592: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-04 10:31:59.309486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f30480-ea0f-403a-8111-8f79ea96df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class distillTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model = None, temperature = None, alpha_ce = None, alpha_cos = None, alpha_mlm = None, **kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.temperature = temperature\n",
    "        self.alpha_ce = alpha_ce\n",
    "        self.alpha_cos = alpha_cos\n",
    "        self.alpha_mlm = alpha_mlm\n",
    "        self.teacher.eval()\n",
    "        if self.alpha_cos > 0.0:\n",
    "            self.cosine_loss_fct = nn.CosineEmbeddingLoss(reduction=\"mean\")\n",
    "        self.layer_logs = []\n",
    "\n",
    "    def distillation_loss(self, student_logits, teacher_logits):\n",
    "        #soft target probabilities\n",
    "        soft_student = F.log_softmax(student_logits / self.temperature, dim = -1)\n",
    "        soft_teacher = F.softmax(teacher_logits / self.temperature, dim = -1)\n",
    "        #Kullback Leibler Divergence\n",
    "        loss = F.kl_div(soft_student, soft_teacher, reduction = 'batchmean') * (self.temperature**2) \n",
    "        return loss\n",
    "\n",
    "    def cosine_embedding_loss(self, student_outputs, teacher_outputs):\n",
    "        s_hidden_states = student_outputs.hidden_states[-1]\n",
    "        t_hidden_states = teacher_outputs.hidden_states[-1]\n",
    "        assert t_hidden_states.size() == s_hidden_states.size()\n",
    "        dim = s_hidden_states.size(-1)\n",
    "        s_hidden_states_slct = s_hidden_states.view(-1, dim)\n",
    "        t_hidden_states_slct = t_hidden_states.view(-1, dim)\n",
    "\n",
    "        target = s_hidden_states_slct.new(s_hidden_states_slct.size(0)).fill_(1) \n",
    "        loss = self.cosine_loss_fct(s_hidden_states_slct, t_hidden_states_slct, target)\n",
    "        return loss\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs = False):\n",
    "        #Distillation loss over soft target probabilities of teacher and student, KL DIV\n",
    "        #Cosine embedding loss\n",
    "        #supervised training loss\n",
    "        #Attention Score Alignment???\n",
    "        \n",
    "        student_outputs = model(**inputs)\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        student_loss = student_outputs.loss\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher(**inputs)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "            \n",
    "        l_ce = self.distillation_loss(student_logits, teacher_logits)\n",
    "        \n",
    "        l_cos = self.cosine_embedding_loss(student_outputs, teacher_outputs) if self.alpha_cos > 0 else 0\n",
    "\n",
    "        #Combine losses\n",
    "        loss = self.alpha_ce * l_ce + l_cos * self.alpha_cos + student_loss * self.alpha_mlm\n",
    "        \n",
    "        return (loss, student_outputs) if return_outputs else loss\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f795d8f-164d-42eb-b25a-2b70de7b6106",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0, 'pos': 1}\n",
      "{0: 'neg', 1: 'pos'}\n"
     ]
    }
   ],
   "source": [
    "labels = tokenized_data['train'].features['label'].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = {}, {}\n",
    "\n",
    "for idx, lbl in enumerate(labels):\n",
    "    label2id[lbl] = idx\n",
    "    id2label[idx] = lbl\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1700fea5-f8c0-48c3-9dea-af25c76927e8",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iDistilBertForSequenceClassification(\n",
       "  (distilbert): iDistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): iTransformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x iTransformerBlock(\n",
       "          (attention): iMultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification, DistilBertConfig, DataCollatorWithPadding\n",
    "from iDistilbert import iDistilBertForSequenceClassification\n",
    "\n",
    "#Load Models\n",
    "teacher_id = \"textattack/bert-base-uncased-imdb\"\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_id,\n",
    "    num_labels = num_labels,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "student_config = DistilBertConfig(\n",
    "    output_hidden_states = True,\n",
    "    distance_metric = \"manhattan_distance\",\n",
    "    activation_function = \"relu\",\n",
    "    signed_inhibitor =  True,\n",
    "    alpha = 0,\n",
    "    center = True,\n",
    "    num_labels = num_labels,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    "    )\n",
    "student_model = iDistilBertForSequenceClassification.from_pretrained('/mnt/tony/MSc2024/distilbert_init/models/distilbert_init.pth')\n",
    "\n",
    "#initialized_weights = torch.load('/mnt/tony/MSc2024/distilbert_init/models/distilbert_init.pth')\n",
    "#student_model.load_state_dict(initialized_weights, strict=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "teacher_model.to(device) \n",
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29fef96a-804b-410c-a873-b151b35062a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "#experiment = comet_ml.get_global_experiment()\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Preprocess the logits to ensure they are in the correct format for metric computation.\n",
    "    This function will be called during the evaluation process.\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple):  \n",
    "        logits = logits[0]  # get logit tensors\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    return pred_ids, labels\n",
    "    \n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    return accuracy.compute(predictions=predictions[0], references=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b398599a-3e82-4dd3-8f1d-4b75d5956719",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: COMET_MODE=ONLINE\n",
      "env: COMET_LOG_ASSETS=TRUE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%env COMET_MODE=ONLINE\n",
    "%env COMET_LOG_ASSETS=TRUE\n",
    "\n",
    "EPOCHS = 8\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    num_train_epochs = EPOCHS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    logging_dir = './logs',\n",
    "    load_best_model_at_end= True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps = 500,\n",
    "    save_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    seed = 42,\n",
    "    #report_to=['comet_ml', 'tensorboard'],\n",
    "    report_to=['tensorboard'],\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45972814-8066-4191-9584-d8d4ad02c08b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = distillTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    model=student_model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=tokenized_data['train'],         \n",
    "    eval_dataset=tokenized_data['test'],\n",
    "    compute_metrics = compute_metrics,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    temperature = 2,\n",
    "    alpha_ce = 5,\n",
    "    alpha_cos = 2,\n",
    "    alpha_mlm = 1,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "115d2d2a-781a-4558-bfda-0dcbd7ad386a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3120' max='3120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3120/3120 6:23:31, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>9.884000</td>\n",
       "      <td>6.866150</td>\n",
       "      <td>0.834160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.826900</td>\n",
       "      <td>5.877510</td>\n",
       "      <td>0.858200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.494700</td>\n",
       "      <td>5.850067</td>\n",
       "      <td>0.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.495700</td>\n",
       "      <td>6.097703</td>\n",
       "      <td>0.873160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.910900</td>\n",
       "      <td>6.360767</td>\n",
       "      <td>0.868880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.519200</td>\n",
       "      <td>6.605359</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3120, training_loss=4.762308600010016, metrics={'train_runtime': 23019.0509, 'train_samples_per_second': 8.688, 'train_steps_per_second': 0.136, 'total_flos': 2.617113894479856e+16, 'train_loss': 4.762308600010016, 'epoch': 7.984644913627639})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bd01e-2dad-4e19-8488-0da8fd2c08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fa1b8a0-6dc8-4069-87ce-b86773458566",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef143270-9bfe-4071-8c64-0e6a114055fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_trainer_loss(trainer):\n",
    "    # Extract the logged values\n",
    "    log_history = trainer.state.log_history\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_steps = []\n",
    "    val_steps = []\n",
    "    \n",
    "    for entry in log_history:\n",
    "        if 'loss' in entry:\n",
    "            train_loss.append(entry['loss'])\n",
    "            train_steps.append(entry['step'])\n",
    "        if 'eval_loss' in entry:\n",
    "            val_loss.append(entry['eval_loss'])\n",
    "            val_steps.append(entry['step'])\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.plot(train_steps, train_loss, label='Training Loss')\n",
    "    \n",
    "    # Plot validation loss\n",
    "    plt.plot(val_steps, val_loss, label='Validation Loss')\n",
    "    \n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_trainer_loss(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5ad514-2f20-4940-9b08-2ec9b916baa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [1 , 2 , 3 , 4 ,5 , 6]\n",
    "lst[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d41ff-7976-49a6-894b-0a5801ee3fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
