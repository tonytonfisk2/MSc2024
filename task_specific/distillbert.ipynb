{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c51b6c7-3f1e-40c2-9858-096d412800bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8245442a-5180-4bae-b1fe-156b6d70049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 12500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 12500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "student_id = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(student_id)\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "def pre_process(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation = True, max_length = 512)\n",
    "\n",
    "tokenized_data = dataset.map(pre_process, batched = True)\n",
    "\n",
    "test_valid = tokenized_data['test'].train_test_split(test_size=0.5)\n",
    "tokenized_data = DatasetDict({\n",
    "    'train': tokenized_data['train'],\n",
    "    'test': test_valid['train'],\n",
    "    'validation': test_valid['test']\n",
    "})\n",
    "\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cf56d9e-2717-4142-85b8-de0c121c59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_ml.init(project_name=\"distilbert_dotprod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab886cc4-95e0-484b-94f7-19ca4579b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87f30480-ea0f-403a-8111-8f79ea96df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class distillTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model = None, temperature = None, alpha_ce = None, alpha_cos = None, **kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.temperature = temperature\n",
    "        self.alpha_ce = alpha_ce\n",
    "        self.alpha_cos = alpha_cos\n",
    "        self.teacher.eval()\n",
    "\n",
    "    def distillation_loss(self, student_logits, teacher_logits):\n",
    "        #soft target probabilities\n",
    "        soft_student = F.log_softmax(student_logits / self.temperature, dim = -1)\n",
    "        soft_teacher = F.softmax(teacher_logits / self.temperature, dim = -1)\n",
    "        #Kullback Leibler Divergence\n",
    "        distill_loss = F.kl_div(soft_student, soft_teacher, reduction = 'batchmean') * (self.temperature**2) \n",
    "        return distill_loss\n",
    "\n",
    "    def cosine_embedding_loss(self, student_outputs, teacher_outputs):\n",
    "        #cosine embedding loss\n",
    "        teacher_hidden = torch.stack(teacher_outputs.hidden_states, dim = -1)\n",
    "        student_hidden = torch.stack(student_outputs.hidden_states, dim = -1)\n",
    "        assert student_hidden.size() == teacher_hidden.size(), \"Hidden State Size Dont Match\"\n",
    "        cosine_embedding_loss = torch.mean(1 - F.cosine_similarity(student_hidden, teacher_hidden, dim = -2))\n",
    "        return cosine_embedding_loss\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs = False):\n",
    "        #Distillation loss over soft target probabilities of teacher and student, KL DIV\n",
    "        #Cosine embedding loss\n",
    "        #supervised training loss\n",
    "        #Attention Score Alignment???\n",
    "        \n",
    "        student_outputs = model(**inputs)\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        student_loss = student_outputs.loss\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher(**inputs)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "            \n",
    "        l_ce = self.distillation_loss(student_logits, teacher_logits)\n",
    "        \n",
    "        l_cos = 0\n",
    "        if self.alpha_cos > 0:\n",
    "            l_cos += self.cosine_embedding_loss(student_outputs, teacher_outputs)\n",
    "\n",
    "        #Combine losses\n",
    "        loss = self.alpha_ce * l_ce + l_cos * self.alpha_cos + student_loss * (1 - (self.alpha_ce + self.alpha_cos)) \n",
    "        \n",
    "        return (loss, student_outputs) if return_outputs else loss\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f795d8f-164d-42eb-b25a-2b70de7b6106",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0, 'pos': 1}\n",
      "{0: 'neg', 1: 'pos'}\n"
     ]
    }
   ],
   "source": [
    "labels = tokenized_data['train'].features['label'].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = {}, {}\n",
    "\n",
    "for idx, lbl in enumerate(labels):\n",
    "    label2id[lbl] = idx\n",
    "    id2label[idx] = lbl\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1700fea5-f8c0-48c3-9dea-af25c76927e8",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification, DistilBertConfig, DataCollatorWithPadding\n",
    "\n",
    "#Load Models\n",
    "teacher_id = \"lvwerra/distilbert-imdb\"\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_id,\n",
    "    num_labels = num_labels,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "student_config = DistilBertConfig(output_hidden_states = True)\n",
    "student_model = DistilBertForSequenceClassification(student_config)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29fef96a-804b-410c-a873-b151b35062a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "experiment = comet_ml.get_global_experiment()\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Preprocess the logits to ensure they are in the correct format for metric computation.\n",
    "    This function will be called during the evaluation process.\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple):  \n",
    "        logits = logits[0]  # get logit tensors\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    return pred_ids, labels\n",
    "    \n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    return accuracy.compute(predictions=predictions[0], references=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b398599a-3e82-4dd3-8f1d-4b75d5956719",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: COMET_MODE=ONLINE\n",
      "env: COMET_LOG_ASSETS=TRUE\n"
     ]
    }
   ],
   "source": [
    "%env COMET_MODE=ONLINE\n",
    "%env COMET_LOG_ASSETS=TRUE\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 0.00002\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './task_specific/results',\n",
    "    num_train_epochs = EPOCHS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    logging_dir = './task_specific/logs',\n",
    "    load_best_model_at_end= True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps = 500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    report_to=['comet_ml', 'tensorboard'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45972814-8066-4191-9584-d8d4ad02c08b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = distillTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    model=student_model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=tokenized_data['train'],         \n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    compute_metrics = compute_metrics,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    temperature = 5,\n",
    "    alpha_ce = 0.25,\n",
    "    alpha_cos = 0.25,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "115d2d2a-781a-4558-bfda-0dcbd7ad386a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/tonytonfisk2/distilbert-dotprod/020618a9575a4505b402da6b3e35da3d\n",
      "\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3126' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3126/3126 43:04, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.870300</td>\n",
       "      <td>0.592735</td>\n",
       "      <td>0.813280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.551100</td>\n",
       "      <td>0.466201</td>\n",
       "      <td>0.855840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.482400</td>\n",
       "      <td>0.469772</td>\n",
       "      <td>0.855840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.406700</td>\n",
       "      <td>0.474402</td>\n",
       "      <td>0.868880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.365300</td>\n",
       "      <td>0.414717</td>\n",
       "      <td>0.873760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.366700</td>\n",
       "      <td>0.411120</td>\n",
       "      <td>0.876480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : hilarious_singularity_2458\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/tonytonfisk2/distilbert-dotprod/020618a9575a4505b402da6b3e35da3d\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epoch [13]                     : (0.3198976327575176, 2.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/accuracy [6]              : (0.81328, 0.87648)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/loss [6]                  : (0.4111196994781494, 0.5927345156669617)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/runtime [6]               : (189.7708, 190.1615)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/samples_per_second [6]    : (65.734, 65.869)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/steps_per_second [6]      : (4.112, 4.121)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_accuracy [6]              : (0.81328, 0.87648)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_loss [6]                  : (0.4111196994781494, 0.5927345156669617)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_runtime [6]               : (189.7708, 190.1615)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_samples_per_second [6]    : (65.734, 65.869)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_steps_per_second [6]      : (4.112, 4.121)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     grad_norm [6]                  : (6.573944091796875, 21.461502075195312)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate [6]              : (8.061420345489445e-07, 1.6801023672424827e-05)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [318]                     : (0.18809039890766144, 1.3588929176330566)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     total_flos                     : 6556904415524352.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/epoch [13]               : (0.3198976327575176, 2.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/grad_norm [6]            : (6.573944091796875, 21.461502075195312)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/learning_rate [6]        : (8.061420345489445e-07, 1.6801023672424827e-05)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/loss [6]                 : (0.3653, 0.8703)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/total_flos               : 6556904415524352.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_loss               : 0.5011479358831736\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_runtime            : 2513.4656\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_samples_per_second : 19.893\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_steps_per_second   : 1.244\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss                     : 0.5011479358831736\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_runtime                  : 2513.4656\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_samples_per_second       : 19.893\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_steps_per_second         : 1.244\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_n_gpu                                  : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_no_sync_in_gradient_accumulation       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_setup_devices                          : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/accelerator_config                      : AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adafactor                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta1                              : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta2                              : 0.999\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_epsilon                            : 1e-08\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/auto_find_batch_size                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/batch_eval_metrics                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16_full_eval                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/data_seed                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_drop_last                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_num_workers                  : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_persistent_workers           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_pin_memory                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_prefetch_factor              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_backend                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_broadcast_buffers                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_bucket_cap_mb                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_find_unused_parameters              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout                             : 1800\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout_delta                       : 0:30:00\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/debug                                   : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed_plugin                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/default_optim                           : adamw_torch\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/device                                  : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/disable_tqdm                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dispatch_batches                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/distributed_state                       : Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_eval                                 : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_predict                              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_train                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_accumulation_steps                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_batch_size                         : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_delay                              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_do_concat_batches                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_steps                              : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_strategy                           : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/evaluation_strategy                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_backend                            : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_full_eval                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_opt_level                          : O1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/framework                               : pt\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp                                    : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_config                             : {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_min_num_params                     : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_transformer_layer_cls_to_wrap      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/full_determinism                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_accumulation_steps             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_checkpointing                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_checkpointing_kwargs           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/greater_is_better                       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/group_by_length                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/half_precision_backend                  : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_always_push                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_model_id                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_private_repo                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_strategy                            : HubStrategy.EVERY_SAVE\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_token                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ignore_data_skip                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_inputs_for_metrics              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_num_input_tokens_seen           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_tokens_per_second               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/jit_mode_eval                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_names                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_smoothing_factor                  : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/learning_rate                           : 2e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/length_column_name                      : length\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/load_best_model_at_end                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_process_index                     : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_rank                              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level                               : passive\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level_replica                       : warning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_on_each_node                        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_dir                             : ./task_specific/logs\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_first_step                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_nan_inf_filter                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_steps                           : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_strategy                        : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/lr_scheduler_kwargs                     : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/lr_scheduler_type                       : SchedulerType.LINEAR\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_grad_norm                           : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_steps                               : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/metric_for_best_model                   : accuracy\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/mp_parameters                           : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/n_gpu                                   : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/neftune_noise_alpha                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/no_cuda                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/num_train_epochs                        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim                                   : OptimizerNames.ADAMW_TORCH\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim_args                              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim_target_modules                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/output_dir                              : ./task_specific/results\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/overwrite_output_dir                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/parallel_mode                           : ParallelMode.NOT_DISTRIBUTED\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/past_index                              : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_eval_batch_size              : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_train_batch_size             : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_eval_batch_size                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_train_batch_size                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/place_model_on_device                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/prediction_loss_only                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/process_index                           : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_model_id                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_organization                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_token                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ray_scope                               : last\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/remove_unused_columns                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/report_to                               : ['comet_ml', 'tensorboard']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/restore_callback_states_from_checkpoint : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/resume_from_checkpoint                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/run_name                                : ./task_specific/results\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_on_each_node                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_only_model                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_safetensors                        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_steps                              : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_strategy                           : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_total_limit                        : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/seed                                    : 42\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_log                              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_save                             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/skip_memory_metrics                     : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/split_batches                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tf32                                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_backend                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_mode                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torchdynamo                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_metrics_debug                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_num_cores                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/train_batch_size                        : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_cpu                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_ipex                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_legacy_prediction_loop              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_mps_device                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_ratio                            : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_steps                            : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/weight_decay                            : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/world_size                              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_attn_implementation                  : eager\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_attn_implementation_internal         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_auto_class                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_commit_hash                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_name_or_path                         : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/activation                            : gelu\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/add_cross_attention                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/architectures                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attention_dropout                     : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attribute_map                         : {'hidden_size': 'dim', 'num_attention_heads': 'n_heads', 'num_hidden_layers': 'n_layers'}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bad_words_ids                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/begin_suppress_tokens                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bos_token_id                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/chunk_size_feed_forward               : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/cross_attention_hidden_size           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/decoder_start_token_id                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/dim                                   : 768\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/diversity_penalty                     : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/do_sample                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/dropout                               : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/early_stopping                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/encoder_no_repeat_ngram_size          : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/eos_token_id                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/exponential_decay_length_penalty      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/finetuning_task                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_bos_token_id                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_eos_token_id                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/hidden_dim                            : 3072\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/id2label                              : {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/initializer_range                     : 0.02\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_composition                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_decoder                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_encoder_decoder                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/label2id                              : {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/length_penalty                        : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_length                            : 20\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_position_embeddings               : 512\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/min_length                            : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/model_type                            : distilbert\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/n_heads                               : 12\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/n_layers                              : 6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/name_or_path                          : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/no_repeat_ngram_size                  : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beam_groups                       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beams                             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_labels                            : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_return_sequences                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_attentions                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_hidden_states                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_scores                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pad_token_id                          : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/prefix                                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/problem_type                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pruned_heads                          : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/qa_dropout                            : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/remove_invalid_values                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/repetition_penalty                    : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict                           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict_in_generate               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/sep_token_id                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/seq_classif_dropout                   : 0.2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/sinusoidal_pos_embds                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/suppress_tokens                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/task_specific_params                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/temperature                           : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tf_legacy_loss                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_encoder_decoder                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_word_embeddings                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tokenizer_class                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_k                                 : 50\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_p                                 : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torch_dtype                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torchscript                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/transformers_version                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/typical_p                             : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_bfloat16                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_return_dict                       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/vocab_size                            : 30522\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset                    : 22 (1.50 GB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (5.97 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 324 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 273 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 272 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 2 file(s), remaining 271.82 MB/1021.82 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3126, training_loss=0.5011479358831736, metrics={'train_runtime': 2513.4656, 'train_samples_per_second': 19.893, 'train_steps_per_second': 1.244, 'total_flos': 6556904415524352.0, 'train_loss': 0.5011479358831736, 'epoch': 2.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a480372-5003-4754-9fb4-b516ff1d875c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(tokenized_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901366a5-f598-4ad2-8056-b94ec982e2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
