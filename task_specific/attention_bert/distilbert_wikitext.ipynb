{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6decd693-8376-4211-bf0c-485a68841179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-09 01:58:22.738701: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-09 01:58:22.760921: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-09 01:58:22.760946: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-09 01:58:22.776712: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import os\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6afd25-9436-4f5e-8ff8-221a1a600905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1801350/1801350 [07:37<00:00, 3933.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_and_tokenize(examples):\n",
    "    processed_texts = []\n",
    "    for text in examples['text']:\n",
    "        # Preprocess text\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        \n",
    "        text = text.strip()\n",
    "        \n",
    "        text = re.sub(r'@.@', '-', text)\n",
    "        processed_texts.append(text)\n",
    "        \n",
    "    # Tokenize the processed texts\n",
    "    tokenized = tokenizer(\n",
    "        processed_texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "# Load the tokenizer\n",
    "student_id = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(student_id)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\")\n",
    "\n",
    "# Apply preprocessing and tokenization\n",
    "tokenized_data = dataset.map(\n",
    "    preprocess_and_tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc10ac8-d8e2-4f17-924b-0f996b1d460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class distillTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model = None, temperature = None, alpha_ce = None, alpha_cos = None, **kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.temperature = temperature\n",
    "        self.alpha_ce = alpha_ce\n",
    "        self.alpha_cos = alpha_cos\n",
    "        self.teacher.eval()\n",
    "        self.ce_loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        if self.alpha_cos > 0.0:\n",
    "            self.cosine_loss_fct = nn.CosineEmbeddingLoss(reduction=\"mean\")\n",
    "\n",
    "    def distillation_loss(self, student_outputs, teacher_outputs, attention_mask):\n",
    "        #soft target probabilities\n",
    "        s_logits = student_outputs.logits  # (bs, seq_length, voc_size)\n",
    "        t_logits = teacher_outputs.logits  # (bs, seq_length, voc_size)\n",
    "\n",
    "        attention_mask = attention_mask.bool()\n",
    "        mask = attention_mask.unsqueeze(-1).expand_as(s_logits)  # (bs, seq_length, voc_size)\n",
    "        \n",
    "        s_logits_slct = torch.masked_select(s_logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
    "        s_logits_slct = s_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
    "        t_logits_slct = torch.masked_select(t_logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
    "        t_logits_slct = t_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
    "        assert t_logits_slct.size() == s_logits_slct.size()\n",
    "        \n",
    "        soft_student = F.log_softmax(s_logits_slct / self.temperature, dim = -1)\n",
    "        soft_teacher = F.softmax(t_logits_slct / self.temperature, dim = -1)\n",
    "        #Kullback Leibler Divergence\n",
    "        distill_loss = self.ce_loss_fct(soft_student, soft_teacher, reduction = 'batchmean') * (self.temperature**2) \n",
    "        return distill_loss\n",
    "\n",
    "    def cosine_embedding_loss(self, student_outputs, teacher_outputs, attention_mask):\n",
    "        #cosine embedding loss\n",
    "        s_hidden_states = student_outputs.hidden_states[-1]  # (bs, seq_length, dim)\n",
    "        t_hidden_states = teacher_outputs.hidden_states[-1]  # (bs, seq_length, dim)\n",
    "        \n",
    "        attention_mask = attention_mask.bool()\n",
    "        mask = attention_mask.unsqueeze(-1).expand_as(s_hidden_states)  # (bs, seq_length, dim)\n",
    "        assert s_hidden_states.size() == t_hidden_states.size()\n",
    "        dim = s_hidden_states.size(-1)\n",
    "\n",
    "        s_hidden_states_slct = torch.masked_select(s_hidden_states, mask)  # (bs * seq_length * dim)\n",
    "        s_hidden_states_slct = s_hidden_states_slct.view(-1, dim)  # (bs * seq_length, dim)\n",
    "        t_hidden_states_slct = torch.masked_select(t_hidden_states, mask)  # (bs * seq_length * dim)\n",
    "        t_hidden_states_slct = t_hidden_states_slct.view(-1, dim)  # (bs * seq_length, dim)\n",
    "\n",
    "        target = s_hidden_states_slct.new(s_hidden_states_slct.size(0)).fill_(1)  # (bs * seq_length,)\n",
    "        loss_cos = self.cosine_loss_fct(s_hidden_states_slct, t_hidden_states_slct, target)\n",
    "        return loss_cos\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs = False):\n",
    "        #Distillation loss over soft target probabilities of teacher and student, KL DIV\n",
    "        #Cosine embedding loss\n",
    "        #supervised training loss\n",
    "        #Attention Score Alignment???\n",
    "        \n",
    "        student_outputs = model(**inputs)\n",
    "        student_loss = student_outputs.loss\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher(**inputs)\n",
    "            \n",
    "        l_ce = self.distillation_loss(student_outputs, teacher_outputs, inputs['attention_mask'])\n",
    "        \n",
    "        l_cos = 0\n",
    "        if self.alpha_cos > 0:\n",
    "            l_cos += self.cosine_embedding_loss(student_outputs, teacher_outputs, inputs['attention_mask'])\n",
    "\n",
    "        #Combine losses\n",
    "        loss = self.alpha_ce * l_ce + l_cos * self.alpha_cos + student_loss * (1 - (self.alpha_ce + self.alpha_cos)) \n",
    "        \n",
    "        return (loss, student_outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "677f9b7a-ca92-4f9d-83ce-d18d4eec968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForMaskedLM were not initialized from the model checkpoint at lvwerra/distilbert-imdb and are newly initialized: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForMaskedLM(\n",
       "  (activation): GELUActivation()\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (vocab_projector): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  (mlm_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification, DistilBertConfig, DataCollatorWithPadding, BertForMaskedLM, DistilBertForMaskedLM \n",
    "\n",
    "#Load Models\n",
    "teacher_id = \"google-bert/bert-base-uncased\"\n",
    "teacher_model = DistilBertForMaskedLM.from_pretrained(\n",
    "    teacher_id,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "student_config = DistilBertConfig(output_hidden_states = True)\n",
    "student_model = DistilBertForMaskedLM(student_config)\n",
    "initialized_weights = torch.load('/mnt/tony/MSc2024/distilbert_init/distilbert_init.pth')\n",
    "student_model.load_state_dict(initialized_weights, strict=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11cc5da4-bea1-4d92-bf20-9b18188edbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Preprocess the logits to ensure they are in the correct format for metric computation.\n",
    "    This function will be called during the evaluation process.\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple):  \n",
    "        logits = logits[0]  # get logit tensors\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    return pred_ids, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    # Flatten the arrays if they're multi-dimensional\n",
    "    predictions = predictions[0].flatten()\n",
    "    labels = labels.flatten()\n",
    "\n",
    "    # Compute MLM accuracy only on masked tokens\n",
    "    masked_tokens = labels != -100\n",
    "    mlm_accuracy = accuracy_score(labels[masked_tokens], predictions[masked_tokens])\n",
    "    \n",
    "    # Compute overall accuracy, precision, recall, and F1\n",
    "    # Ignore padding tokens (-100)\n",
    "    valid_tokens = labels != -100\n",
    "    accuracy = accuracy_score(labels[valid_tokens], predictions[valid_tokens])\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels[valid_tokens], \n",
    "        predictions[valid_tokens], \n",
    "        average='weighted',\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    return { \n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e682e48-62e7-44a2-b13a-8f67a3047fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    num_train_epochs = EPOCHS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    logging_dir = './logs',\n",
    "    load_best_model_at_end= True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps = 500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    #report_to=['comet_ml', 'tensorboard'],\n",
    "    report_to=['tensorboard'],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "trainer = distillTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    model=student_model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=tokenized_data['train'],         \n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    temperature = 5,\n",
    "    alpha_ce = 0.3,\n",
    "    alpha_cos = 0.2,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "860b70ee-9891-4c9e-8307-b7537c7e96e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='291258' max='291258' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [291258/291258 54:07:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mlm Accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>63.982700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.067072</td>\n",
       "      <td>0.067072</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>0.067072</td>\n",
       "      <td>0.009636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>56.046900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.074271</td>\n",
       "      <td>0.074271</td>\n",
       "      <td>0.019811</td>\n",
       "      <td>0.074271</td>\n",
       "      <td>0.017891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>54.997600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.073492</td>\n",
       "      <td>0.073492</td>\n",
       "      <td>0.019056</td>\n",
       "      <td>0.073492</td>\n",
       "      <td>0.019154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>54.508900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.077844</td>\n",
       "      <td>0.077844</td>\n",
       "      <td>0.016811</td>\n",
       "      <td>0.077844</td>\n",
       "      <td>0.020753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>53.857000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.073212</td>\n",
       "      <td>0.073212</td>\n",
       "      <td>0.017591</td>\n",
       "      <td>0.073212</td>\n",
       "      <td>0.021553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>53.098200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.076129</td>\n",
       "      <td>0.076129</td>\n",
       "      <td>0.029193</td>\n",
       "      <td>0.076129</td>\n",
       "      <td>0.026445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>53.322600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.077201</td>\n",
       "      <td>0.077201</td>\n",
       "      <td>0.057508</td>\n",
       "      <td>0.077201</td>\n",
       "      <td>0.029157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>52.445400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.079999</td>\n",
       "      <td>0.079999</td>\n",
       "      <td>0.075715</td>\n",
       "      <td>0.079999</td>\n",
       "      <td>0.034903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>51.711200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.084324</td>\n",
       "      <td>0.084324</td>\n",
       "      <td>0.064318</td>\n",
       "      <td>0.084324</td>\n",
       "      <td>0.040335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>50.922200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.083149</td>\n",
       "      <td>0.083149</td>\n",
       "      <td>0.049324</td>\n",
       "      <td>0.083149</td>\n",
       "      <td>0.038476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>50.421200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.082436</td>\n",
       "      <td>0.082436</td>\n",
       "      <td>0.047046</td>\n",
       "      <td>0.082436</td>\n",
       "      <td>0.037843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>49.982600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.085041</td>\n",
       "      <td>0.085041</td>\n",
       "      <td>0.036020</td>\n",
       "      <td>0.085041</td>\n",
       "      <td>0.037375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>49.580500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.085686</td>\n",
       "      <td>0.085686</td>\n",
       "      <td>0.061450</td>\n",
       "      <td>0.085686</td>\n",
       "      <td>0.040314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>49.941400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.085243</td>\n",
       "      <td>0.085243</td>\n",
       "      <td>0.052134</td>\n",
       "      <td>0.085243</td>\n",
       "      <td>0.040416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>49.254300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.085013</td>\n",
       "      <td>0.085013</td>\n",
       "      <td>0.046050</td>\n",
       "      <td>0.085013</td>\n",
       "      <td>0.039935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>48.892900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.085196</td>\n",
       "      <td>0.085196</td>\n",
       "      <td>0.058564</td>\n",
       "      <td>0.085196</td>\n",
       "      <td>0.040282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>48.747700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.081386</td>\n",
       "      <td>0.081386</td>\n",
       "      <td>0.048092</td>\n",
       "      <td>0.081386</td>\n",
       "      <td>0.037016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>48.224400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.085050</td>\n",
       "      <td>0.085050</td>\n",
       "      <td>0.031443</td>\n",
       "      <td>0.085050</td>\n",
       "      <td>0.037578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>48.175300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.085008</td>\n",
       "      <td>0.085008</td>\n",
       "      <td>0.046901</td>\n",
       "      <td>0.085008</td>\n",
       "      <td>0.038739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>48.129700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.082907</td>\n",
       "      <td>0.082907</td>\n",
       "      <td>0.050060</td>\n",
       "      <td>0.082907</td>\n",
       "      <td>0.037704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>47.692400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.087886</td>\n",
       "      <td>0.087886</td>\n",
       "      <td>0.055608</td>\n",
       "      <td>0.087886</td>\n",
       "      <td>0.040485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>47.318400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.082879</td>\n",
       "      <td>0.082879</td>\n",
       "      <td>0.042769</td>\n",
       "      <td>0.082879</td>\n",
       "      <td>0.037275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>47.712900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.087024</td>\n",
       "      <td>0.087024</td>\n",
       "      <td>0.045477</td>\n",
       "      <td>0.087024</td>\n",
       "      <td>0.041438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>47.134400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.084763</td>\n",
       "      <td>0.084763</td>\n",
       "      <td>0.049753</td>\n",
       "      <td>0.084763</td>\n",
       "      <td>0.038202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>46.918500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.086864</td>\n",
       "      <td>0.086864</td>\n",
       "      <td>0.052355</td>\n",
       "      <td>0.086864</td>\n",
       "      <td>0.039883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>47.086600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.083399</td>\n",
       "      <td>0.083399</td>\n",
       "      <td>0.039844</td>\n",
       "      <td>0.083399</td>\n",
       "      <td>0.038316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>46.336800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.085322</td>\n",
       "      <td>0.085322</td>\n",
       "      <td>0.043427</td>\n",
       "      <td>0.085322</td>\n",
       "      <td>0.037591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>46.823900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.082543</td>\n",
       "      <td>0.082543</td>\n",
       "      <td>0.036234</td>\n",
       "      <td>0.082543</td>\n",
       "      <td>0.037662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>46.480700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.083937</td>\n",
       "      <td>0.083937</td>\n",
       "      <td>0.036260</td>\n",
       "      <td>0.083937</td>\n",
       "      <td>0.037811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>46.247100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.084448</td>\n",
       "      <td>0.084448</td>\n",
       "      <td>0.047239</td>\n",
       "      <td>0.084448</td>\n",
       "      <td>0.038045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>46.369900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.086451</td>\n",
       "      <td>0.086451</td>\n",
       "      <td>0.037503</td>\n",
       "      <td>0.086451</td>\n",
       "      <td>0.038721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>45.909200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.085455</td>\n",
       "      <td>0.085455</td>\n",
       "      <td>0.032850</td>\n",
       "      <td>0.085455</td>\n",
       "      <td>0.038025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>45.926400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.081114</td>\n",
       "      <td>0.081114</td>\n",
       "      <td>0.030116</td>\n",
       "      <td>0.081114</td>\n",
       "      <td>0.034507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>45.540600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.088009</td>\n",
       "      <td>0.088009</td>\n",
       "      <td>0.039365</td>\n",
       "      <td>0.088009</td>\n",
       "      <td>0.039817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>45.450600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.086020</td>\n",
       "      <td>0.086020</td>\n",
       "      <td>0.032847</td>\n",
       "      <td>0.086020</td>\n",
       "      <td>0.036995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>45.888200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.089561</td>\n",
       "      <td>0.089561</td>\n",
       "      <td>0.043051</td>\n",
       "      <td>0.089561</td>\n",
       "      <td>0.040819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>45.442600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.087154</td>\n",
       "      <td>0.087154</td>\n",
       "      <td>0.035913</td>\n",
       "      <td>0.087154</td>\n",
       "      <td>0.038689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>44.928200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.085273</td>\n",
       "      <td>0.085273</td>\n",
       "      <td>0.036846</td>\n",
       "      <td>0.085273</td>\n",
       "      <td>0.038907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>45.350100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.084203</td>\n",
       "      <td>0.084203</td>\n",
       "      <td>0.029855</td>\n",
       "      <td>0.084203</td>\n",
       "      <td>0.037040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>44.963200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.087170</td>\n",
       "      <td>0.087170</td>\n",
       "      <td>0.043879</td>\n",
       "      <td>0.087170</td>\n",
       "      <td>0.037768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>45.249200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.084434</td>\n",
       "      <td>0.084434</td>\n",
       "      <td>0.035198</td>\n",
       "      <td>0.084434</td>\n",
       "      <td>0.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>45.047200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.090377</td>\n",
       "      <td>0.090377</td>\n",
       "      <td>0.037771</td>\n",
       "      <td>0.090377</td>\n",
       "      <td>0.041637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>44.627900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.088269</td>\n",
       "      <td>0.088269</td>\n",
       "      <td>0.038515</td>\n",
       "      <td>0.088269</td>\n",
       "      <td>0.039389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>44.736100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.086768</td>\n",
       "      <td>0.086768</td>\n",
       "      <td>0.037344</td>\n",
       "      <td>0.086768</td>\n",
       "      <td>0.038619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>44.653700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.086007</td>\n",
       "      <td>0.086007</td>\n",
       "      <td>0.036039</td>\n",
       "      <td>0.086007</td>\n",
       "      <td>0.037910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>44.452500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.089655</td>\n",
       "      <td>0.089655</td>\n",
       "      <td>0.038453</td>\n",
       "      <td>0.089655</td>\n",
       "      <td>0.040065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>44.020700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.089656</td>\n",
       "      <td>0.089656</td>\n",
       "      <td>0.039071</td>\n",
       "      <td>0.089656</td>\n",
       "      <td>0.039546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>44.586300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.087995</td>\n",
       "      <td>0.087995</td>\n",
       "      <td>0.038746</td>\n",
       "      <td>0.087995</td>\n",
       "      <td>0.038846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>44.149400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.091123</td>\n",
       "      <td>0.091123</td>\n",
       "      <td>0.044490</td>\n",
       "      <td>0.091123</td>\n",
       "      <td>0.040746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>44.105500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.089432</td>\n",
       "      <td>0.089432</td>\n",
       "      <td>0.043495</td>\n",
       "      <td>0.089432</td>\n",
       "      <td>0.040625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>43.780700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.084685</td>\n",
       "      <td>0.084685</td>\n",
       "      <td>0.047293</td>\n",
       "      <td>0.084685</td>\n",
       "      <td>0.038598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>43.613100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.090972</td>\n",
       "      <td>0.090972</td>\n",
       "      <td>0.054008</td>\n",
       "      <td>0.090972</td>\n",
       "      <td>0.041784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>43.533000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.088531</td>\n",
       "      <td>0.088531</td>\n",
       "      <td>0.037532</td>\n",
       "      <td>0.088531</td>\n",
       "      <td>0.039629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>43.770600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.089537</td>\n",
       "      <td>0.089537</td>\n",
       "      <td>0.038661</td>\n",
       "      <td>0.089537</td>\n",
       "      <td>0.040009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>43.581500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.088812</td>\n",
       "      <td>0.088812</td>\n",
       "      <td>0.054850</td>\n",
       "      <td>0.088812</td>\n",
       "      <td>0.041846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>43.222800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.093430</td>\n",
       "      <td>0.093430</td>\n",
       "      <td>0.058456</td>\n",
       "      <td>0.093430</td>\n",
       "      <td>0.044935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>43.098100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.093708</td>\n",
       "      <td>0.093708</td>\n",
       "      <td>0.060015</td>\n",
       "      <td>0.093708</td>\n",
       "      <td>0.045881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>43.517300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.091756</td>\n",
       "      <td>0.091756</td>\n",
       "      <td>0.052694</td>\n",
       "      <td>0.091756</td>\n",
       "      <td>0.043440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>43.158700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.092419</td>\n",
       "      <td>0.092419</td>\n",
       "      <td>0.051372</td>\n",
       "      <td>0.092419</td>\n",
       "      <td>0.043867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>43.610200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.093687</td>\n",
       "      <td>0.093687</td>\n",
       "      <td>0.062743</td>\n",
       "      <td>0.093687</td>\n",
       "      <td>0.046840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>42.855100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.092359</td>\n",
       "      <td>0.092359</td>\n",
       "      <td>0.064522</td>\n",
       "      <td>0.092359</td>\n",
       "      <td>0.045421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>43.141500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.090904</td>\n",
       "      <td>0.090904</td>\n",
       "      <td>0.052143</td>\n",
       "      <td>0.090904</td>\n",
       "      <td>0.044057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>42.948500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.094923</td>\n",
       "      <td>0.094923</td>\n",
       "      <td>0.060631</td>\n",
       "      <td>0.094923</td>\n",
       "      <td>0.047261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>42.531600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.095303</td>\n",
       "      <td>0.095303</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.095303</td>\n",
       "      <td>0.048036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>42.038300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.093535</td>\n",
       "      <td>0.093535</td>\n",
       "      <td>0.058766</td>\n",
       "      <td>0.093535</td>\n",
       "      <td>0.046737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>42.822900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.093478</td>\n",
       "      <td>0.093478</td>\n",
       "      <td>0.058398</td>\n",
       "      <td>0.093478</td>\n",
       "      <td>0.045904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>42.490800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.094357</td>\n",
       "      <td>0.094357</td>\n",
       "      <td>0.066984</td>\n",
       "      <td>0.094357</td>\n",
       "      <td>0.049405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>42.642200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.092649</td>\n",
       "      <td>0.092649</td>\n",
       "      <td>0.061789</td>\n",
       "      <td>0.092649</td>\n",
       "      <td>0.047046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>42.181700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.095196</td>\n",
       "      <td>0.095196</td>\n",
       "      <td>0.081913</td>\n",
       "      <td>0.095196</td>\n",
       "      <td>0.049693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>41.766100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.093747</td>\n",
       "      <td>0.093747</td>\n",
       "      <td>0.081958</td>\n",
       "      <td>0.093747</td>\n",
       "      <td>0.046651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>41.967500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.096841</td>\n",
       "      <td>0.096841</td>\n",
       "      <td>0.065242</td>\n",
       "      <td>0.096841</td>\n",
       "      <td>0.048918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>42.114900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.098220</td>\n",
       "      <td>0.098220</td>\n",
       "      <td>0.063129</td>\n",
       "      <td>0.098220</td>\n",
       "      <td>0.051156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>42.213400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.094648</td>\n",
       "      <td>0.094648</td>\n",
       "      <td>0.073592</td>\n",
       "      <td>0.094648</td>\n",
       "      <td>0.049410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>42.352000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.093323</td>\n",
       "      <td>0.093323</td>\n",
       "      <td>0.089774</td>\n",
       "      <td>0.093323</td>\n",
       "      <td>0.047110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>42.502700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.095235</td>\n",
       "      <td>0.095235</td>\n",
       "      <td>0.080409</td>\n",
       "      <td>0.095235</td>\n",
       "      <td>0.047415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>41.706000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.094757</td>\n",
       "      <td>0.094757</td>\n",
       "      <td>0.080672</td>\n",
       "      <td>0.094757</td>\n",
       "      <td>0.047046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>41.372700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.097227</td>\n",
       "      <td>0.097227</td>\n",
       "      <td>0.088363</td>\n",
       "      <td>0.097227</td>\n",
       "      <td>0.052323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>41.170400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.096623</td>\n",
       "      <td>0.096623</td>\n",
       "      <td>0.089449</td>\n",
       "      <td>0.096623</td>\n",
       "      <td>0.050193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>41.632800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.094132</td>\n",
       "      <td>0.094132</td>\n",
       "      <td>0.078646</td>\n",
       "      <td>0.094132</td>\n",
       "      <td>0.048332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>41.466000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.093888</td>\n",
       "      <td>0.093888</td>\n",
       "      <td>0.086839</td>\n",
       "      <td>0.093888</td>\n",
       "      <td>0.049159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>41.980600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.097730</td>\n",
       "      <td>0.097730</td>\n",
       "      <td>0.091131</td>\n",
       "      <td>0.097730</td>\n",
       "      <td>0.051507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>40.973900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.096009</td>\n",
       "      <td>0.096009</td>\n",
       "      <td>0.104148</td>\n",
       "      <td>0.096009</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>41.102800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.097524</td>\n",
       "      <td>0.097524</td>\n",
       "      <td>0.074681</td>\n",
       "      <td>0.097524</td>\n",
       "      <td>0.050533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>41.171000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.099638</td>\n",
       "      <td>0.099638</td>\n",
       "      <td>0.084038</td>\n",
       "      <td>0.099638</td>\n",
       "      <td>0.051985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>40.709000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.098361</td>\n",
       "      <td>0.098361</td>\n",
       "      <td>0.101936</td>\n",
       "      <td>0.098361</td>\n",
       "      <td>0.051233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>40.784400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.098841</td>\n",
       "      <td>0.098841</td>\n",
       "      <td>0.103205</td>\n",
       "      <td>0.098841</td>\n",
       "      <td>0.052958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>40.965200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.094214</td>\n",
       "      <td>0.094214</td>\n",
       "      <td>0.095597</td>\n",
       "      <td>0.094214</td>\n",
       "      <td>0.050772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>40.858700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.101482</td>\n",
       "      <td>0.101482</td>\n",
       "      <td>0.106363</td>\n",
       "      <td>0.101482</td>\n",
       "      <td>0.053373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>41.074600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.098816</td>\n",
       "      <td>0.098816</td>\n",
       "      <td>0.095419</td>\n",
       "      <td>0.098816</td>\n",
       "      <td>0.051677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>40.755100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.100141</td>\n",
       "      <td>0.100141</td>\n",
       "      <td>0.093694</td>\n",
       "      <td>0.100141</td>\n",
       "      <td>0.054114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>40.798000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.098917</td>\n",
       "      <td>0.098917</td>\n",
       "      <td>0.105714</td>\n",
       "      <td>0.098917</td>\n",
       "      <td>0.050995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>40.806800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.100995</td>\n",
       "      <td>0.100995</td>\n",
       "      <td>0.087062</td>\n",
       "      <td>0.100995</td>\n",
       "      <td>0.053848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>40.219600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.105698</td>\n",
       "      <td>0.105698</td>\n",
       "      <td>0.093389</td>\n",
       "      <td>0.105698</td>\n",
       "      <td>0.056483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>40.910400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.101239</td>\n",
       "      <td>0.101239</td>\n",
       "      <td>0.102123</td>\n",
       "      <td>0.101239</td>\n",
       "      <td>0.054169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>40.531000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.102663</td>\n",
       "      <td>0.102663</td>\n",
       "      <td>0.106285</td>\n",
       "      <td>0.102663</td>\n",
       "      <td>0.054228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>40.225100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.103968</td>\n",
       "      <td>0.103968</td>\n",
       "      <td>0.080597</td>\n",
       "      <td>0.103968</td>\n",
       "      <td>0.056641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>39.991600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.103341</td>\n",
       "      <td>0.103341</td>\n",
       "      <td>0.093840</td>\n",
       "      <td>0.103341</td>\n",
       "      <td>0.056183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>39.799900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.102707</td>\n",
       "      <td>0.102707</td>\n",
       "      <td>0.102681</td>\n",
       "      <td>0.102707</td>\n",
       "      <td>0.054319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>39.843300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.106282</td>\n",
       "      <td>0.106282</td>\n",
       "      <td>0.104015</td>\n",
       "      <td>0.106282</td>\n",
       "      <td>0.057343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>39.650500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.103652</td>\n",
       "      <td>0.103652</td>\n",
       "      <td>0.102497</td>\n",
       "      <td>0.103652</td>\n",
       "      <td>0.056146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>39.915900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.108878</td>\n",
       "      <td>0.108878</td>\n",
       "      <td>0.103183</td>\n",
       "      <td>0.108878</td>\n",
       "      <td>0.060070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>40.041800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.107288</td>\n",
       "      <td>0.107288</td>\n",
       "      <td>0.096881</td>\n",
       "      <td>0.107288</td>\n",
       "      <td>0.057958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>39.727200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.107077</td>\n",
       "      <td>0.107077</td>\n",
       "      <td>0.075553</td>\n",
       "      <td>0.107077</td>\n",
       "      <td>0.058606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>39.534300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.108433</td>\n",
       "      <td>0.108433</td>\n",
       "      <td>0.103188</td>\n",
       "      <td>0.108433</td>\n",
       "      <td>0.060473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>39.714700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.108172</td>\n",
       "      <td>0.108172</td>\n",
       "      <td>0.093069</td>\n",
       "      <td>0.108172</td>\n",
       "      <td>0.058868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>39.583000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.108186</td>\n",
       "      <td>0.108186</td>\n",
       "      <td>0.101487</td>\n",
       "      <td>0.108186</td>\n",
       "      <td>0.058935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>39.454900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.111978</td>\n",
       "      <td>0.111978</td>\n",
       "      <td>0.103341</td>\n",
       "      <td>0.111978</td>\n",
       "      <td>0.059531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>39.533900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.087917</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.061612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>39.385800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.112261</td>\n",
       "      <td>0.112261</td>\n",
       "      <td>0.095594</td>\n",
       "      <td>0.112261</td>\n",
       "      <td>0.060164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>38.808500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.110996</td>\n",
       "      <td>0.110996</td>\n",
       "      <td>0.110221</td>\n",
       "      <td>0.110996</td>\n",
       "      <td>0.059551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>38.836600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.111693</td>\n",
       "      <td>0.111693</td>\n",
       "      <td>0.091399</td>\n",
       "      <td>0.111693</td>\n",
       "      <td>0.061877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>39.011700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.110727</td>\n",
       "      <td>0.110727</td>\n",
       "      <td>0.096199</td>\n",
       "      <td>0.110727</td>\n",
       "      <td>0.061734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>38.619900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.111821</td>\n",
       "      <td>0.111821</td>\n",
       "      <td>0.096025</td>\n",
       "      <td>0.111821</td>\n",
       "      <td>0.062216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>38.897300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.118098</td>\n",
       "      <td>0.118098</td>\n",
       "      <td>0.087796</td>\n",
       "      <td>0.118098</td>\n",
       "      <td>0.065493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>38.750100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.114521</td>\n",
       "      <td>0.114521</td>\n",
       "      <td>0.110120</td>\n",
       "      <td>0.114521</td>\n",
       "      <td>0.062865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>38.582300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.114951</td>\n",
       "      <td>0.114951</td>\n",
       "      <td>0.105684</td>\n",
       "      <td>0.114951</td>\n",
       "      <td>0.063375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>38.605400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.117258</td>\n",
       "      <td>0.117258</td>\n",
       "      <td>0.113627</td>\n",
       "      <td>0.117258</td>\n",
       "      <td>0.065722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>38.851300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.116650</td>\n",
       "      <td>0.116650</td>\n",
       "      <td>0.088405</td>\n",
       "      <td>0.116650</td>\n",
       "      <td>0.065270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>38.382600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.116836</td>\n",
       "      <td>0.116836</td>\n",
       "      <td>0.101367</td>\n",
       "      <td>0.116836</td>\n",
       "      <td>0.063874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>38.614000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.117456</td>\n",
       "      <td>0.117456</td>\n",
       "      <td>0.087223</td>\n",
       "      <td>0.117456</td>\n",
       "      <td>0.063427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>38.254800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.118615</td>\n",
       "      <td>0.118615</td>\n",
       "      <td>0.101098</td>\n",
       "      <td>0.118615</td>\n",
       "      <td>0.065413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>38.561900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.118415</td>\n",
       "      <td>0.118415</td>\n",
       "      <td>0.075699</td>\n",
       "      <td>0.118415</td>\n",
       "      <td>0.064231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>38.460100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.118128</td>\n",
       "      <td>0.118128</td>\n",
       "      <td>0.095314</td>\n",
       "      <td>0.118128</td>\n",
       "      <td>0.063825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>38.431200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.120271</td>\n",
       "      <td>0.120271</td>\n",
       "      <td>0.089800</td>\n",
       "      <td>0.120271</td>\n",
       "      <td>0.064477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>38.279400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.122046</td>\n",
       "      <td>0.122046</td>\n",
       "      <td>0.095045</td>\n",
       "      <td>0.122046</td>\n",
       "      <td>0.066576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>38.120400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.121185</td>\n",
       "      <td>0.121185</td>\n",
       "      <td>0.082613</td>\n",
       "      <td>0.121185</td>\n",
       "      <td>0.065901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>38.450000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.122050</td>\n",
       "      <td>0.122050</td>\n",
       "      <td>0.103567</td>\n",
       "      <td>0.122050</td>\n",
       "      <td>0.067325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>38.423100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.124170</td>\n",
       "      <td>0.124170</td>\n",
       "      <td>0.092892</td>\n",
       "      <td>0.124170</td>\n",
       "      <td>0.066602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>37.964200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.125026</td>\n",
       "      <td>0.125026</td>\n",
       "      <td>0.091882</td>\n",
       "      <td>0.125026</td>\n",
       "      <td>0.068618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>37.894900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.122165</td>\n",
       "      <td>0.122165</td>\n",
       "      <td>0.089196</td>\n",
       "      <td>0.122165</td>\n",
       "      <td>0.066780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>38.184200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.126110</td>\n",
       "      <td>0.126110</td>\n",
       "      <td>0.088344</td>\n",
       "      <td>0.126110</td>\n",
       "      <td>0.069872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>37.836400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.125726</td>\n",
       "      <td>0.125726</td>\n",
       "      <td>0.064118</td>\n",
       "      <td>0.125726</td>\n",
       "      <td>0.067954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>38.093200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.126429</td>\n",
       "      <td>0.126429</td>\n",
       "      <td>0.081290</td>\n",
       "      <td>0.126429</td>\n",
       "      <td>0.067871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>37.703200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.130841</td>\n",
       "      <td>0.130841</td>\n",
       "      <td>0.079375</td>\n",
       "      <td>0.130841</td>\n",
       "      <td>0.071199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>37.801700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.130019</td>\n",
       "      <td>0.130019</td>\n",
       "      <td>0.074667</td>\n",
       "      <td>0.130019</td>\n",
       "      <td>0.070581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>38.069600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.129676</td>\n",
       "      <td>0.129676</td>\n",
       "      <td>0.095761</td>\n",
       "      <td>0.129676</td>\n",
       "      <td>0.070704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>37.749300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.134571</td>\n",
       "      <td>0.134571</td>\n",
       "      <td>0.075439</td>\n",
       "      <td>0.134571</td>\n",
       "      <td>0.073708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>37.251100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.133540</td>\n",
       "      <td>0.133540</td>\n",
       "      <td>0.078680</td>\n",
       "      <td>0.133540</td>\n",
       "      <td>0.073027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>37.484700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.131571</td>\n",
       "      <td>0.131571</td>\n",
       "      <td>0.084610</td>\n",
       "      <td>0.131571</td>\n",
       "      <td>0.072054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>37.305700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.137253</td>\n",
       "      <td>0.137253</td>\n",
       "      <td>0.073223</td>\n",
       "      <td>0.137253</td>\n",
       "      <td>0.075033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>37.432900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.141145</td>\n",
       "      <td>0.141145</td>\n",
       "      <td>0.077097</td>\n",
       "      <td>0.141145</td>\n",
       "      <td>0.077869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>37.575200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.137569</td>\n",
       "      <td>0.137569</td>\n",
       "      <td>0.073092</td>\n",
       "      <td>0.137569</td>\n",
       "      <td>0.074238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>37.321600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.144601</td>\n",
       "      <td>0.144601</td>\n",
       "      <td>0.073451</td>\n",
       "      <td>0.144601</td>\n",
       "      <td>0.078102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>37.171500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.140138</td>\n",
       "      <td>0.140138</td>\n",
       "      <td>0.071754</td>\n",
       "      <td>0.140138</td>\n",
       "      <td>0.076052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>37.416900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.139443</td>\n",
       "      <td>0.139443</td>\n",
       "      <td>0.073895</td>\n",
       "      <td>0.139443</td>\n",
       "      <td>0.076471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>37.086800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.142333</td>\n",
       "      <td>0.142333</td>\n",
       "      <td>0.078530</td>\n",
       "      <td>0.142333</td>\n",
       "      <td>0.077330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>37.013000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.143749</td>\n",
       "      <td>0.143749</td>\n",
       "      <td>0.073444</td>\n",
       "      <td>0.143749</td>\n",
       "      <td>0.079036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>36.912500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.144759</td>\n",
       "      <td>0.144759</td>\n",
       "      <td>0.075873</td>\n",
       "      <td>0.144759</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>36.725700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.145264</td>\n",
       "      <td>0.145264</td>\n",
       "      <td>0.082629</td>\n",
       "      <td>0.145264</td>\n",
       "      <td>0.083177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>36.513200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.147013</td>\n",
       "      <td>0.147013</td>\n",
       "      <td>0.074095</td>\n",
       "      <td>0.147013</td>\n",
       "      <td>0.081209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>36.770300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.147785</td>\n",
       "      <td>0.147785</td>\n",
       "      <td>0.080446</td>\n",
       "      <td>0.147785</td>\n",
       "      <td>0.083996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>36.643600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.152625</td>\n",
       "      <td>0.152625</td>\n",
       "      <td>0.078774</td>\n",
       "      <td>0.152625</td>\n",
       "      <td>0.086229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>36.780400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.154373</td>\n",
       "      <td>0.154373</td>\n",
       "      <td>0.086136</td>\n",
       "      <td>0.154373</td>\n",
       "      <td>0.086861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>37.158600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.154151</td>\n",
       "      <td>0.154151</td>\n",
       "      <td>0.075560</td>\n",
       "      <td>0.154151</td>\n",
       "      <td>0.086515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>36.642100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.154965</td>\n",
       "      <td>0.154965</td>\n",
       "      <td>0.077684</td>\n",
       "      <td>0.154965</td>\n",
       "      <td>0.086773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>36.832400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.151187</td>\n",
       "      <td>0.151187</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.151187</td>\n",
       "      <td>0.084534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>36.265900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.156963</td>\n",
       "      <td>0.156963</td>\n",
       "      <td>0.089542</td>\n",
       "      <td>0.156963</td>\n",
       "      <td>0.088258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>36.256200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.157893</td>\n",
       "      <td>0.157893</td>\n",
       "      <td>0.079091</td>\n",
       "      <td>0.157893</td>\n",
       "      <td>0.088050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>36.326500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.153639</td>\n",
       "      <td>0.153639</td>\n",
       "      <td>0.077389</td>\n",
       "      <td>0.153639</td>\n",
       "      <td>0.085919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>36.278600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.155573</td>\n",
       "      <td>0.155573</td>\n",
       "      <td>0.081716</td>\n",
       "      <td>0.155573</td>\n",
       "      <td>0.088124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>36.107600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.158235</td>\n",
       "      <td>0.158235</td>\n",
       "      <td>0.085831</td>\n",
       "      <td>0.158235</td>\n",
       "      <td>0.089812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>36.350900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.158429</td>\n",
       "      <td>0.158429</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.158429</td>\n",
       "      <td>0.088260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>36.414400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.160205</td>\n",
       "      <td>0.160205</td>\n",
       "      <td>0.078537</td>\n",
       "      <td>0.160205</td>\n",
       "      <td>0.089669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>36.102700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.165992</td>\n",
       "      <td>0.165992</td>\n",
       "      <td>0.085315</td>\n",
       "      <td>0.165992</td>\n",
       "      <td>0.095501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>36.397100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.164181</td>\n",
       "      <td>0.164181</td>\n",
       "      <td>0.085004</td>\n",
       "      <td>0.164181</td>\n",
       "      <td>0.092065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>35.729300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.163724</td>\n",
       "      <td>0.163724</td>\n",
       "      <td>0.081348</td>\n",
       "      <td>0.163724</td>\n",
       "      <td>0.092381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>36.164200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.162368</td>\n",
       "      <td>0.162368</td>\n",
       "      <td>0.077836</td>\n",
       "      <td>0.162368</td>\n",
       "      <td>0.090621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>35.772300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.162593</td>\n",
       "      <td>0.162593</td>\n",
       "      <td>0.080342</td>\n",
       "      <td>0.162593</td>\n",
       "      <td>0.091675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>36.217900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.164253</td>\n",
       "      <td>0.164253</td>\n",
       "      <td>0.078339</td>\n",
       "      <td>0.164253</td>\n",
       "      <td>0.091967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>36.111300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.162946</td>\n",
       "      <td>0.162946</td>\n",
       "      <td>0.077502</td>\n",
       "      <td>0.162946</td>\n",
       "      <td>0.091771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>35.576700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.169537</td>\n",
       "      <td>0.169537</td>\n",
       "      <td>0.084302</td>\n",
       "      <td>0.169537</td>\n",
       "      <td>0.095757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>35.426700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.166115</td>\n",
       "      <td>0.166115</td>\n",
       "      <td>0.085655</td>\n",
       "      <td>0.166115</td>\n",
       "      <td>0.094100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>35.367900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.167339</td>\n",
       "      <td>0.167339</td>\n",
       "      <td>0.083636</td>\n",
       "      <td>0.167339</td>\n",
       "      <td>0.096851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>35.673600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.168913</td>\n",
       "      <td>0.168913</td>\n",
       "      <td>0.083206</td>\n",
       "      <td>0.168913</td>\n",
       "      <td>0.094974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>35.496000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.167839</td>\n",
       "      <td>0.167839</td>\n",
       "      <td>0.080521</td>\n",
       "      <td>0.167839</td>\n",
       "      <td>0.096014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>35.640800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.169927</td>\n",
       "      <td>0.169927</td>\n",
       "      <td>0.087320</td>\n",
       "      <td>0.169927</td>\n",
       "      <td>0.096639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>35.577200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.169164</td>\n",
       "      <td>0.169164</td>\n",
       "      <td>0.082038</td>\n",
       "      <td>0.169164</td>\n",
       "      <td>0.096359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>35.305900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.169498</td>\n",
       "      <td>0.169498</td>\n",
       "      <td>0.086621</td>\n",
       "      <td>0.169498</td>\n",
       "      <td>0.096636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>35.272000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.171564</td>\n",
       "      <td>0.171564</td>\n",
       "      <td>0.082516</td>\n",
       "      <td>0.171564</td>\n",
       "      <td>0.098114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>35.251800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.172876</td>\n",
       "      <td>0.172876</td>\n",
       "      <td>0.081508</td>\n",
       "      <td>0.172876</td>\n",
       "      <td>0.098170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>35.299900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.174177</td>\n",
       "      <td>0.174177</td>\n",
       "      <td>0.090596</td>\n",
       "      <td>0.174177</td>\n",
       "      <td>0.099470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>35.135900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.172014</td>\n",
       "      <td>0.172014</td>\n",
       "      <td>0.089675</td>\n",
       "      <td>0.172014</td>\n",
       "      <td>0.099044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91500</td>\n",
       "      <td>35.269900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.172570</td>\n",
       "      <td>0.172570</td>\n",
       "      <td>0.083565</td>\n",
       "      <td>0.172570</td>\n",
       "      <td>0.096457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>35.269500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.173125</td>\n",
       "      <td>0.173125</td>\n",
       "      <td>0.088023</td>\n",
       "      <td>0.173125</td>\n",
       "      <td>0.099583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>35.119400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.177049</td>\n",
       "      <td>0.177049</td>\n",
       "      <td>0.085771</td>\n",
       "      <td>0.177049</td>\n",
       "      <td>0.101750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>35.083500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.174703</td>\n",
       "      <td>0.174703</td>\n",
       "      <td>0.088982</td>\n",
       "      <td>0.174703</td>\n",
       "      <td>0.100018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93500</td>\n",
       "      <td>35.346400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.174806</td>\n",
       "      <td>0.174806</td>\n",
       "      <td>0.094498</td>\n",
       "      <td>0.174806</td>\n",
       "      <td>0.102392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>34.929500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.175870</td>\n",
       "      <td>0.175870</td>\n",
       "      <td>0.081212</td>\n",
       "      <td>0.175870</td>\n",
       "      <td>0.100486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94500</td>\n",
       "      <td>35.018800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.174235</td>\n",
       "      <td>0.174235</td>\n",
       "      <td>0.090640</td>\n",
       "      <td>0.174235</td>\n",
       "      <td>0.100907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>35.296700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.172962</td>\n",
       "      <td>0.172962</td>\n",
       "      <td>0.085922</td>\n",
       "      <td>0.172962</td>\n",
       "      <td>0.100061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95500</td>\n",
       "      <td>34.933700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.176646</td>\n",
       "      <td>0.176646</td>\n",
       "      <td>0.087021</td>\n",
       "      <td>0.176646</td>\n",
       "      <td>0.101039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>34.732800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.178519</td>\n",
       "      <td>0.178519</td>\n",
       "      <td>0.083181</td>\n",
       "      <td>0.178519</td>\n",
       "      <td>0.102852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96500</td>\n",
       "      <td>34.911800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.180388</td>\n",
       "      <td>0.180388</td>\n",
       "      <td>0.088802</td>\n",
       "      <td>0.180388</td>\n",
       "      <td>0.102919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>34.518400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.182596</td>\n",
       "      <td>0.182596</td>\n",
       "      <td>0.093207</td>\n",
       "      <td>0.182596</td>\n",
       "      <td>0.105929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97500</td>\n",
       "      <td>35.155800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.179198</td>\n",
       "      <td>0.179198</td>\n",
       "      <td>0.090578</td>\n",
       "      <td>0.179198</td>\n",
       "      <td>0.103731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>34.872100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.180752</td>\n",
       "      <td>0.180752</td>\n",
       "      <td>0.087903</td>\n",
       "      <td>0.180752</td>\n",
       "      <td>0.103927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98500</td>\n",
       "      <td>34.945000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.179101</td>\n",
       "      <td>0.179101</td>\n",
       "      <td>0.094693</td>\n",
       "      <td>0.179101</td>\n",
       "      <td>0.104124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>34.110100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.182358</td>\n",
       "      <td>0.182358</td>\n",
       "      <td>0.091565</td>\n",
       "      <td>0.182358</td>\n",
       "      <td>0.105175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99500</td>\n",
       "      <td>34.489300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.180685</td>\n",
       "      <td>0.180685</td>\n",
       "      <td>0.094319</td>\n",
       "      <td>0.180685</td>\n",
       "      <td>0.105171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>34.692300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.182795</td>\n",
       "      <td>0.182795</td>\n",
       "      <td>0.093574</td>\n",
       "      <td>0.182795</td>\n",
       "      <td>0.106688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100500</td>\n",
       "      <td>34.437100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.182261</td>\n",
       "      <td>0.182261</td>\n",
       "      <td>0.090162</td>\n",
       "      <td>0.182261</td>\n",
       "      <td>0.104986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>34.471600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.182597</td>\n",
       "      <td>0.182597</td>\n",
       "      <td>0.088923</td>\n",
       "      <td>0.182597</td>\n",
       "      <td>0.106072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101500</td>\n",
       "      <td>34.331200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.186444</td>\n",
       "      <td>0.186444</td>\n",
       "      <td>0.090455</td>\n",
       "      <td>0.186444</td>\n",
       "      <td>0.108502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>34.525400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.187632</td>\n",
       "      <td>0.187632</td>\n",
       "      <td>0.093563</td>\n",
       "      <td>0.187632</td>\n",
       "      <td>0.108920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102500</td>\n",
       "      <td>34.174500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.185886</td>\n",
       "      <td>0.185886</td>\n",
       "      <td>0.092608</td>\n",
       "      <td>0.185886</td>\n",
       "      <td>0.107039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>34.184400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.185220</td>\n",
       "      <td>0.185220</td>\n",
       "      <td>0.095322</td>\n",
       "      <td>0.185220</td>\n",
       "      <td>0.107307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103500</td>\n",
       "      <td>34.365400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.181605</td>\n",
       "      <td>0.181605</td>\n",
       "      <td>0.085669</td>\n",
       "      <td>0.181605</td>\n",
       "      <td>0.104677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>34.239600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.187449</td>\n",
       "      <td>0.187449</td>\n",
       "      <td>0.093010</td>\n",
       "      <td>0.187449</td>\n",
       "      <td>0.110031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104500</td>\n",
       "      <td>34.127600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.183924</td>\n",
       "      <td>0.183924</td>\n",
       "      <td>0.091124</td>\n",
       "      <td>0.183924</td>\n",
       "      <td>0.106506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>33.844600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.185707</td>\n",
       "      <td>0.185707</td>\n",
       "      <td>0.089911</td>\n",
       "      <td>0.185707</td>\n",
       "      <td>0.106691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105500</td>\n",
       "      <td>34.036100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.186923</td>\n",
       "      <td>0.186923</td>\n",
       "      <td>0.097870</td>\n",
       "      <td>0.186923</td>\n",
       "      <td>0.108079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>34.123600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.190057</td>\n",
       "      <td>0.190057</td>\n",
       "      <td>0.096653</td>\n",
       "      <td>0.190057</td>\n",
       "      <td>0.110775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106500</td>\n",
       "      <td>34.009000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.187271</td>\n",
       "      <td>0.187271</td>\n",
       "      <td>0.101897</td>\n",
       "      <td>0.187271</td>\n",
       "      <td>0.109313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>33.789600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.188699</td>\n",
       "      <td>0.188699</td>\n",
       "      <td>0.095539</td>\n",
       "      <td>0.188699</td>\n",
       "      <td>0.112062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107500</td>\n",
       "      <td>33.739600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.189455</td>\n",
       "      <td>0.189455</td>\n",
       "      <td>0.094965</td>\n",
       "      <td>0.189455</td>\n",
       "      <td>0.111156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>33.518400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.190842</td>\n",
       "      <td>0.190842</td>\n",
       "      <td>0.094642</td>\n",
       "      <td>0.190842</td>\n",
       "      <td>0.110900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108500</td>\n",
       "      <td>33.905900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.183641</td>\n",
       "      <td>0.183641</td>\n",
       "      <td>0.091196</td>\n",
       "      <td>0.183641</td>\n",
       "      <td>0.106808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>33.870000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.188987</td>\n",
       "      <td>0.188987</td>\n",
       "      <td>0.091629</td>\n",
       "      <td>0.188987</td>\n",
       "      <td>0.110282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109500</td>\n",
       "      <td>33.935400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.189835</td>\n",
       "      <td>0.189835</td>\n",
       "      <td>0.094350</td>\n",
       "      <td>0.189835</td>\n",
       "      <td>0.111183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>33.191300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.188241</td>\n",
       "      <td>0.188241</td>\n",
       "      <td>0.090408</td>\n",
       "      <td>0.188241</td>\n",
       "      <td>0.108157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110500</td>\n",
       "      <td>34.035400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.189588</td>\n",
       "      <td>0.189588</td>\n",
       "      <td>0.093392</td>\n",
       "      <td>0.189588</td>\n",
       "      <td>0.110422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>33.493100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.189639</td>\n",
       "      <td>0.189639</td>\n",
       "      <td>0.091737</td>\n",
       "      <td>0.189639</td>\n",
       "      <td>0.110871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111500</td>\n",
       "      <td>33.676000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.194804</td>\n",
       "      <td>0.194804</td>\n",
       "      <td>0.103142</td>\n",
       "      <td>0.194804</td>\n",
       "      <td>0.114512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>33.424100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.190528</td>\n",
       "      <td>0.190528</td>\n",
       "      <td>0.097236</td>\n",
       "      <td>0.190528</td>\n",
       "      <td>0.112929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112500</td>\n",
       "      <td>33.919900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.193774</td>\n",
       "      <td>0.193774</td>\n",
       "      <td>0.103674</td>\n",
       "      <td>0.193774</td>\n",
       "      <td>0.114896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>33.717900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.191895</td>\n",
       "      <td>0.191895</td>\n",
       "      <td>0.105294</td>\n",
       "      <td>0.191895</td>\n",
       "      <td>0.112914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113500</td>\n",
       "      <td>33.658700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.191671</td>\n",
       "      <td>0.191671</td>\n",
       "      <td>0.102311</td>\n",
       "      <td>0.191671</td>\n",
       "      <td>0.113191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>33.410300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.196102</td>\n",
       "      <td>0.196102</td>\n",
       "      <td>0.100506</td>\n",
       "      <td>0.196102</td>\n",
       "      <td>0.116816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114500</td>\n",
       "      <td>33.472200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.191430</td>\n",
       "      <td>0.191430</td>\n",
       "      <td>0.100988</td>\n",
       "      <td>0.191430</td>\n",
       "      <td>0.113489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>33.547500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.194649</td>\n",
       "      <td>0.194649</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.194649</td>\n",
       "      <td>0.116011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115500</td>\n",
       "      <td>32.908500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.197163</td>\n",
       "      <td>0.197163</td>\n",
       "      <td>0.104520</td>\n",
       "      <td>0.197163</td>\n",
       "      <td>0.117669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>33.350000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.193446</td>\n",
       "      <td>0.193446</td>\n",
       "      <td>0.106040</td>\n",
       "      <td>0.193446</td>\n",
       "      <td>0.114747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116500</td>\n",
       "      <td>33.332800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.198459</td>\n",
       "      <td>0.198459</td>\n",
       "      <td>0.099169</td>\n",
       "      <td>0.198459</td>\n",
       "      <td>0.117715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>33.301600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.195265</td>\n",
       "      <td>0.195265</td>\n",
       "      <td>0.103145</td>\n",
       "      <td>0.195265</td>\n",
       "      <td>0.116119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117500</td>\n",
       "      <td>32.896400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.200746</td>\n",
       "      <td>0.200746</td>\n",
       "      <td>0.106461</td>\n",
       "      <td>0.200746</td>\n",
       "      <td>0.119527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>33.189100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.194997</td>\n",
       "      <td>0.194997</td>\n",
       "      <td>0.108141</td>\n",
       "      <td>0.194997</td>\n",
       "      <td>0.118229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118500</td>\n",
       "      <td>33.300500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.197095</td>\n",
       "      <td>0.197095</td>\n",
       "      <td>0.112382</td>\n",
       "      <td>0.197095</td>\n",
       "      <td>0.118688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>33.173100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.194490</td>\n",
       "      <td>0.194490</td>\n",
       "      <td>0.102484</td>\n",
       "      <td>0.194490</td>\n",
       "      <td>0.116805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119500</td>\n",
       "      <td>32.705000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.196390</td>\n",
       "      <td>0.196390</td>\n",
       "      <td>0.106186</td>\n",
       "      <td>0.196390</td>\n",
       "      <td>0.118067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>32.933700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.201346</td>\n",
       "      <td>0.201346</td>\n",
       "      <td>0.105554</td>\n",
       "      <td>0.201346</td>\n",
       "      <td>0.121911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120500</td>\n",
       "      <td>33.115200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.201678</td>\n",
       "      <td>0.201678</td>\n",
       "      <td>0.106620</td>\n",
       "      <td>0.201678</td>\n",
       "      <td>0.122667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>32.966100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.201290</td>\n",
       "      <td>0.201290</td>\n",
       "      <td>0.102958</td>\n",
       "      <td>0.201290</td>\n",
       "      <td>0.119265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121500</td>\n",
       "      <td>33.034400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.199959</td>\n",
       "      <td>0.199959</td>\n",
       "      <td>0.103608</td>\n",
       "      <td>0.199959</td>\n",
       "      <td>0.120049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>32.848900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.203860</td>\n",
       "      <td>0.203860</td>\n",
       "      <td>0.113504</td>\n",
       "      <td>0.203860</td>\n",
       "      <td>0.124541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122500</td>\n",
       "      <td>32.650400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.201448</td>\n",
       "      <td>0.201448</td>\n",
       "      <td>0.105770</td>\n",
       "      <td>0.201448</td>\n",
       "      <td>0.121736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>32.455100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.199573</td>\n",
       "      <td>0.199573</td>\n",
       "      <td>0.109881</td>\n",
       "      <td>0.199573</td>\n",
       "      <td>0.120709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123500</td>\n",
       "      <td>32.801800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.205445</td>\n",
       "      <td>0.205445</td>\n",
       "      <td>0.110844</td>\n",
       "      <td>0.205445</td>\n",
       "      <td>0.125933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>32.797300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.200943</td>\n",
       "      <td>0.200943</td>\n",
       "      <td>0.104757</td>\n",
       "      <td>0.200943</td>\n",
       "      <td>0.120777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124500</td>\n",
       "      <td>32.658800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.201726</td>\n",
       "      <td>0.201726</td>\n",
       "      <td>0.105784</td>\n",
       "      <td>0.201726</td>\n",
       "      <td>0.121525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>32.180800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.201338</td>\n",
       "      <td>0.201338</td>\n",
       "      <td>0.109930</td>\n",
       "      <td>0.201338</td>\n",
       "      <td>0.121465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125500</td>\n",
       "      <td>32.477200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.208716</td>\n",
       "      <td>0.208716</td>\n",
       "      <td>0.118099</td>\n",
       "      <td>0.208716</td>\n",
       "      <td>0.128729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>32.829300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.206544</td>\n",
       "      <td>0.206544</td>\n",
       "      <td>0.118276</td>\n",
       "      <td>0.206544</td>\n",
       "      <td>0.126444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126500</td>\n",
       "      <td>32.722200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.204538</td>\n",
       "      <td>0.204538</td>\n",
       "      <td>0.113998</td>\n",
       "      <td>0.204538</td>\n",
       "      <td>0.124228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>32.328600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.204637</td>\n",
       "      <td>0.204637</td>\n",
       "      <td>0.110715</td>\n",
       "      <td>0.204637</td>\n",
       "      <td>0.123019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127500</td>\n",
       "      <td>32.484600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.207793</td>\n",
       "      <td>0.207793</td>\n",
       "      <td>0.120762</td>\n",
       "      <td>0.207793</td>\n",
       "      <td>0.129256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>32.587900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.207723</td>\n",
       "      <td>0.207723</td>\n",
       "      <td>0.107282</td>\n",
       "      <td>0.207723</td>\n",
       "      <td>0.126007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128500</td>\n",
       "      <td>32.409800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.205892</td>\n",
       "      <td>0.205892</td>\n",
       "      <td>0.116166</td>\n",
       "      <td>0.205892</td>\n",
       "      <td>0.127374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>32.387400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.207418</td>\n",
       "      <td>0.207418</td>\n",
       "      <td>0.113980</td>\n",
       "      <td>0.207418</td>\n",
       "      <td>0.127866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129500</td>\n",
       "      <td>32.188500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.206530</td>\n",
       "      <td>0.206530</td>\n",
       "      <td>0.112385</td>\n",
       "      <td>0.206530</td>\n",
       "      <td>0.125405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>32.161100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.211709</td>\n",
       "      <td>0.211709</td>\n",
       "      <td>0.112851</td>\n",
       "      <td>0.211709</td>\n",
       "      <td>0.129792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130500</td>\n",
       "      <td>32.492100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.208598</td>\n",
       "      <td>0.208598</td>\n",
       "      <td>0.116113</td>\n",
       "      <td>0.208598</td>\n",
       "      <td>0.127450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>32.322000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.210027</td>\n",
       "      <td>0.210027</td>\n",
       "      <td>0.112561</td>\n",
       "      <td>0.210027</td>\n",
       "      <td>0.128091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131500</td>\n",
       "      <td>32.104000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.207546</td>\n",
       "      <td>0.207546</td>\n",
       "      <td>0.114459</td>\n",
       "      <td>0.207546</td>\n",
       "      <td>0.126823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>32.225100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.208260</td>\n",
       "      <td>0.208260</td>\n",
       "      <td>0.113356</td>\n",
       "      <td>0.208260</td>\n",
       "      <td>0.129021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132500</td>\n",
       "      <td>32.232700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.208214</td>\n",
       "      <td>0.208214</td>\n",
       "      <td>0.111735</td>\n",
       "      <td>0.208214</td>\n",
       "      <td>0.127305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>32.019600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.208107</td>\n",
       "      <td>0.208107</td>\n",
       "      <td>0.115002</td>\n",
       "      <td>0.208107</td>\n",
       "      <td>0.128083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133500</td>\n",
       "      <td>32.160500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.213078</td>\n",
       "      <td>0.213078</td>\n",
       "      <td>0.117766</td>\n",
       "      <td>0.213078</td>\n",
       "      <td>0.130891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>32.146100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.209906</td>\n",
       "      <td>0.209906</td>\n",
       "      <td>0.114890</td>\n",
       "      <td>0.209906</td>\n",
       "      <td>0.130063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134500</td>\n",
       "      <td>31.624300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.208435</td>\n",
       "      <td>0.208435</td>\n",
       "      <td>0.116521</td>\n",
       "      <td>0.208435</td>\n",
       "      <td>0.127476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>32.367900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.210411</td>\n",
       "      <td>0.210411</td>\n",
       "      <td>0.118292</td>\n",
       "      <td>0.210411</td>\n",
       "      <td>0.130018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135500</td>\n",
       "      <td>31.837200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.209443</td>\n",
       "      <td>0.209443</td>\n",
       "      <td>0.115339</td>\n",
       "      <td>0.209443</td>\n",
       "      <td>0.129621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>31.757500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.216219</td>\n",
       "      <td>0.216219</td>\n",
       "      <td>0.123892</td>\n",
       "      <td>0.216219</td>\n",
       "      <td>0.134599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136500</td>\n",
       "      <td>31.884200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.213629</td>\n",
       "      <td>0.213629</td>\n",
       "      <td>0.115877</td>\n",
       "      <td>0.213629</td>\n",
       "      <td>0.132609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>32.027700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.213375</td>\n",
       "      <td>0.213375</td>\n",
       "      <td>0.124285</td>\n",
       "      <td>0.213375</td>\n",
       "      <td>0.132593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137500</td>\n",
       "      <td>31.758600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.211598</td>\n",
       "      <td>0.211598</td>\n",
       "      <td>0.113659</td>\n",
       "      <td>0.211598</td>\n",
       "      <td>0.130507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>31.892700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.214869</td>\n",
       "      <td>0.214869</td>\n",
       "      <td>0.114436</td>\n",
       "      <td>0.214869</td>\n",
       "      <td>0.133475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138500</td>\n",
       "      <td>31.526600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.214706</td>\n",
       "      <td>0.214706</td>\n",
       "      <td>0.118778</td>\n",
       "      <td>0.214706</td>\n",
       "      <td>0.133977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>31.823500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.210689</td>\n",
       "      <td>0.210689</td>\n",
       "      <td>0.117578</td>\n",
       "      <td>0.210689</td>\n",
       "      <td>0.130356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139500</td>\n",
       "      <td>31.764100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.215922</td>\n",
       "      <td>0.215922</td>\n",
       "      <td>0.120713</td>\n",
       "      <td>0.215922</td>\n",
       "      <td>0.134496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>31.903100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.214083</td>\n",
       "      <td>0.214083</td>\n",
       "      <td>0.113750</td>\n",
       "      <td>0.214083</td>\n",
       "      <td>0.132335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140500</td>\n",
       "      <td>31.753100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.213846</td>\n",
       "      <td>0.213846</td>\n",
       "      <td>0.115877</td>\n",
       "      <td>0.213846</td>\n",
       "      <td>0.132421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>31.585000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.213202</td>\n",
       "      <td>0.213202</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.213202</td>\n",
       "      <td>0.133496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141500</td>\n",
       "      <td>31.884500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.214943</td>\n",
       "      <td>0.214943</td>\n",
       "      <td>0.114016</td>\n",
       "      <td>0.214943</td>\n",
       "      <td>0.134301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>31.182000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.210228</td>\n",
       "      <td>0.210228</td>\n",
       "      <td>0.117539</td>\n",
       "      <td>0.210228</td>\n",
       "      <td>0.130556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142500</td>\n",
       "      <td>31.764300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.212997</td>\n",
       "      <td>0.212997</td>\n",
       "      <td>0.117279</td>\n",
       "      <td>0.212997</td>\n",
       "      <td>0.132424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>31.573000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.217665</td>\n",
       "      <td>0.217665</td>\n",
       "      <td>0.118373</td>\n",
       "      <td>0.217665</td>\n",
       "      <td>0.136045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143500</td>\n",
       "      <td>31.609300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.218196</td>\n",
       "      <td>0.218196</td>\n",
       "      <td>0.116933</td>\n",
       "      <td>0.218196</td>\n",
       "      <td>0.136156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>31.603600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.215609</td>\n",
       "      <td>0.215609</td>\n",
       "      <td>0.126622</td>\n",
       "      <td>0.215609</td>\n",
       "      <td>0.136602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144500</td>\n",
       "      <td>31.368800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.214171</td>\n",
       "      <td>0.214171</td>\n",
       "      <td>0.114019</td>\n",
       "      <td>0.214171</td>\n",
       "      <td>0.134384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>31.545800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.220508</td>\n",
       "      <td>0.220508</td>\n",
       "      <td>0.127776</td>\n",
       "      <td>0.220508</td>\n",
       "      <td>0.140501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145500</td>\n",
       "      <td>31.536500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.217315</td>\n",
       "      <td>0.217315</td>\n",
       "      <td>0.121358</td>\n",
       "      <td>0.217315</td>\n",
       "      <td>0.136010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>31.307700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.121560</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.134573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146500</td>\n",
       "      <td>31.428900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.215292</td>\n",
       "      <td>0.215292</td>\n",
       "      <td>0.122546</td>\n",
       "      <td>0.215292</td>\n",
       "      <td>0.133994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>31.065800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.219518</td>\n",
       "      <td>0.219518</td>\n",
       "      <td>0.123698</td>\n",
       "      <td>0.219518</td>\n",
       "      <td>0.138932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147500</td>\n",
       "      <td>31.011100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.217503</td>\n",
       "      <td>0.217503</td>\n",
       "      <td>0.123998</td>\n",
       "      <td>0.217503</td>\n",
       "      <td>0.137373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>31.432000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>0.123465</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>0.138252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148500</td>\n",
       "      <td>31.105000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.212475</td>\n",
       "      <td>0.212475</td>\n",
       "      <td>0.125245</td>\n",
       "      <td>0.212475</td>\n",
       "      <td>0.133162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149000</td>\n",
       "      <td>31.078000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.220034</td>\n",
       "      <td>0.220034</td>\n",
       "      <td>0.125751</td>\n",
       "      <td>0.220034</td>\n",
       "      <td>0.140052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149500</td>\n",
       "      <td>30.974400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.219624</td>\n",
       "      <td>0.219624</td>\n",
       "      <td>0.121105</td>\n",
       "      <td>0.219624</td>\n",
       "      <td>0.138144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>31.154000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.220742</td>\n",
       "      <td>0.220742</td>\n",
       "      <td>0.128398</td>\n",
       "      <td>0.220742</td>\n",
       "      <td>0.139019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150500</td>\n",
       "      <td>31.307700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.222242</td>\n",
       "      <td>0.222242</td>\n",
       "      <td>0.128390</td>\n",
       "      <td>0.222242</td>\n",
       "      <td>0.141655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151000</td>\n",
       "      <td>31.043100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.219416</td>\n",
       "      <td>0.219416</td>\n",
       "      <td>0.118439</td>\n",
       "      <td>0.219416</td>\n",
       "      <td>0.139493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151500</td>\n",
       "      <td>31.523800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.218529</td>\n",
       "      <td>0.218529</td>\n",
       "      <td>0.125230</td>\n",
       "      <td>0.218529</td>\n",
       "      <td>0.138229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>31.348300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.222489</td>\n",
       "      <td>0.222489</td>\n",
       "      <td>0.131017</td>\n",
       "      <td>0.222489</td>\n",
       "      <td>0.140710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152500</td>\n",
       "      <td>31.077000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.218912</td>\n",
       "      <td>0.218912</td>\n",
       "      <td>0.120702</td>\n",
       "      <td>0.218912</td>\n",
       "      <td>0.138664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153000</td>\n",
       "      <td>30.888700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.220639</td>\n",
       "      <td>0.220639</td>\n",
       "      <td>0.130225</td>\n",
       "      <td>0.220639</td>\n",
       "      <td>0.141865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153500</td>\n",
       "      <td>31.130500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.220093</td>\n",
       "      <td>0.220093</td>\n",
       "      <td>0.122448</td>\n",
       "      <td>0.220093</td>\n",
       "      <td>0.139674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154000</td>\n",
       "      <td>30.534200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.218590</td>\n",
       "      <td>0.218590</td>\n",
       "      <td>0.126745</td>\n",
       "      <td>0.218590</td>\n",
       "      <td>0.138762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154500</td>\n",
       "      <td>30.652900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.222384</td>\n",
       "      <td>0.222384</td>\n",
       "      <td>0.126328</td>\n",
       "      <td>0.222384</td>\n",
       "      <td>0.141555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>30.918800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.220752</td>\n",
       "      <td>0.220752</td>\n",
       "      <td>0.126625</td>\n",
       "      <td>0.220752</td>\n",
       "      <td>0.141197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155500</td>\n",
       "      <td>30.903000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.222177</td>\n",
       "      <td>0.222177</td>\n",
       "      <td>0.129951</td>\n",
       "      <td>0.222177</td>\n",
       "      <td>0.142502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>30.938600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.223240</td>\n",
       "      <td>0.223240</td>\n",
       "      <td>0.127679</td>\n",
       "      <td>0.223240</td>\n",
       "      <td>0.141922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156500</td>\n",
       "      <td>30.848500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.219302</td>\n",
       "      <td>0.219302</td>\n",
       "      <td>0.128941</td>\n",
       "      <td>0.219302</td>\n",
       "      <td>0.140013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157000</td>\n",
       "      <td>31.064500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.219999</td>\n",
       "      <td>0.219999</td>\n",
       "      <td>0.126452</td>\n",
       "      <td>0.219999</td>\n",
       "      <td>0.141609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157500</td>\n",
       "      <td>31.061900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.221444</td>\n",
       "      <td>0.221444</td>\n",
       "      <td>0.119930</td>\n",
       "      <td>0.221444</td>\n",
       "      <td>0.142024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158000</td>\n",
       "      <td>30.707700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.221138</td>\n",
       "      <td>0.221138</td>\n",
       "      <td>0.124172</td>\n",
       "      <td>0.221138</td>\n",
       "      <td>0.140875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158500</td>\n",
       "      <td>30.917600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.223277</td>\n",
       "      <td>0.223277</td>\n",
       "      <td>0.124497</td>\n",
       "      <td>0.223277</td>\n",
       "      <td>0.142611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159000</td>\n",
       "      <td>30.707700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.222852</td>\n",
       "      <td>0.222852</td>\n",
       "      <td>0.125527</td>\n",
       "      <td>0.222852</td>\n",
       "      <td>0.142091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159500</td>\n",
       "      <td>30.917400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.226667</td>\n",
       "      <td>0.226667</td>\n",
       "      <td>0.129154</td>\n",
       "      <td>0.226667</td>\n",
       "      <td>0.146816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>30.908800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.226953</td>\n",
       "      <td>0.226953</td>\n",
       "      <td>0.125258</td>\n",
       "      <td>0.226953</td>\n",
       "      <td>0.146057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160500</td>\n",
       "      <td>30.477400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.222123</td>\n",
       "      <td>0.222123</td>\n",
       "      <td>0.129620</td>\n",
       "      <td>0.222123</td>\n",
       "      <td>0.143718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161000</td>\n",
       "      <td>30.631400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.223179</td>\n",
       "      <td>0.223179</td>\n",
       "      <td>0.125342</td>\n",
       "      <td>0.223179</td>\n",
       "      <td>0.144540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161500</td>\n",
       "      <td>30.694300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.223599</td>\n",
       "      <td>0.223599</td>\n",
       "      <td>0.127490</td>\n",
       "      <td>0.223599</td>\n",
       "      <td>0.144042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162000</td>\n",
       "      <td>30.813600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.220679</td>\n",
       "      <td>0.220679</td>\n",
       "      <td>0.121128</td>\n",
       "      <td>0.220679</td>\n",
       "      <td>0.141520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162500</td>\n",
       "      <td>30.281400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.227609</td>\n",
       "      <td>0.227609</td>\n",
       "      <td>0.131250</td>\n",
       "      <td>0.227609</td>\n",
       "      <td>0.147310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163000</td>\n",
       "      <td>30.523100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.229643</td>\n",
       "      <td>0.229643</td>\n",
       "      <td>0.135064</td>\n",
       "      <td>0.229643</td>\n",
       "      <td>0.150831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163500</td>\n",
       "      <td>30.688600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.222148</td>\n",
       "      <td>0.222148</td>\n",
       "      <td>0.126462</td>\n",
       "      <td>0.222148</td>\n",
       "      <td>0.142767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164000</td>\n",
       "      <td>30.603100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.225418</td>\n",
       "      <td>0.225418</td>\n",
       "      <td>0.127044</td>\n",
       "      <td>0.225418</td>\n",
       "      <td>0.143912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164500</td>\n",
       "      <td>30.671900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.227013</td>\n",
       "      <td>0.227013</td>\n",
       "      <td>0.131138</td>\n",
       "      <td>0.227013</td>\n",
       "      <td>0.146084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>30.686800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.226510</td>\n",
       "      <td>0.226510</td>\n",
       "      <td>0.134457</td>\n",
       "      <td>0.226510</td>\n",
       "      <td>0.148911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165500</td>\n",
       "      <td>30.798400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.224754</td>\n",
       "      <td>0.224754</td>\n",
       "      <td>0.130564</td>\n",
       "      <td>0.224754</td>\n",
       "      <td>0.144389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166000</td>\n",
       "      <td>30.569400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.227917</td>\n",
       "      <td>0.227917</td>\n",
       "      <td>0.127049</td>\n",
       "      <td>0.227917</td>\n",
       "      <td>0.147642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166500</td>\n",
       "      <td>30.149900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.228845</td>\n",
       "      <td>0.228845</td>\n",
       "      <td>0.130601</td>\n",
       "      <td>0.228845</td>\n",
       "      <td>0.149579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167000</td>\n",
       "      <td>30.896400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.228045</td>\n",
       "      <td>0.228045</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.228045</td>\n",
       "      <td>0.148189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167500</td>\n",
       "      <td>30.757900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.226786</td>\n",
       "      <td>0.226786</td>\n",
       "      <td>0.130995</td>\n",
       "      <td>0.226786</td>\n",
       "      <td>0.147381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>30.548500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.228185</td>\n",
       "      <td>0.228185</td>\n",
       "      <td>0.131999</td>\n",
       "      <td>0.228185</td>\n",
       "      <td>0.147203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168500</td>\n",
       "      <td>30.206000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.231781</td>\n",
       "      <td>0.231781</td>\n",
       "      <td>0.133735</td>\n",
       "      <td>0.231781</td>\n",
       "      <td>0.150728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169000</td>\n",
       "      <td>30.202000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.231581</td>\n",
       "      <td>0.231581</td>\n",
       "      <td>0.134989</td>\n",
       "      <td>0.231581</td>\n",
       "      <td>0.152666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169500</td>\n",
       "      <td>30.638400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.226928</td>\n",
       "      <td>0.226928</td>\n",
       "      <td>0.138704</td>\n",
       "      <td>0.226928</td>\n",
       "      <td>0.147855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>30.367000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.227705</td>\n",
       "      <td>0.227705</td>\n",
       "      <td>0.132684</td>\n",
       "      <td>0.227705</td>\n",
       "      <td>0.147653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170500</td>\n",
       "      <td>30.295200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.226679</td>\n",
       "      <td>0.226679</td>\n",
       "      <td>0.135047</td>\n",
       "      <td>0.226679</td>\n",
       "      <td>0.150330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171000</td>\n",
       "      <td>30.376800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.227139</td>\n",
       "      <td>0.227139</td>\n",
       "      <td>0.132673</td>\n",
       "      <td>0.227139</td>\n",
       "      <td>0.148056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171500</td>\n",
       "      <td>30.275800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.226552</td>\n",
       "      <td>0.226552</td>\n",
       "      <td>0.129498</td>\n",
       "      <td>0.226552</td>\n",
       "      <td>0.147470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172000</td>\n",
       "      <td>30.050000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.230543</td>\n",
       "      <td>0.230543</td>\n",
       "      <td>0.136596</td>\n",
       "      <td>0.230543</td>\n",
       "      <td>0.151182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172500</td>\n",
       "      <td>30.163600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.229891</td>\n",
       "      <td>0.229891</td>\n",
       "      <td>0.136177</td>\n",
       "      <td>0.229891</td>\n",
       "      <td>0.150261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173000</td>\n",
       "      <td>30.360800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.231288</td>\n",
       "      <td>0.231288</td>\n",
       "      <td>0.137873</td>\n",
       "      <td>0.231288</td>\n",
       "      <td>0.151105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173500</td>\n",
       "      <td>30.437800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.229643</td>\n",
       "      <td>0.229643</td>\n",
       "      <td>0.131519</td>\n",
       "      <td>0.229643</td>\n",
       "      <td>0.148773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174000</td>\n",
       "      <td>30.173700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.228820</td>\n",
       "      <td>0.228820</td>\n",
       "      <td>0.129802</td>\n",
       "      <td>0.228820</td>\n",
       "      <td>0.147755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174500</td>\n",
       "      <td>30.081200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.227969</td>\n",
       "      <td>0.227969</td>\n",
       "      <td>0.135243</td>\n",
       "      <td>0.227969</td>\n",
       "      <td>0.148887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175000</td>\n",
       "      <td>29.934600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.229355</td>\n",
       "      <td>0.229355</td>\n",
       "      <td>0.131339</td>\n",
       "      <td>0.229355</td>\n",
       "      <td>0.150560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175500</td>\n",
       "      <td>30.188400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.228447</td>\n",
       "      <td>0.228447</td>\n",
       "      <td>0.132088</td>\n",
       "      <td>0.228447</td>\n",
       "      <td>0.149824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176000</td>\n",
       "      <td>30.600000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.228066</td>\n",
       "      <td>0.228066</td>\n",
       "      <td>0.133511</td>\n",
       "      <td>0.228066</td>\n",
       "      <td>0.148753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176500</td>\n",
       "      <td>30.245100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.231616</td>\n",
       "      <td>0.231616</td>\n",
       "      <td>0.135152</td>\n",
       "      <td>0.231616</td>\n",
       "      <td>0.151623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177000</td>\n",
       "      <td>30.326700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.232467</td>\n",
       "      <td>0.232467</td>\n",
       "      <td>0.140186</td>\n",
       "      <td>0.232467</td>\n",
       "      <td>0.153815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177500</td>\n",
       "      <td>30.106300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.233784</td>\n",
       "      <td>0.233784</td>\n",
       "      <td>0.136337</td>\n",
       "      <td>0.233784</td>\n",
       "      <td>0.153903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178000</td>\n",
       "      <td>29.928200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.229752</td>\n",
       "      <td>0.229752</td>\n",
       "      <td>0.135672</td>\n",
       "      <td>0.229752</td>\n",
       "      <td>0.150551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178500</td>\n",
       "      <td>30.048300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.230035</td>\n",
       "      <td>0.230035</td>\n",
       "      <td>0.137146</td>\n",
       "      <td>0.230035</td>\n",
       "      <td>0.150043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179000</td>\n",
       "      <td>30.215000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.231134</td>\n",
       "      <td>0.231134</td>\n",
       "      <td>0.141285</td>\n",
       "      <td>0.231134</td>\n",
       "      <td>0.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179500</td>\n",
       "      <td>29.945500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.230302</td>\n",
       "      <td>0.230302</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.230302</td>\n",
       "      <td>0.152199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>30.101100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.230258</td>\n",
       "      <td>0.230258</td>\n",
       "      <td>0.132774</td>\n",
       "      <td>0.230258</td>\n",
       "      <td>0.150996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180500</td>\n",
       "      <td>29.916400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.233619</td>\n",
       "      <td>0.233619</td>\n",
       "      <td>0.137423</td>\n",
       "      <td>0.233619</td>\n",
       "      <td>0.154531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181000</td>\n",
       "      <td>29.844300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.230820</td>\n",
       "      <td>0.230820</td>\n",
       "      <td>0.136130</td>\n",
       "      <td>0.230820</td>\n",
       "      <td>0.150261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181500</td>\n",
       "      <td>29.616300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.235211</td>\n",
       "      <td>0.235211</td>\n",
       "      <td>0.139851</td>\n",
       "      <td>0.235211</td>\n",
       "      <td>0.154895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182000</td>\n",
       "      <td>29.903600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.230102</td>\n",
       "      <td>0.230102</td>\n",
       "      <td>0.138473</td>\n",
       "      <td>0.230102</td>\n",
       "      <td>0.150403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182500</td>\n",
       "      <td>29.653200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.232337</td>\n",
       "      <td>0.232337</td>\n",
       "      <td>0.139552</td>\n",
       "      <td>0.232337</td>\n",
       "      <td>0.154165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183000</td>\n",
       "      <td>30.109200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.233505</td>\n",
       "      <td>0.233505</td>\n",
       "      <td>0.138557</td>\n",
       "      <td>0.233505</td>\n",
       "      <td>0.154847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183500</td>\n",
       "      <td>30.279700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.231278</td>\n",
       "      <td>0.231278</td>\n",
       "      <td>0.135771</td>\n",
       "      <td>0.231278</td>\n",
       "      <td>0.152977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184000</td>\n",
       "      <td>29.876700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.229793</td>\n",
       "      <td>0.229793</td>\n",
       "      <td>0.134353</td>\n",
       "      <td>0.229793</td>\n",
       "      <td>0.151167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184500</td>\n",
       "      <td>29.593200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.235174</td>\n",
       "      <td>0.235174</td>\n",
       "      <td>0.140654</td>\n",
       "      <td>0.235174</td>\n",
       "      <td>0.155604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185000</td>\n",
       "      <td>29.707100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.235979</td>\n",
       "      <td>0.235979</td>\n",
       "      <td>0.143882</td>\n",
       "      <td>0.235979</td>\n",
       "      <td>0.157114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185500</td>\n",
       "      <td>29.722500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.234556</td>\n",
       "      <td>0.234556</td>\n",
       "      <td>0.145583</td>\n",
       "      <td>0.234556</td>\n",
       "      <td>0.155275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186000</td>\n",
       "      <td>29.684500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.234395</td>\n",
       "      <td>0.234395</td>\n",
       "      <td>0.143116</td>\n",
       "      <td>0.234395</td>\n",
       "      <td>0.156101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186500</td>\n",
       "      <td>29.677300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.235632</td>\n",
       "      <td>0.235632</td>\n",
       "      <td>0.141197</td>\n",
       "      <td>0.235632</td>\n",
       "      <td>0.157608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187000</td>\n",
       "      <td>29.605800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.234464</td>\n",
       "      <td>0.234464</td>\n",
       "      <td>0.140682</td>\n",
       "      <td>0.234464</td>\n",
       "      <td>0.156084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187500</td>\n",
       "      <td>29.677200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.238452</td>\n",
       "      <td>0.238452</td>\n",
       "      <td>0.140456</td>\n",
       "      <td>0.238452</td>\n",
       "      <td>0.156918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188000</td>\n",
       "      <td>29.413800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.232903</td>\n",
       "      <td>0.232903</td>\n",
       "      <td>0.137054</td>\n",
       "      <td>0.232903</td>\n",
       "      <td>0.152520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188500</td>\n",
       "      <td>29.624500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.236167</td>\n",
       "      <td>0.236167</td>\n",
       "      <td>0.140619</td>\n",
       "      <td>0.236167</td>\n",
       "      <td>0.156508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189000</td>\n",
       "      <td>29.716500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.231697</td>\n",
       "      <td>0.231697</td>\n",
       "      <td>0.142465</td>\n",
       "      <td>0.231697</td>\n",
       "      <td>0.153681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189500</td>\n",
       "      <td>29.677600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.141027</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.156124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>29.474200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.234012</td>\n",
       "      <td>0.234012</td>\n",
       "      <td>0.138663</td>\n",
       "      <td>0.234012</td>\n",
       "      <td>0.156067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190500</td>\n",
       "      <td>29.746200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.232531</td>\n",
       "      <td>0.232531</td>\n",
       "      <td>0.138521</td>\n",
       "      <td>0.232531</td>\n",
       "      <td>0.154379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191000</td>\n",
       "      <td>29.485800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240589</td>\n",
       "      <td>0.240589</td>\n",
       "      <td>0.142585</td>\n",
       "      <td>0.240589</td>\n",
       "      <td>0.159670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191500</td>\n",
       "      <td>29.440800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.235459</td>\n",
       "      <td>0.235459</td>\n",
       "      <td>0.141016</td>\n",
       "      <td>0.235459</td>\n",
       "      <td>0.157249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192000</td>\n",
       "      <td>29.518500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.233048</td>\n",
       "      <td>0.233048</td>\n",
       "      <td>0.139226</td>\n",
       "      <td>0.233048</td>\n",
       "      <td>0.154648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192500</td>\n",
       "      <td>29.521400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.238032</td>\n",
       "      <td>0.238032</td>\n",
       "      <td>0.138111</td>\n",
       "      <td>0.238032</td>\n",
       "      <td>0.158814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193000</td>\n",
       "      <td>29.615800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.238454</td>\n",
       "      <td>0.238454</td>\n",
       "      <td>0.138345</td>\n",
       "      <td>0.238454</td>\n",
       "      <td>0.158454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193500</td>\n",
       "      <td>29.485000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.235682</td>\n",
       "      <td>0.235682</td>\n",
       "      <td>0.138013</td>\n",
       "      <td>0.235682</td>\n",
       "      <td>0.156641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194000</td>\n",
       "      <td>29.348200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.234119</td>\n",
       "      <td>0.234119</td>\n",
       "      <td>0.140619</td>\n",
       "      <td>0.234119</td>\n",
       "      <td>0.156142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194500</td>\n",
       "      <td>29.864900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.233521</td>\n",
       "      <td>0.233521</td>\n",
       "      <td>0.136801</td>\n",
       "      <td>0.233521</td>\n",
       "      <td>0.154209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195000</td>\n",
       "      <td>29.596400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.238530</td>\n",
       "      <td>0.238530</td>\n",
       "      <td>0.139897</td>\n",
       "      <td>0.238530</td>\n",
       "      <td>0.156556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195500</td>\n",
       "      <td>29.330300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.235365</td>\n",
       "      <td>0.235365</td>\n",
       "      <td>0.141324</td>\n",
       "      <td>0.235365</td>\n",
       "      <td>0.156449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196000</td>\n",
       "      <td>29.111300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.237055</td>\n",
       "      <td>0.237055</td>\n",
       "      <td>0.144866</td>\n",
       "      <td>0.237055</td>\n",
       "      <td>0.158129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196500</td>\n",
       "      <td>29.398100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.237259</td>\n",
       "      <td>0.237259</td>\n",
       "      <td>0.142615</td>\n",
       "      <td>0.237259</td>\n",
       "      <td>0.158769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197000</td>\n",
       "      <td>29.371800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.235920</td>\n",
       "      <td>0.235920</td>\n",
       "      <td>0.140859</td>\n",
       "      <td>0.235920</td>\n",
       "      <td>0.156776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197500</td>\n",
       "      <td>29.490300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.236726</td>\n",
       "      <td>0.236726</td>\n",
       "      <td>0.143867</td>\n",
       "      <td>0.236726</td>\n",
       "      <td>0.159067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198000</td>\n",
       "      <td>29.702700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239032</td>\n",
       "      <td>0.239032</td>\n",
       "      <td>0.145611</td>\n",
       "      <td>0.239032</td>\n",
       "      <td>0.159561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198500</td>\n",
       "      <td>29.325800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.235379</td>\n",
       "      <td>0.235379</td>\n",
       "      <td>0.143884</td>\n",
       "      <td>0.235379</td>\n",
       "      <td>0.156671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199000</td>\n",
       "      <td>29.554000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239344</td>\n",
       "      <td>0.239344</td>\n",
       "      <td>0.144433</td>\n",
       "      <td>0.239344</td>\n",
       "      <td>0.159441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199500</td>\n",
       "      <td>29.358300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240482</td>\n",
       "      <td>0.240482</td>\n",
       "      <td>0.143445</td>\n",
       "      <td>0.240482</td>\n",
       "      <td>0.161681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>29.110500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239857</td>\n",
       "      <td>0.239857</td>\n",
       "      <td>0.140687</td>\n",
       "      <td>0.239857</td>\n",
       "      <td>0.158958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200500</td>\n",
       "      <td>29.284500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.238404</td>\n",
       "      <td>0.238404</td>\n",
       "      <td>0.141309</td>\n",
       "      <td>0.238404</td>\n",
       "      <td>0.159128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201000</td>\n",
       "      <td>29.337400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239047</td>\n",
       "      <td>0.239047</td>\n",
       "      <td>0.144848</td>\n",
       "      <td>0.239047</td>\n",
       "      <td>0.159070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201500</td>\n",
       "      <td>29.340400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>0.143938</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>0.159738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202000</td>\n",
       "      <td>29.260400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.234053</td>\n",
       "      <td>0.234053</td>\n",
       "      <td>0.138636</td>\n",
       "      <td>0.234053</td>\n",
       "      <td>0.155297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202500</td>\n",
       "      <td>29.173000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.238946</td>\n",
       "      <td>0.238946</td>\n",
       "      <td>0.144626</td>\n",
       "      <td>0.238946</td>\n",
       "      <td>0.160292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203000</td>\n",
       "      <td>29.155900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>0.142502</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>0.160397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203500</td>\n",
       "      <td>29.367000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.237453</td>\n",
       "      <td>0.237453</td>\n",
       "      <td>0.142966</td>\n",
       "      <td>0.237453</td>\n",
       "      <td>0.158339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204000</td>\n",
       "      <td>29.207200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.236736</td>\n",
       "      <td>0.236736</td>\n",
       "      <td>0.144673</td>\n",
       "      <td>0.236736</td>\n",
       "      <td>0.159506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204500</td>\n",
       "      <td>29.164800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239601</td>\n",
       "      <td>0.239601</td>\n",
       "      <td>0.144993</td>\n",
       "      <td>0.239601</td>\n",
       "      <td>0.161944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205000</td>\n",
       "      <td>29.289600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240219</td>\n",
       "      <td>0.240219</td>\n",
       "      <td>0.145136</td>\n",
       "      <td>0.240219</td>\n",
       "      <td>0.161932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205500</td>\n",
       "      <td>29.416700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.238343</td>\n",
       "      <td>0.238343</td>\n",
       "      <td>0.145059</td>\n",
       "      <td>0.238343</td>\n",
       "      <td>0.159450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206000</td>\n",
       "      <td>28.918200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.236081</td>\n",
       "      <td>0.236081</td>\n",
       "      <td>0.140786</td>\n",
       "      <td>0.236081</td>\n",
       "      <td>0.158056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206500</td>\n",
       "      <td>29.041700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240752</td>\n",
       "      <td>0.240752</td>\n",
       "      <td>0.145972</td>\n",
       "      <td>0.240752</td>\n",
       "      <td>0.163313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207000</td>\n",
       "      <td>28.731300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.241552</td>\n",
       "      <td>0.241552</td>\n",
       "      <td>0.144109</td>\n",
       "      <td>0.241552</td>\n",
       "      <td>0.162171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207500</td>\n",
       "      <td>29.186800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239310</td>\n",
       "      <td>0.239310</td>\n",
       "      <td>0.148173</td>\n",
       "      <td>0.239310</td>\n",
       "      <td>0.161240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208000</td>\n",
       "      <td>29.415700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240432</td>\n",
       "      <td>0.240432</td>\n",
       "      <td>0.146609</td>\n",
       "      <td>0.240432</td>\n",
       "      <td>0.162856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208500</td>\n",
       "      <td>28.975600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240586</td>\n",
       "      <td>0.240586</td>\n",
       "      <td>0.149547</td>\n",
       "      <td>0.240586</td>\n",
       "      <td>0.161743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209000</td>\n",
       "      <td>28.766100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240448</td>\n",
       "      <td>0.240448</td>\n",
       "      <td>0.145622</td>\n",
       "      <td>0.240448</td>\n",
       "      <td>0.160477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209500</td>\n",
       "      <td>29.345500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.243835</td>\n",
       "      <td>0.243835</td>\n",
       "      <td>0.149627</td>\n",
       "      <td>0.243835</td>\n",
       "      <td>0.164671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>29.073600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.242334</td>\n",
       "      <td>0.242334</td>\n",
       "      <td>0.149964</td>\n",
       "      <td>0.242334</td>\n",
       "      <td>0.165564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210500</td>\n",
       "      <td>28.790700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.237419</td>\n",
       "      <td>0.237419</td>\n",
       "      <td>0.145614</td>\n",
       "      <td>0.237419</td>\n",
       "      <td>0.159185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211000</td>\n",
       "      <td>29.012600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239539</td>\n",
       "      <td>0.239539</td>\n",
       "      <td>0.142640</td>\n",
       "      <td>0.239539</td>\n",
       "      <td>0.160894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211500</td>\n",
       "      <td>28.995600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239441</td>\n",
       "      <td>0.239441</td>\n",
       "      <td>0.143449</td>\n",
       "      <td>0.239441</td>\n",
       "      <td>0.161893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212000</td>\n",
       "      <td>29.035000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240753</td>\n",
       "      <td>0.240753</td>\n",
       "      <td>0.146657</td>\n",
       "      <td>0.240753</td>\n",
       "      <td>0.161503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212500</td>\n",
       "      <td>29.037900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239067</td>\n",
       "      <td>0.239067</td>\n",
       "      <td>0.147662</td>\n",
       "      <td>0.239067</td>\n",
       "      <td>0.160932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213000</td>\n",
       "      <td>29.197900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240677</td>\n",
       "      <td>0.240677</td>\n",
       "      <td>0.147345</td>\n",
       "      <td>0.240677</td>\n",
       "      <td>0.161488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213500</td>\n",
       "      <td>29.329600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239654</td>\n",
       "      <td>0.239654</td>\n",
       "      <td>0.138851</td>\n",
       "      <td>0.239654</td>\n",
       "      <td>0.159002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214000</td>\n",
       "      <td>28.745800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239761</td>\n",
       "      <td>0.239761</td>\n",
       "      <td>0.142087</td>\n",
       "      <td>0.239761</td>\n",
       "      <td>0.161759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214500</td>\n",
       "      <td>29.153500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239892</td>\n",
       "      <td>0.239892</td>\n",
       "      <td>0.146761</td>\n",
       "      <td>0.239892</td>\n",
       "      <td>0.162250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215000</td>\n",
       "      <td>29.113300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.241802</td>\n",
       "      <td>0.241802</td>\n",
       "      <td>0.146428</td>\n",
       "      <td>0.241802</td>\n",
       "      <td>0.163809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215500</td>\n",
       "      <td>29.098800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.242523</td>\n",
       "      <td>0.242523</td>\n",
       "      <td>0.144986</td>\n",
       "      <td>0.242523</td>\n",
       "      <td>0.163157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216000</td>\n",
       "      <td>29.018600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239799</td>\n",
       "      <td>0.239799</td>\n",
       "      <td>0.144025</td>\n",
       "      <td>0.239799</td>\n",
       "      <td>0.160937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216500</td>\n",
       "      <td>28.629800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240538</td>\n",
       "      <td>0.240538</td>\n",
       "      <td>0.146540</td>\n",
       "      <td>0.240538</td>\n",
       "      <td>0.161649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217000</td>\n",
       "      <td>28.862100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240477</td>\n",
       "      <td>0.240477</td>\n",
       "      <td>0.147157</td>\n",
       "      <td>0.240477</td>\n",
       "      <td>0.163062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217500</td>\n",
       "      <td>28.930600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.243007</td>\n",
       "      <td>0.243007</td>\n",
       "      <td>0.148470</td>\n",
       "      <td>0.243007</td>\n",
       "      <td>0.164055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218000</td>\n",
       "      <td>28.856700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.242492</td>\n",
       "      <td>0.242492</td>\n",
       "      <td>0.148914</td>\n",
       "      <td>0.242492</td>\n",
       "      <td>0.164279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218500</td>\n",
       "      <td>28.967000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245157</td>\n",
       "      <td>0.245157</td>\n",
       "      <td>0.146238</td>\n",
       "      <td>0.245157</td>\n",
       "      <td>0.165396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219000</td>\n",
       "      <td>28.745100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.241439</td>\n",
       "      <td>0.241439</td>\n",
       "      <td>0.150105</td>\n",
       "      <td>0.241439</td>\n",
       "      <td>0.163271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219500</td>\n",
       "      <td>28.991400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245851</td>\n",
       "      <td>0.245851</td>\n",
       "      <td>0.147950</td>\n",
       "      <td>0.245851</td>\n",
       "      <td>0.167667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>28.885800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.243090</td>\n",
       "      <td>0.243090</td>\n",
       "      <td>0.144192</td>\n",
       "      <td>0.243090</td>\n",
       "      <td>0.163474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220500</td>\n",
       "      <td>28.686800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.242744</td>\n",
       "      <td>0.242744</td>\n",
       "      <td>0.146320</td>\n",
       "      <td>0.242744</td>\n",
       "      <td>0.163285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221000</td>\n",
       "      <td>28.788500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.242216</td>\n",
       "      <td>0.242216</td>\n",
       "      <td>0.146781</td>\n",
       "      <td>0.242216</td>\n",
       "      <td>0.163939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221500</td>\n",
       "      <td>28.988600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.241855</td>\n",
       "      <td>0.241855</td>\n",
       "      <td>0.146607</td>\n",
       "      <td>0.241855</td>\n",
       "      <td>0.163943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222000</td>\n",
       "      <td>28.700800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.242372</td>\n",
       "      <td>0.242372</td>\n",
       "      <td>0.147580</td>\n",
       "      <td>0.242372</td>\n",
       "      <td>0.162963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222500</td>\n",
       "      <td>28.604500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.243042</td>\n",
       "      <td>0.243042</td>\n",
       "      <td>0.149172</td>\n",
       "      <td>0.243042</td>\n",
       "      <td>0.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223000</td>\n",
       "      <td>28.721000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.244761</td>\n",
       "      <td>0.244761</td>\n",
       "      <td>0.146776</td>\n",
       "      <td>0.244761</td>\n",
       "      <td>0.166351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223500</td>\n",
       "      <td>28.828900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.244018</td>\n",
       "      <td>0.244018</td>\n",
       "      <td>0.150054</td>\n",
       "      <td>0.244018</td>\n",
       "      <td>0.164848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224000</td>\n",
       "      <td>28.206900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.242596</td>\n",
       "      <td>0.242596</td>\n",
       "      <td>0.147787</td>\n",
       "      <td>0.242596</td>\n",
       "      <td>0.164007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224500</td>\n",
       "      <td>28.886000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240386</td>\n",
       "      <td>0.240386</td>\n",
       "      <td>0.146626</td>\n",
       "      <td>0.240386</td>\n",
       "      <td>0.161977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225000</td>\n",
       "      <td>28.589800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.244566</td>\n",
       "      <td>0.244566</td>\n",
       "      <td>0.144613</td>\n",
       "      <td>0.244566</td>\n",
       "      <td>0.165496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225500</td>\n",
       "      <td>28.795400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245582</td>\n",
       "      <td>0.245582</td>\n",
       "      <td>0.150468</td>\n",
       "      <td>0.245582</td>\n",
       "      <td>0.166065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226000</td>\n",
       "      <td>28.646000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240938</td>\n",
       "      <td>0.240938</td>\n",
       "      <td>0.145138</td>\n",
       "      <td>0.240938</td>\n",
       "      <td>0.161573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226500</td>\n",
       "      <td>28.590100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.241069</td>\n",
       "      <td>0.241069</td>\n",
       "      <td>0.148962</td>\n",
       "      <td>0.241069</td>\n",
       "      <td>0.163250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227000</td>\n",
       "      <td>28.793800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.239656</td>\n",
       "      <td>0.239656</td>\n",
       "      <td>0.143863</td>\n",
       "      <td>0.239656</td>\n",
       "      <td>0.161773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227500</td>\n",
       "      <td>28.760300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.244658</td>\n",
       "      <td>0.244658</td>\n",
       "      <td>0.149340</td>\n",
       "      <td>0.244658</td>\n",
       "      <td>0.165695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228000</td>\n",
       "      <td>28.839500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.242794</td>\n",
       "      <td>0.242794</td>\n",
       "      <td>0.146418</td>\n",
       "      <td>0.242794</td>\n",
       "      <td>0.164352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228500</td>\n",
       "      <td>28.809100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.242144</td>\n",
       "      <td>0.242144</td>\n",
       "      <td>0.145947</td>\n",
       "      <td>0.242144</td>\n",
       "      <td>0.164128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229000</td>\n",
       "      <td>28.759900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246324</td>\n",
       "      <td>0.246324</td>\n",
       "      <td>0.148228</td>\n",
       "      <td>0.246324</td>\n",
       "      <td>0.167182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229500</td>\n",
       "      <td>28.703900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246398</td>\n",
       "      <td>0.246398</td>\n",
       "      <td>0.150463</td>\n",
       "      <td>0.246398</td>\n",
       "      <td>0.167252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230000</td>\n",
       "      <td>28.520500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.242826</td>\n",
       "      <td>0.242826</td>\n",
       "      <td>0.147221</td>\n",
       "      <td>0.242826</td>\n",
       "      <td>0.164184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230500</td>\n",
       "      <td>28.421400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245855</td>\n",
       "      <td>0.245855</td>\n",
       "      <td>0.148005</td>\n",
       "      <td>0.245855</td>\n",
       "      <td>0.166324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231000</td>\n",
       "      <td>28.626400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245913</td>\n",
       "      <td>0.245913</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>0.245913</td>\n",
       "      <td>0.166023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231500</td>\n",
       "      <td>28.559600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240211</td>\n",
       "      <td>0.240211</td>\n",
       "      <td>0.145999</td>\n",
       "      <td>0.240211</td>\n",
       "      <td>0.162755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232000</td>\n",
       "      <td>28.604800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.243972</td>\n",
       "      <td>0.243972</td>\n",
       "      <td>0.147584</td>\n",
       "      <td>0.243972</td>\n",
       "      <td>0.165473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232500</td>\n",
       "      <td>27.991000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.242712</td>\n",
       "      <td>0.242712</td>\n",
       "      <td>0.144429</td>\n",
       "      <td>0.242712</td>\n",
       "      <td>0.164343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233000</td>\n",
       "      <td>28.322100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240395</td>\n",
       "      <td>0.240395</td>\n",
       "      <td>0.141846</td>\n",
       "      <td>0.240395</td>\n",
       "      <td>0.160570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233500</td>\n",
       "      <td>28.550700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240389</td>\n",
       "      <td>0.240389</td>\n",
       "      <td>0.150176</td>\n",
       "      <td>0.240389</td>\n",
       "      <td>0.163526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234000</td>\n",
       "      <td>28.486800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.244836</td>\n",
       "      <td>0.244836</td>\n",
       "      <td>0.152824</td>\n",
       "      <td>0.244836</td>\n",
       "      <td>0.167610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234500</td>\n",
       "      <td>28.333100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.252001</td>\n",
       "      <td>0.252001</td>\n",
       "      <td>0.150890</td>\n",
       "      <td>0.252001</td>\n",
       "      <td>0.170859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235000</td>\n",
       "      <td>28.208400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246895</td>\n",
       "      <td>0.246895</td>\n",
       "      <td>0.150840</td>\n",
       "      <td>0.246895</td>\n",
       "      <td>0.168048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235500</td>\n",
       "      <td>29.036300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246472</td>\n",
       "      <td>0.246472</td>\n",
       "      <td>0.147477</td>\n",
       "      <td>0.246472</td>\n",
       "      <td>0.168058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236000</td>\n",
       "      <td>28.696300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.243837</td>\n",
       "      <td>0.243837</td>\n",
       "      <td>0.150957</td>\n",
       "      <td>0.243837</td>\n",
       "      <td>0.166473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236500</td>\n",
       "      <td>28.549100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.244433</td>\n",
       "      <td>0.244433</td>\n",
       "      <td>0.148663</td>\n",
       "      <td>0.244433</td>\n",
       "      <td>0.166258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237000</td>\n",
       "      <td>29.104400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245690</td>\n",
       "      <td>0.245690</td>\n",
       "      <td>0.151999</td>\n",
       "      <td>0.245690</td>\n",
       "      <td>0.167257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237500</td>\n",
       "      <td>28.234100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.244094</td>\n",
       "      <td>0.244094</td>\n",
       "      <td>0.148408</td>\n",
       "      <td>0.244094</td>\n",
       "      <td>0.165423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238000</td>\n",
       "      <td>28.507500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246224</td>\n",
       "      <td>0.246224</td>\n",
       "      <td>0.151378</td>\n",
       "      <td>0.246224</td>\n",
       "      <td>0.167213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238500</td>\n",
       "      <td>28.503600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246923</td>\n",
       "      <td>0.246923</td>\n",
       "      <td>0.149890</td>\n",
       "      <td>0.246923</td>\n",
       "      <td>0.168424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239000</td>\n",
       "      <td>28.786200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.244459</td>\n",
       "      <td>0.244459</td>\n",
       "      <td>0.149968</td>\n",
       "      <td>0.244459</td>\n",
       "      <td>0.165952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239500</td>\n",
       "      <td>28.515900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.244283</td>\n",
       "      <td>0.244283</td>\n",
       "      <td>0.147657</td>\n",
       "      <td>0.244283</td>\n",
       "      <td>0.166353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240000</td>\n",
       "      <td>28.227600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246042</td>\n",
       "      <td>0.246042</td>\n",
       "      <td>0.148662</td>\n",
       "      <td>0.246042</td>\n",
       "      <td>0.168097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240500</td>\n",
       "      <td>28.088700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246147</td>\n",
       "      <td>0.246147</td>\n",
       "      <td>0.149988</td>\n",
       "      <td>0.246147</td>\n",
       "      <td>0.167583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241000</td>\n",
       "      <td>28.069100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247215</td>\n",
       "      <td>0.247215</td>\n",
       "      <td>0.151764</td>\n",
       "      <td>0.247215</td>\n",
       "      <td>0.169690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241500</td>\n",
       "      <td>27.969100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246466</td>\n",
       "      <td>0.246466</td>\n",
       "      <td>0.151848</td>\n",
       "      <td>0.246466</td>\n",
       "      <td>0.168642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242000</td>\n",
       "      <td>28.421800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247221</td>\n",
       "      <td>0.247221</td>\n",
       "      <td>0.149408</td>\n",
       "      <td>0.247221</td>\n",
       "      <td>0.167998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242500</td>\n",
       "      <td>28.067600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.250585</td>\n",
       "      <td>0.250585</td>\n",
       "      <td>0.153451</td>\n",
       "      <td>0.250585</td>\n",
       "      <td>0.171188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243000</td>\n",
       "      <td>28.478700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246933</td>\n",
       "      <td>0.246933</td>\n",
       "      <td>0.150964</td>\n",
       "      <td>0.246933</td>\n",
       "      <td>0.168640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243500</td>\n",
       "      <td>28.494200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247012</td>\n",
       "      <td>0.247012</td>\n",
       "      <td>0.152038</td>\n",
       "      <td>0.247012</td>\n",
       "      <td>0.169472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244000</td>\n",
       "      <td>28.549100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245210</td>\n",
       "      <td>0.245210</td>\n",
       "      <td>0.150320</td>\n",
       "      <td>0.245210</td>\n",
       "      <td>0.167063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244500</td>\n",
       "      <td>28.402800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246520</td>\n",
       "      <td>0.246520</td>\n",
       "      <td>0.151656</td>\n",
       "      <td>0.246520</td>\n",
       "      <td>0.168939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245000</td>\n",
       "      <td>28.315000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248376</td>\n",
       "      <td>0.248376</td>\n",
       "      <td>0.149533</td>\n",
       "      <td>0.248376</td>\n",
       "      <td>0.169596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245500</td>\n",
       "      <td>28.379900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249242</td>\n",
       "      <td>0.249242</td>\n",
       "      <td>0.156521</td>\n",
       "      <td>0.249242</td>\n",
       "      <td>0.171059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246000</td>\n",
       "      <td>28.088400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245943</td>\n",
       "      <td>0.245943</td>\n",
       "      <td>0.148561</td>\n",
       "      <td>0.245943</td>\n",
       "      <td>0.167778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246500</td>\n",
       "      <td>28.251400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246213</td>\n",
       "      <td>0.246213</td>\n",
       "      <td>0.149183</td>\n",
       "      <td>0.246213</td>\n",
       "      <td>0.168549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247000</td>\n",
       "      <td>28.458300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246790</td>\n",
       "      <td>0.246790</td>\n",
       "      <td>0.152529</td>\n",
       "      <td>0.246790</td>\n",
       "      <td>0.169456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247500</td>\n",
       "      <td>28.181900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.243794</td>\n",
       "      <td>0.243794</td>\n",
       "      <td>0.144818</td>\n",
       "      <td>0.243794</td>\n",
       "      <td>0.165038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248000</td>\n",
       "      <td>28.273800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.240644</td>\n",
       "      <td>0.240644</td>\n",
       "      <td>0.145526</td>\n",
       "      <td>0.240644</td>\n",
       "      <td>0.162589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248500</td>\n",
       "      <td>28.335300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249291</td>\n",
       "      <td>0.249291</td>\n",
       "      <td>0.153190</td>\n",
       "      <td>0.249291</td>\n",
       "      <td>0.170914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249000</td>\n",
       "      <td>28.393400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245002</td>\n",
       "      <td>0.245002</td>\n",
       "      <td>0.148423</td>\n",
       "      <td>0.245002</td>\n",
       "      <td>0.166935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249500</td>\n",
       "      <td>28.468800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247212</td>\n",
       "      <td>0.247212</td>\n",
       "      <td>0.150850</td>\n",
       "      <td>0.247212</td>\n",
       "      <td>0.168180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250000</td>\n",
       "      <td>28.362600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248324</td>\n",
       "      <td>0.248324</td>\n",
       "      <td>0.151501</td>\n",
       "      <td>0.248324</td>\n",
       "      <td>0.170065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250500</td>\n",
       "      <td>28.242100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.242668</td>\n",
       "      <td>0.242668</td>\n",
       "      <td>0.149141</td>\n",
       "      <td>0.242668</td>\n",
       "      <td>0.165205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251000</td>\n",
       "      <td>28.109200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245533</td>\n",
       "      <td>0.245533</td>\n",
       "      <td>0.150896</td>\n",
       "      <td>0.245533</td>\n",
       "      <td>0.166709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251500</td>\n",
       "      <td>28.270300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249550</td>\n",
       "      <td>0.249550</td>\n",
       "      <td>0.151479</td>\n",
       "      <td>0.249550</td>\n",
       "      <td>0.170405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252000</td>\n",
       "      <td>27.980400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248509</td>\n",
       "      <td>0.248509</td>\n",
       "      <td>0.150194</td>\n",
       "      <td>0.248509</td>\n",
       "      <td>0.169355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252500</td>\n",
       "      <td>28.189100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.244929</td>\n",
       "      <td>0.244929</td>\n",
       "      <td>0.150813</td>\n",
       "      <td>0.244929</td>\n",
       "      <td>0.167458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253000</td>\n",
       "      <td>28.266500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245646</td>\n",
       "      <td>0.245646</td>\n",
       "      <td>0.149322</td>\n",
       "      <td>0.245646</td>\n",
       "      <td>0.166285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253500</td>\n",
       "      <td>28.021200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245619</td>\n",
       "      <td>0.245619</td>\n",
       "      <td>0.148557</td>\n",
       "      <td>0.245619</td>\n",
       "      <td>0.167034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254000</td>\n",
       "      <td>28.368000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249682</td>\n",
       "      <td>0.249682</td>\n",
       "      <td>0.151224</td>\n",
       "      <td>0.249682</td>\n",
       "      <td>0.170627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254500</td>\n",
       "      <td>28.387600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245358</td>\n",
       "      <td>0.245358</td>\n",
       "      <td>0.149553</td>\n",
       "      <td>0.245358</td>\n",
       "      <td>0.167928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255000</td>\n",
       "      <td>28.266500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249488</td>\n",
       "      <td>0.249488</td>\n",
       "      <td>0.158406</td>\n",
       "      <td>0.249488</td>\n",
       "      <td>0.171896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255500</td>\n",
       "      <td>28.173000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245685</td>\n",
       "      <td>0.245685</td>\n",
       "      <td>0.143528</td>\n",
       "      <td>0.245685</td>\n",
       "      <td>0.167278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256000</td>\n",
       "      <td>28.266500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245582</td>\n",
       "      <td>0.245582</td>\n",
       "      <td>0.148141</td>\n",
       "      <td>0.245582</td>\n",
       "      <td>0.167001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256500</td>\n",
       "      <td>28.028300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248561</td>\n",
       "      <td>0.248561</td>\n",
       "      <td>0.154792</td>\n",
       "      <td>0.248561</td>\n",
       "      <td>0.170139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257000</td>\n",
       "      <td>28.280700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249840</td>\n",
       "      <td>0.249840</td>\n",
       "      <td>0.150846</td>\n",
       "      <td>0.249840</td>\n",
       "      <td>0.169001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257500</td>\n",
       "      <td>27.923900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245615</td>\n",
       "      <td>0.245615</td>\n",
       "      <td>0.151062</td>\n",
       "      <td>0.245615</td>\n",
       "      <td>0.168198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258000</td>\n",
       "      <td>28.225100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249276</td>\n",
       "      <td>0.249276</td>\n",
       "      <td>0.152262</td>\n",
       "      <td>0.249276</td>\n",
       "      <td>0.171105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258500</td>\n",
       "      <td>28.269900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249645</td>\n",
       "      <td>0.249645</td>\n",
       "      <td>0.150904</td>\n",
       "      <td>0.249645</td>\n",
       "      <td>0.169699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259000</td>\n",
       "      <td>28.464400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.250636</td>\n",
       "      <td>0.250636</td>\n",
       "      <td>0.150969</td>\n",
       "      <td>0.250636</td>\n",
       "      <td>0.169766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259500</td>\n",
       "      <td>28.437100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246735</td>\n",
       "      <td>0.246735</td>\n",
       "      <td>0.155281</td>\n",
       "      <td>0.246735</td>\n",
       "      <td>0.169237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260000</td>\n",
       "      <td>28.545000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246973</td>\n",
       "      <td>0.246973</td>\n",
       "      <td>0.149070</td>\n",
       "      <td>0.246973</td>\n",
       "      <td>0.167617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260500</td>\n",
       "      <td>27.940400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.251631</td>\n",
       "      <td>0.251631</td>\n",
       "      <td>0.150482</td>\n",
       "      <td>0.251631</td>\n",
       "      <td>0.172402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261000</td>\n",
       "      <td>28.000100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248982</td>\n",
       "      <td>0.248982</td>\n",
       "      <td>0.153141</td>\n",
       "      <td>0.248982</td>\n",
       "      <td>0.170192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261500</td>\n",
       "      <td>28.004800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.250093</td>\n",
       "      <td>0.250093</td>\n",
       "      <td>0.151638</td>\n",
       "      <td>0.250093</td>\n",
       "      <td>0.171262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262000</td>\n",
       "      <td>28.052600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247537</td>\n",
       "      <td>0.247537</td>\n",
       "      <td>0.151732</td>\n",
       "      <td>0.247537</td>\n",
       "      <td>0.169513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262500</td>\n",
       "      <td>27.950300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248438</td>\n",
       "      <td>0.248438</td>\n",
       "      <td>0.153121</td>\n",
       "      <td>0.248438</td>\n",
       "      <td>0.170204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263000</td>\n",
       "      <td>27.888800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247278</td>\n",
       "      <td>0.247278</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.247278</td>\n",
       "      <td>0.168177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263500</td>\n",
       "      <td>28.265100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247695</td>\n",
       "      <td>0.247695</td>\n",
       "      <td>0.154681</td>\n",
       "      <td>0.247695</td>\n",
       "      <td>0.170703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264000</td>\n",
       "      <td>28.161500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249174</td>\n",
       "      <td>0.249174</td>\n",
       "      <td>0.151678</td>\n",
       "      <td>0.249174</td>\n",
       "      <td>0.170915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264500</td>\n",
       "      <td>28.242300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248124</td>\n",
       "      <td>0.248124</td>\n",
       "      <td>0.154160</td>\n",
       "      <td>0.248124</td>\n",
       "      <td>0.171121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265000</td>\n",
       "      <td>28.519400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248567</td>\n",
       "      <td>0.248567</td>\n",
       "      <td>0.149892</td>\n",
       "      <td>0.248567</td>\n",
       "      <td>0.168653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265500</td>\n",
       "      <td>28.251200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246382</td>\n",
       "      <td>0.246382</td>\n",
       "      <td>0.149843</td>\n",
       "      <td>0.246382</td>\n",
       "      <td>0.168519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266000</td>\n",
       "      <td>28.387000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249441</td>\n",
       "      <td>0.249441</td>\n",
       "      <td>0.150484</td>\n",
       "      <td>0.249441</td>\n",
       "      <td>0.170743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266500</td>\n",
       "      <td>28.150600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.251597</td>\n",
       "      <td>0.251597</td>\n",
       "      <td>0.154824</td>\n",
       "      <td>0.251597</td>\n",
       "      <td>0.172573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267000</td>\n",
       "      <td>27.952900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.251317</td>\n",
       "      <td>0.251317</td>\n",
       "      <td>0.150343</td>\n",
       "      <td>0.251317</td>\n",
       "      <td>0.171347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267500</td>\n",
       "      <td>28.102300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248808</td>\n",
       "      <td>0.248808</td>\n",
       "      <td>0.157286</td>\n",
       "      <td>0.248808</td>\n",
       "      <td>0.171185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268000</td>\n",
       "      <td>28.523300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246025</td>\n",
       "      <td>0.246025</td>\n",
       "      <td>0.147040</td>\n",
       "      <td>0.246025</td>\n",
       "      <td>0.166036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268500</td>\n",
       "      <td>28.032400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247779</td>\n",
       "      <td>0.247779</td>\n",
       "      <td>0.150388</td>\n",
       "      <td>0.247779</td>\n",
       "      <td>0.168819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269000</td>\n",
       "      <td>28.248800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.251312</td>\n",
       "      <td>0.251312</td>\n",
       "      <td>0.154563</td>\n",
       "      <td>0.251312</td>\n",
       "      <td>0.172973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269500</td>\n",
       "      <td>27.928400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249197</td>\n",
       "      <td>0.249197</td>\n",
       "      <td>0.152749</td>\n",
       "      <td>0.249197</td>\n",
       "      <td>0.169946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270000</td>\n",
       "      <td>27.922300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249397</td>\n",
       "      <td>0.249397</td>\n",
       "      <td>0.152466</td>\n",
       "      <td>0.249397</td>\n",
       "      <td>0.170705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270500</td>\n",
       "      <td>28.115300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.251872</td>\n",
       "      <td>0.251872</td>\n",
       "      <td>0.153727</td>\n",
       "      <td>0.251872</td>\n",
       "      <td>0.173017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271000</td>\n",
       "      <td>27.947200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.250072</td>\n",
       "      <td>0.250072</td>\n",
       "      <td>0.150796</td>\n",
       "      <td>0.250072</td>\n",
       "      <td>0.169729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271500</td>\n",
       "      <td>28.163600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246739</td>\n",
       "      <td>0.246739</td>\n",
       "      <td>0.152818</td>\n",
       "      <td>0.246739</td>\n",
       "      <td>0.168780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272000</td>\n",
       "      <td>28.055100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.243036</td>\n",
       "      <td>0.243036</td>\n",
       "      <td>0.149293</td>\n",
       "      <td>0.243036</td>\n",
       "      <td>0.165253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272500</td>\n",
       "      <td>28.280900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.251699</td>\n",
       "      <td>0.251699</td>\n",
       "      <td>0.155402</td>\n",
       "      <td>0.251699</td>\n",
       "      <td>0.172625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273000</td>\n",
       "      <td>27.675300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247816</td>\n",
       "      <td>0.247816</td>\n",
       "      <td>0.148501</td>\n",
       "      <td>0.247816</td>\n",
       "      <td>0.168613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273500</td>\n",
       "      <td>27.901900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.251656</td>\n",
       "      <td>0.251656</td>\n",
       "      <td>0.155952</td>\n",
       "      <td>0.251656</td>\n",
       "      <td>0.172938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274000</td>\n",
       "      <td>28.149900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248878</td>\n",
       "      <td>0.248878</td>\n",
       "      <td>0.150819</td>\n",
       "      <td>0.248878</td>\n",
       "      <td>0.169703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274500</td>\n",
       "      <td>28.258800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249451</td>\n",
       "      <td>0.249451</td>\n",
       "      <td>0.151653</td>\n",
       "      <td>0.249451</td>\n",
       "      <td>0.169697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275000</td>\n",
       "      <td>28.109900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245378</td>\n",
       "      <td>0.245378</td>\n",
       "      <td>0.150192</td>\n",
       "      <td>0.245378</td>\n",
       "      <td>0.168555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275500</td>\n",
       "      <td>28.074500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247544</td>\n",
       "      <td>0.247544</td>\n",
       "      <td>0.152581</td>\n",
       "      <td>0.247544</td>\n",
       "      <td>0.170115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276000</td>\n",
       "      <td>28.091500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.252615</td>\n",
       "      <td>0.252615</td>\n",
       "      <td>0.152817</td>\n",
       "      <td>0.252615</td>\n",
       "      <td>0.173094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276500</td>\n",
       "      <td>27.892800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248818</td>\n",
       "      <td>0.248818</td>\n",
       "      <td>0.150845</td>\n",
       "      <td>0.248818</td>\n",
       "      <td>0.169712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277000</td>\n",
       "      <td>27.886100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.246497</td>\n",
       "      <td>0.246497</td>\n",
       "      <td>0.149505</td>\n",
       "      <td>0.246497</td>\n",
       "      <td>0.168346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277500</td>\n",
       "      <td>27.721200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.250631</td>\n",
       "      <td>0.250631</td>\n",
       "      <td>0.153741</td>\n",
       "      <td>0.250631</td>\n",
       "      <td>0.172349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278000</td>\n",
       "      <td>28.288700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.252149</td>\n",
       "      <td>0.252149</td>\n",
       "      <td>0.153795</td>\n",
       "      <td>0.252149</td>\n",
       "      <td>0.173016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278500</td>\n",
       "      <td>27.752200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249676</td>\n",
       "      <td>0.249676</td>\n",
       "      <td>0.152917</td>\n",
       "      <td>0.249676</td>\n",
       "      <td>0.170351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279000</td>\n",
       "      <td>28.107800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247031</td>\n",
       "      <td>0.247031</td>\n",
       "      <td>0.150072</td>\n",
       "      <td>0.247031</td>\n",
       "      <td>0.168167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279500</td>\n",
       "      <td>28.235000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249537</td>\n",
       "      <td>0.249537</td>\n",
       "      <td>0.152294</td>\n",
       "      <td>0.249537</td>\n",
       "      <td>0.170110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280000</td>\n",
       "      <td>28.185200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.252207</td>\n",
       "      <td>0.252207</td>\n",
       "      <td>0.155658</td>\n",
       "      <td>0.252207</td>\n",
       "      <td>0.174053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280500</td>\n",
       "      <td>27.965800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248876</td>\n",
       "      <td>0.248876</td>\n",
       "      <td>0.148801</td>\n",
       "      <td>0.248876</td>\n",
       "      <td>0.169922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281000</td>\n",
       "      <td>28.326800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249311</td>\n",
       "      <td>0.249311</td>\n",
       "      <td>0.153246</td>\n",
       "      <td>0.249311</td>\n",
       "      <td>0.171434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281500</td>\n",
       "      <td>27.913700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249617</td>\n",
       "      <td>0.249617</td>\n",
       "      <td>0.150584</td>\n",
       "      <td>0.249617</td>\n",
       "      <td>0.170525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282000</td>\n",
       "      <td>28.085200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.159432</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.175466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282500</td>\n",
       "      <td>28.193900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247836</td>\n",
       "      <td>0.247836</td>\n",
       "      <td>0.150321</td>\n",
       "      <td>0.247836</td>\n",
       "      <td>0.169612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283000</td>\n",
       "      <td>28.064100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.250246</td>\n",
       "      <td>0.250246</td>\n",
       "      <td>0.150151</td>\n",
       "      <td>0.250246</td>\n",
       "      <td>0.170653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283500</td>\n",
       "      <td>28.116200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.250268</td>\n",
       "      <td>0.250268</td>\n",
       "      <td>0.151743</td>\n",
       "      <td>0.250268</td>\n",
       "      <td>0.171783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284000</td>\n",
       "      <td>28.007400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.252077</td>\n",
       "      <td>0.252077</td>\n",
       "      <td>0.152530</td>\n",
       "      <td>0.252077</td>\n",
       "      <td>0.172313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284500</td>\n",
       "      <td>27.848600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247705</td>\n",
       "      <td>0.247705</td>\n",
       "      <td>0.152401</td>\n",
       "      <td>0.247705</td>\n",
       "      <td>0.168566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285000</td>\n",
       "      <td>28.159400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249384</td>\n",
       "      <td>0.249384</td>\n",
       "      <td>0.150857</td>\n",
       "      <td>0.249384</td>\n",
       "      <td>0.170338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285500</td>\n",
       "      <td>27.970600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.245586</td>\n",
       "      <td>0.245586</td>\n",
       "      <td>0.155025</td>\n",
       "      <td>0.245586</td>\n",
       "      <td>0.168437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286000</td>\n",
       "      <td>28.072400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249089</td>\n",
       "      <td>0.249089</td>\n",
       "      <td>0.155418</td>\n",
       "      <td>0.249089</td>\n",
       "      <td>0.170990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286500</td>\n",
       "      <td>28.292100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249100</td>\n",
       "      <td>0.249100</td>\n",
       "      <td>0.154246</td>\n",
       "      <td>0.249100</td>\n",
       "      <td>0.171171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287000</td>\n",
       "      <td>27.958800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.244154</td>\n",
       "      <td>0.244154</td>\n",
       "      <td>0.151216</td>\n",
       "      <td>0.244154</td>\n",
       "      <td>0.166322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287500</td>\n",
       "      <td>27.969800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.253083</td>\n",
       "      <td>0.253083</td>\n",
       "      <td>0.157412</td>\n",
       "      <td>0.253083</td>\n",
       "      <td>0.175019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288000</td>\n",
       "      <td>27.621200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.252203</td>\n",
       "      <td>0.252203</td>\n",
       "      <td>0.153332</td>\n",
       "      <td>0.252203</td>\n",
       "      <td>0.173053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288500</td>\n",
       "      <td>27.814900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.248036</td>\n",
       "      <td>0.248036</td>\n",
       "      <td>0.150945</td>\n",
       "      <td>0.248036</td>\n",
       "      <td>0.169795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289000</td>\n",
       "      <td>27.897700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.254342</td>\n",
       "      <td>0.254342</td>\n",
       "      <td>0.159140</td>\n",
       "      <td>0.254342</td>\n",
       "      <td>0.175672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289500</td>\n",
       "      <td>27.955400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.247100</td>\n",
       "      <td>0.247100</td>\n",
       "      <td>0.152296</td>\n",
       "      <td>0.247100</td>\n",
       "      <td>0.169583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290000</td>\n",
       "      <td>27.950000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.254067</td>\n",
       "      <td>0.254067</td>\n",
       "      <td>0.159296</td>\n",
       "      <td>0.254067</td>\n",
       "      <td>0.175179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290500</td>\n",
       "      <td>27.877100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.254492</td>\n",
       "      <td>0.254492</td>\n",
       "      <td>0.158219</td>\n",
       "      <td>0.254492</td>\n",
       "      <td>0.175164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291000</td>\n",
       "      <td>27.887800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.155920</td>\n",
       "      <td>0.249604</td>\n",
       "      <td>0.171354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=291258, training_loss=33.77319262173432, metrics={'train_runtime': 194854.3061, 'train_samples_per_second': 11.958, 'train_steps_per_second': 1.495, 'total_flos': 3.088751822507336e+17, 'train_loss': 33.77319262173432, 'epoch': 2.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abf37afd-a278-423d-83e6-939727142231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1031, 2516, 1033, 7570, 7849, 2271, 13091, 7946, 1031, 1013, 2516, 1033, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 7570, 7849, 2271, 13091, 7946, 1010, 2124, 2004, 1996, 2647, 27940, 2030, 2691, 27940, 1010, 2003, 1037, 2427, 1997, 22544, 27940, 2013, 1996, 2789, 4448, 4153, 1010, 7095, 2712, 1998, 3033, 1997, 1996, 2304, 2712, 1012, 2009, 2003, 4876, 3141, 2000, 1996, 2137, 27940, 1010, 1044, 1012, 2137, 2271, 1012, 2009, 2089, 4982, 2000, 1037, 3091, 1997, 3438, 4642, 1006, 2484, 1999, 1007, 1998, 1037, 3742, 1997, 1020, 18857, 1006, 2410, 6053, 1007, 1010, 1998, 6468, 1037, 19194, 3940, 1997, 10702, 1012, 1999, 2166, 1010, 1996, 27940, 2015, 2024, 2630, 1010, 2069, 3352, 1000, 27940, 2417, 1000, 2006, 8434, 1012, 15100, 5158, 1999, 1996, 2621, 1010, 5155, 6763, 2029, 2024, 3344, 2011, 1996, 3801, 2005, 2039, 2000, 1037, 2095, 2077, 11300, 2075, 2046, 24000, 25009, 9673, 1012, 7570, 7849, 2271, 13091, 7946, 2003, 1037, 3811, 19593, 2098, 2833, 1010, 1998, 2003, 4235, 3236, 2478, 27940, 18911, 1010, 3262, 2105, 1996, 2329, 14195, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1031, 2516, 1033, 6412, 1031, 1013, 2516, 1033, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 7570, 7849, 2271, 13091, 7946, 2003, 1037, 2312, 19116, 10732, 2319, 1010, 2007, 1037, 2303, 3091, 2039, 2000, 3438, 13935, 1006, 2484, 1999, 1007, 1998, 15243, 2039, 2000, 1019, 1020, 18857, 1006, 2340, 2410, 6053, 1007, 1010, 2348, 1996, 27940, 2015, 3236, 1999, 27940, 18911, 2024, 2788, 2603, 4229, 4642, 1006, 1023, 2321, 1999, 1007, 2146, 1998, 17042, 1014, 1030, 1012, 1030, 1021, 1016, 1030, 1012, 1030, 1016, 4705, 1006, 1015, 1030, 1012, 1030, 1019, 1018, 1030, 1012, 1030, 1023, 6053, 1007, 1012, 2066, 2060, 19116, 10732, 6962, 1010, 27940, 2015, 2031, 1037, 2524, 4654, 2891, 11705, 18903, 2078, 2029, 2027, 2442, 8328, 1999, 2344, 2000, 4982, 1010, 1999, 1037, 2832, 2170, 14925, 5149, 6190, 1006, 9587, 11314, 2075, 1007, 1012, 2023, 2089, 5258, 2195, 2335, 1037, 2095, 2005, 2402, 27940, 2015, 1010, 2021, 17913, 2000, 2320, 2296, 1015, 1016, 2086, 2005, 3469, 4176, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 2034, 3940, 1997, 23976, 3695, 22925, 2003, 4273, 2007, 1037, 2312, 1010, 2004, 24335, 12589, 2389, 3940, 1997, 10702, 1012, 1996, 3469, 2028, 2003, 1996, 1000, 10188, 2121, 1000, 1010, 1998, 2038, 8352, 7293, 16308, 2109, 2005, 14527, 8336, 1025, 1996, 2060, 2003, 1996, 1000, 16343, 1000, 1010, 2029, 2038, 4629, 5110, 7926, 1010, 1998, 2003, 2109, 2005, 3173, 2030, 13311, 1996, 8336, 1012, 2788, 1010, 1996, 2187, 15020, 2003, 1996, 10188, 2121, 1010, 1998, 1996, 2157, 2003, 1996, 16343, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 4654, 2891, 11705, 18903, 2078, 2003, 3227, 2630, 2682, 1010, 2007, 7516, 2008, 5317, 2229, 3401, 1010, 1998, 3756, 2917, 1012, 1996, 2417, 6120, 3378, 2007, 27940, 2015, 2069, 3544, 2044, 8434, 1012, 2023, 5158, 2138, 1010, 1999, 2166, 1010, 1996, 2417, 28815, 2004, 2696, 18684, 3372, 10606, 2003, 5391, 2000, 1037, 5250, 3375, 1010, 2021, 1996, 3375, 2003, 3714, 2039, 2011, 1996, 3684, 1997, 8434, 1010, 8287, 1996, 2417, 28815, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'special_tokens_mask': [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_and_tokenize(dataset['validation'][0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100dd749-387f-494d-8579-cc8a3ac0f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./models/distilbert_wikitext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bf604-2be3-4d42-8767-ba01e1b6b359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
