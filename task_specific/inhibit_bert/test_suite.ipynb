{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48b64872-82a3-4c80-aaaa-cf14d888a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import PretrainedConfig\n",
    "from iDistilbert import iMultiHeadSelfAttention  # Replace 'your_module' with the actual module name\n",
    "\n",
    "def test_imultihead_attention():\n",
    "    # Set up a configuration\n",
    "    config = PretrainedConfig()\n",
    "    config.n_heads = 8\n",
    "    config.dim = 512\n",
    "    config.attention_dropout = 0.1\n",
    "    config.distance_metric = 'manhattan_distance'\n",
    "    config.activation_function = 'relu'\n",
    "    config.signed_inhibitor = True\n",
    "    config.alpha = 0\n",
    "    config.center = False\n",
    "\n",
    "    # Initialize the attention module\n",
    "    attention = iMultiHeadSelfAttention(config)\n",
    "\n",
    "    # Create sample inputs\n",
    "    batch_size = 2\n",
    "    seq_length = 10\n",
    "    dim = config.dim\n",
    "    query = torch.randn(batch_size, seq_length, dim)\n",
    "    key = torch.randn(batch_size, seq_length, dim)\n",
    "    value = torch.randn(batch_size, seq_length, dim)\n",
    "    mask = torch.ones(batch_size, seq_length)\n",
    "\n",
    "    def shape(x):\n",
    "        return x.view(batch_size, -1, config.n_heads, dim // config.n_heads).transpose(1, 2)\n",
    "\n",
    "    q = shape(attention.q_lin(query))\n",
    "    k = shape(attention.k_lin(key))\n",
    "    v = shape(attention.v_lin(value))\n",
    "\n",
    "    # Test forward pass\n",
    "    output = attention(query, key, value, mask)\n",
    "\n",
    "    # Basic checks\n",
    "    assert isinstance(output, tuple), \"Output should be a tuple\"\n",
    "    assert len(output) == 1, \"Output tuple should have length 1 when output_attentions is False\"\n",
    "    context = output[0]\n",
    "    assert context.shape == (batch_size, seq_length, dim), f\"Expected shape {(batch_size, seq_length, dim)}, got {context.shape}\"\n",
    "\n",
    "    # Test with output_attentions=True\n",
    "    output_with_attentions = attention(query, key, value, mask, output_attentions=True)\n",
    "    assert len(output_with_attentions) == 2, \"Output tuple should have length 2 when output_attentions is True\"\n",
    "    context, attentions = output_with_attentions\n",
    "    assert attentions.shape == (batch_size, config.n_heads, seq_length, seq_length), \\\n",
    "        f\"Expected attention shape {(batch_size, config.n_heads, seq_length, seq_length)}, got {attentions.shape}\"\n",
    "\n",
    "    # Test Manhattan distance calculation\n",
    "    q = attention.q_lin(query).view(batch_size, seq_length, config.n_heads, -1).transpose(1, 2)\n",
    "    k = attention.k_lin(key).view(batch_size, seq_length, config.n_heads, -1).transpose(1, 2)\n",
    "    manhattan_dist = torch.cdist(q, k, p=1) / math.sqrt(dim // config.n_heads)\n",
    "    print(\"manhattan:\", manhattan_dist, \"attention:\", attentions)\n",
    "    assert torch.allclose(manhattan_dist, attentions, atol=1e-4), \"Manhattan distance calculation is incorrect\"\n",
    "\n",
    "    # Test alpha shift\n",
    "    if config.alpha > 0:\n",
    "        assert torch.all(attentions <= manhattan_dist - config.alpha), \"Alpha shift is not applied correctly\"\n",
    "\n",
    "    # Test centering\n",
    "    if config.center: \n",
    "        centered_mean = torch.mean(attentions, dim=-1, keepdim=True)\n",
    "        assert torch.allclose(centered_mean, torch.zeros_like(centered_mean), atol=1e-5), \"Centering is not applied correctly\"\n",
    "\n",
    "    # Test signed inhibitor\n",
    "    v = attention.v_lin(value).view(batch_size, seq_length, config.n_heads, -1).transpose(1, 2)\n",
    "    v_t = v.transpose(-1, -2)\n",
    "    pos_v = torch.nn.functional.relu(v_t)\n",
    "    neg_v = -torch.nn.functional.relu(-v_t)\n",
    "    v_sum = torch.sum(v, dim=-2, keepdim=True)\n",
    "    dist1 = torch.cdist(attentions, pos_v, p=1)\n",
    "    dist2 = torch.cdist(attentions, -neg_v, p=1)\n",
    "    expected_context = 0.5 * (v_sum + dist1 - dist2)\n",
    "    expected_context = expected_context.transpose(1, 2).contiguous().view(batch_size, seq_length, dim)\n",
    "    expected_context = attention.out_lin(expected_context)\n",
    "    assert torch.allclose(context, expected_context, atol=1e-4), \"Signed inhibitor calculation is incorrect\"\n",
    "\n",
    "    print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06cbf33c-e2b5-4693-a42a-49e7d8b42126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manhattan: tensor([[[[6.0772, 5.2858, 6.0801,  ..., 5.6185, 5.7522, 5.2338],\n",
      "          [5.9760, 5.2336, 5.8107,  ..., 4.9800, 4.5297, 5.7370],\n",
      "          [6.4357, 4.8345, 5.5585,  ..., 4.7197, 5.5091, 5.6305],\n",
      "          ...,\n",
      "          [5.0999, 4.7994, 5.2691,  ..., 5.1425, 4.5558, 5.3494],\n",
      "          [5.5670, 4.3111, 4.8021,  ..., 4.8845, 5.0862, 5.7810],\n",
      "          [5.2251, 5.9267, 5.5819,  ..., 6.2157, 5.5792, 6.9216]],\n",
      "\n",
      "         [[5.0905, 6.3175, 4.7354,  ..., 5.7119, 5.8099, 6.0322],\n",
      "          [5.1588, 5.2408, 5.3311,  ..., 4.9445, 5.7702, 5.6704],\n",
      "          [4.6449, 5.7244, 4.7422,  ..., 4.3380, 6.0153, 4.6274],\n",
      "          ...,\n",
      "          [5.0343, 4.9869, 4.5839,  ..., 5.5890, 4.8580, 5.2585],\n",
      "          [4.1412, 5.3241, 4.8187,  ..., 4.7849, 5.0593, 5.2501],\n",
      "          [4.7986, 5.5238, 5.3539,  ..., 5.0259, 5.8376, 5.3515]],\n",
      "\n",
      "         [[6.4377, 5.1312, 5.6195,  ..., 5.1679, 5.4747, 5.8638],\n",
      "          [4.8503, 4.1265, 4.8440,  ..., 4.6355, 4.6717, 4.9571],\n",
      "          [5.1298, 4.5253, 4.7262,  ..., 5.9238, 4.6740, 4.9678],\n",
      "          ...,\n",
      "          [5.5860, 4.6063, 5.6944,  ..., 5.4060, 5.0827, 5.2546],\n",
      "          [4.5355, 4.5651, 5.2732,  ..., 5.2310, 5.0079, 5.1495],\n",
      "          [6.0274, 5.6472, 5.1508,  ..., 5.3390, 5.8938, 6.0995]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[5.4173, 5.7692, 5.6943,  ..., 6.0428, 5.8780, 4.6583],\n",
      "          [4.9710, 5.1215, 4.8121,  ..., 5.2940, 4.4882, 4.3752],\n",
      "          [4.0913, 5.3173, 5.3086,  ..., 5.5156, 4.6370, 4.4962],\n",
      "          ...,\n",
      "          [4.8667, 4.7935, 5.8483,  ..., 5.7628, 4.9645, 4.9389],\n",
      "          [5.1891, 4.8772, 4.7568,  ..., 5.2084, 4.9926, 4.6817],\n",
      "          [5.3697, 4.9480, 5.6293,  ..., 5.1453, 5.3491, 4.8114]],\n",
      "\n",
      "         [[5.7446, 6.7258, 5.1282,  ..., 5.9890, 5.7556, 6.3851],\n",
      "          [4.5374, 5.2428, 5.8761,  ..., 5.0607, 4.8853, 5.0126],\n",
      "          [5.1641, 5.8234, 5.2277,  ..., 5.3974, 5.1907, 5.8840],\n",
      "          ...,\n",
      "          [4.4560, 6.5578, 5.8503,  ..., 5.8571, 5.3529, 5.9263],\n",
      "          [4.0788, 6.2744, 4.9209,  ..., 4.9290, 4.5810, 4.9204],\n",
      "          [5.2865, 6.8501, 5.8910,  ..., 5.2384, 5.5580, 5.4005]],\n",
      "\n",
      "         [[5.3750, 5.4914, 4.7682,  ..., 5.0588, 4.9356, 4.8461],\n",
      "          [5.2503, 5.2033, 4.1158,  ..., 5.0245, 5.0480, 4.6942],\n",
      "          [4.8206, 4.6681, 4.5605,  ..., 5.9509, 4.8793, 4.9401],\n",
      "          ...,\n",
      "          [5.0470, 5.2018, 4.2096,  ..., 5.1563, 5.4520, 4.8081],\n",
      "          [5.2329, 5.2753, 5.2071,  ..., 5.1328, 4.9846, 4.7249],\n",
      "          [5.7775, 4.9103, 5.5678,  ..., 5.8559, 6.1713, 5.1380]]],\n",
      "\n",
      "\n",
      "        [[[5.9708, 4.8429, 5.5565,  ..., 4.8369, 5.1732, 5.7403],\n",
      "          [6.4252, 4.8746, 5.9309,  ..., 4.8247, 5.7480, 6.1516],\n",
      "          [5.2553, 4.9777, 5.1815,  ..., 5.1557, 5.3914, 6.0104],\n",
      "          ...,\n",
      "          [4.4783, 4.5724, 4.8686,  ..., 4.1568, 5.2651, 5.3579],\n",
      "          [4.5518, 5.3739, 5.6487,  ..., 4.5514, 5.4494, 5.7540],\n",
      "          [4.8944, 4.8554, 5.2348,  ..., 4.9770, 5.5652, 6.0890]],\n",
      "\n",
      "         [[5.0070, 5.4974, 4.2909,  ..., 4.5448, 4.1419, 4.8697],\n",
      "          [4.8662, 4.2380, 4.7496,  ..., 4.2943, 5.0952, 5.8446],\n",
      "          [5.5278, 5.4405, 6.0386,  ..., 5.6895, 5.9334, 6.8561],\n",
      "          ...,\n",
      "          [5.3857, 4.7030, 5.4411,  ..., 5.0786, 4.9398, 5.7283],\n",
      "          [6.3065, 5.0902, 6.5187,  ..., 6.4521, 5.9433, 5.8042],\n",
      "          [5.0877, 5.0820, 4.8675,  ..., 4.3552, 4.3740, 4.4210]],\n",
      "\n",
      "         [[6.1588, 5.2871, 5.7825,  ..., 6.0247, 5.0512, 4.6768],\n",
      "          [5.4251, 4.8829, 5.6455,  ..., 5.6620, 5.1155, 4.5585],\n",
      "          [4.8313, 5.5677, 5.5700,  ..., 5.9017, 4.5614, 4.8526],\n",
      "          ...,\n",
      "          [5.0445, 4.9057, 5.4579,  ..., 4.7636, 4.1591, 4.7954],\n",
      "          [5.3694, 6.0783, 5.5727,  ..., 5.4625, 5.6195, 5.5430],\n",
      "          [6.0795, 5.9873, 5.9177,  ..., 5.5735, 5.3069, 5.5520]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[5.1221, 5.4364, 4.7854,  ..., 4.8154, 4.6095, 5.1319],\n",
      "          [5.6714, 6.0980, 4.9588,  ..., 6.2287, 5.7756, 5.7163],\n",
      "          [5.8159, 5.9186, 5.4864,  ..., 4.8515, 5.3994, 5.1062],\n",
      "          ...,\n",
      "          [5.6554, 6.0357, 5.5671,  ..., 5.5319, 5.3458, 5.1464],\n",
      "          [5.4280, 5.1938, 5.8909,  ..., 5.0419, 5.2640, 5.3292],\n",
      "          [5.3872, 5.2859, 4.7734,  ..., 5.8432, 5.0350, 4.9552]],\n",
      "\n",
      "         [[5.1795, 4.8012, 5.3845,  ..., 4.8119, 5.6656, 4.9904],\n",
      "          [5.2103, 5.8210, 5.9901,  ..., 5.2852, 5.6099, 5.5287],\n",
      "          [5.1291, 5.4623, 4.9095,  ..., 4.9277, 5.0993, 6.1067],\n",
      "          ...,\n",
      "          [5.5580, 5.4398, 6.0775,  ..., 4.7346, 4.3993, 5.6061],\n",
      "          [6.0936, 5.5314, 5.9529,  ..., 5.0390, 5.6113, 5.9672],\n",
      "          [4.9460, 4.6146, 5.6900,  ..., 4.6261, 4.8601, 5.1241]],\n",
      "\n",
      "         [[5.5276, 4.8683, 5.0651,  ..., 5.8010, 5.2663, 5.2554],\n",
      "          [5.7723, 6.3882, 5.7100,  ..., 5.2044, 5.5834, 5.4160],\n",
      "          [6.2241, 5.6309, 5.7155,  ..., 5.9411, 5.6604, 5.6392],\n",
      "          ...,\n",
      "          [5.3493, 5.7407, 4.5745,  ..., 5.8177, 5.4493, 5.4439],\n",
      "          [5.6791, 6.2609, 5.5165,  ..., 5.8038, 5.5197, 6.0849],\n",
      "          [5.4014, 5.4177, 5.5276,  ..., 5.7054, 5.4247, 5.6723]]]],\n",
      "       grad_fn=<DivBackward0>) attention: tensor([[[[6.0772, 5.2858, 6.0801,  ..., 5.6185, 5.7522, 5.2338],\n",
      "          [5.9760, 5.2336, 5.8107,  ..., 4.9800, 4.5297, 5.7370],\n",
      "          [6.4357, 4.8345, 5.5585,  ..., 4.7197, 5.5091, 5.6305],\n",
      "          ...,\n",
      "          [5.0999, 4.7994, 5.2691,  ..., 5.1425, 4.5558, 5.3494],\n",
      "          [5.5670, 4.3111, 4.8021,  ..., 4.8845, 5.0862, 5.7810],\n",
      "          [5.2251, 5.9267, 5.5819,  ..., 6.2157, 5.5792, 6.9216]],\n",
      "\n",
      "         [[5.0905, 6.3175, 4.7354,  ..., 5.7119, 5.8099, 6.0322],\n",
      "          [5.1588, 5.2408, 5.3311,  ..., 4.9445, 5.7702, 5.6704],\n",
      "          [4.6449, 5.7244, 4.7422,  ..., 4.3380, 6.0153, 4.6274],\n",
      "          ...,\n",
      "          [5.0343, 4.9869, 4.5839,  ..., 5.5890, 4.8580, 5.2585],\n",
      "          [4.1412, 5.3241, 4.8187,  ..., 4.7849, 5.0593, 5.2501],\n",
      "          [4.7986, 5.5238, 5.3539,  ..., 5.0259, 5.8376, 5.3515]],\n",
      "\n",
      "         [[6.4377, 5.1312, 5.6195,  ..., 5.1679, 5.4747, 5.8638],\n",
      "          [4.8503, 4.1265, 4.8440,  ..., 4.6355, 4.6717, 4.9571],\n",
      "          [5.1298, 4.5253, 4.7262,  ..., 5.9238, 4.6740, 4.9678],\n",
      "          ...,\n",
      "          [5.5860, 4.6063, 5.6944,  ..., 5.4060, 5.0827, 5.2546],\n",
      "          [4.5355, 4.5651, 5.2732,  ..., 5.2310, 5.0079, 5.1495],\n",
      "          [6.0274, 5.6472, 5.1508,  ..., 5.3390, 5.8938, 6.0995]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[5.4173, 5.7692, 5.6943,  ..., 6.0428, 5.8780, 4.6583],\n",
      "          [4.9710, 5.1215, 4.8121,  ..., 5.2940, 4.4882, 4.3752],\n",
      "          [4.0913, 5.3173, 5.3086,  ..., 5.5156, 4.6370, 4.4962],\n",
      "          ...,\n",
      "          [4.8667, 4.7935, 5.8483,  ..., 5.7628, 4.9645, 4.9389],\n",
      "          [5.1891, 4.8772, 4.7568,  ..., 5.2084, 4.9926, 4.6817],\n",
      "          [5.3697, 4.9480, 5.6293,  ..., 5.1453, 5.3491, 4.8114]],\n",
      "\n",
      "         [[5.7446, 6.7258, 5.1282,  ..., 5.9890, 5.7556, 6.3851],\n",
      "          [4.5374, 5.2428, 5.8761,  ..., 5.0607, 4.8853, 5.0126],\n",
      "          [5.1641, 5.8234, 5.2277,  ..., 5.3974, 5.1907, 5.8840],\n",
      "          ...,\n",
      "          [4.4560, 6.5578, 5.8503,  ..., 5.8571, 5.3529, 5.9263],\n",
      "          [4.0788, 6.2744, 4.9209,  ..., 4.9290, 4.5810, 4.9204],\n",
      "          [5.2865, 6.8501, 5.8910,  ..., 5.2384, 5.5580, 5.4005]],\n",
      "\n",
      "         [[5.3750, 5.4914, 4.7682,  ..., 5.0588, 4.9356, 4.8461],\n",
      "          [5.2503, 5.2033, 4.1158,  ..., 5.0245, 5.0480, 4.6942],\n",
      "          [4.8206, 4.6681, 4.5605,  ..., 5.9509, 4.8793, 4.9401],\n",
      "          ...,\n",
      "          [5.0470, 5.2018, 4.2096,  ..., 5.1563, 5.4520, 4.8081],\n",
      "          [5.2329, 5.2753, 5.2071,  ..., 5.1328, 4.9846, 4.7249],\n",
      "          [5.7775, 4.9103, 5.5678,  ..., 5.8559, 6.1713, 5.1380]]],\n",
      "\n",
      "\n",
      "        [[[5.9708, 4.8429, 5.5565,  ..., 4.8369, 5.1732, 5.7403],\n",
      "          [6.4252, 4.8746, 5.9309,  ..., 4.8247, 5.7480, 6.1516],\n",
      "          [5.2553, 4.9777, 5.1815,  ..., 5.1557, 5.3914, 6.0104],\n",
      "          ...,\n",
      "          [4.4783, 4.5724, 4.8686,  ..., 4.1568, 5.2651, 5.3579],\n",
      "          [4.5518, 5.3739, 5.6487,  ..., 4.5514, 5.4494, 5.7540],\n",
      "          [4.8944, 4.8554, 5.2348,  ..., 4.9770, 5.5652, 6.0890]],\n",
      "\n",
      "         [[5.0070, 5.4974, 4.2909,  ..., 4.5448, 4.1419, 4.8697],\n",
      "          [4.8662, 4.2380, 4.7496,  ..., 4.2943, 5.0952, 5.8446],\n",
      "          [5.5278, 5.4405, 6.0386,  ..., 5.6895, 5.9334, 6.8561],\n",
      "          ...,\n",
      "          [5.3857, 4.7030, 5.4411,  ..., 5.0786, 4.9398, 5.7283],\n",
      "          [6.3065, 5.0902, 6.5187,  ..., 6.4521, 5.9433, 5.8042],\n",
      "          [5.0877, 5.0820, 4.8675,  ..., 4.3552, 4.3740, 4.4210]],\n",
      "\n",
      "         [[6.1588, 5.2871, 5.7825,  ..., 6.0247, 5.0512, 4.6768],\n",
      "          [5.4251, 4.8829, 5.6455,  ..., 5.6620, 5.1155, 4.5585],\n",
      "          [4.8313, 5.5677, 5.5700,  ..., 5.9017, 4.5614, 4.8526],\n",
      "          ...,\n",
      "          [5.0445, 4.9057, 5.4579,  ..., 4.7636, 4.1591, 4.7954],\n",
      "          [5.3694, 6.0783, 5.5727,  ..., 5.4625, 5.6195, 5.5430],\n",
      "          [6.0795, 5.9873, 5.9177,  ..., 5.5735, 5.3069, 5.5520]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[5.1221, 5.4364, 4.7854,  ..., 4.8154, 4.6095, 5.1319],\n",
      "          [5.6714, 6.0980, 4.9588,  ..., 6.2287, 5.7756, 5.7163],\n",
      "          [5.8159, 5.9186, 5.4864,  ..., 4.8515, 5.3994, 5.1062],\n",
      "          ...,\n",
      "          [5.6554, 6.0357, 5.5671,  ..., 5.5319, 5.3458, 5.1464],\n",
      "          [5.4280, 5.1938, 5.8909,  ..., 5.0419, 5.2640, 5.3292],\n",
      "          [5.3872, 5.2859, 4.7734,  ..., 5.8432, 5.0350, 4.9552]],\n",
      "\n",
      "         [[5.1795, 4.8012, 5.3845,  ..., 4.8119, 5.6656, 4.9904],\n",
      "          [5.2103, 5.8210, 5.9901,  ..., 5.2852, 5.6099, 5.5287],\n",
      "          [5.1291, 5.4623, 4.9095,  ..., 4.9277, 5.0993, 6.1067],\n",
      "          ...,\n",
      "          [5.5580, 5.4398, 6.0775,  ..., 4.7346, 4.3993, 5.6061],\n",
      "          [6.0936, 5.5314, 5.9529,  ..., 5.0390, 5.6113, 5.9672],\n",
      "          [4.9460, 4.6146, 5.6900,  ..., 4.6261, 4.8601, 5.1241]],\n",
      "\n",
      "         [[5.5276, 4.8683, 5.0651,  ..., 5.8010, 5.2663, 5.2554],\n",
      "          [5.7723, 6.3882, 5.7100,  ..., 5.2044, 5.5834, 5.4160],\n",
      "          [6.2241, 5.6309, 5.7155,  ..., 5.9411, 5.6604, 5.6392],\n",
      "          ...,\n",
      "          [5.3493, 5.7407, 4.5745,  ..., 5.8177, 5.4493, 5.4439],\n",
      "          [5.6791, 6.2609, 5.5165,  ..., 5.8038, 5.5197, 6.0849],\n",
      "          [5.4014, 5.4177, 5.5276,  ..., 5.7054, 5.4247, 5.6723]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "test_imultihead_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a9da324-b002-4596-a515-a0a10e088120",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m]) \n\u001b[0;32m----> 2\u001b[0m \u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(context)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([1,2,3]) \n",
    "context *= 0.5\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265c05eb-2b17-4924-aebc-cb3e71a413c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 3.25k/3.25k [00:00<00:00, 6.86MB/s]\n",
      "Downloading readme: 100%|██████████| 18.5k/18.5k [00:00<00:00, 14.4MB/s]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for bookcorpus/bookcorpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bookcorpus/bookcorpus.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.18G/1.18G [00:35<00:00, 33.0MB/s] \n",
      "Generating train split: 100%|██████████| 74004228/74004228 [11:24<00:00, 108149.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"bookcorpus/bookcorpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ccbc24d-560c-4b55-a409-c442f94fefe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cdist with torch.float32:\n",
      "Input X shape: torch.Size([3, 5]), dtype: torch.float32\n",
      "Input Y shape: torch.Size([2, 5]), dtype: torch.float32\n",
      "Output shape: torch.Size([3, 2]), dtype: torch.float32\n",
      "Output:\n",
      "tensor([[2.6425, 2.3108],\n",
      "        [4.1014, 2.9532],\n",
      "        [3.4673, 2.9974]])\n",
      "\n",
      "Testing cdist with torch.int8:\n",
      "Input X shape: torch.Size([3, 5]), dtype: torch.int8\n",
      "Input Y shape: torch.Size([2, 5]), dtype: torch.int8\n",
      "Output shape: torch.Size([3, 2]), dtype: torch.float32\n",
      "Output:\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n",
      "\n",
      "Testing cdist with int4:\n",
      "Input X shape: torch.Size([3, 5]), dtype: torch.float32\n",
      "Input Y shape: torch.Size([2, 5]), dtype: torch.float32\n",
      "Output shape: torch.Size([3, 2]), dtype: torch.float32\n",
      "Output:\n",
      "tensor([[20.4939, 15.4919],\n",
      "        [27.9643, 15.2971],\n",
      "        [17.3494, 13.4536]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def test_cdist(dtype, simulated_dtype=None):\n",
    "    # Create random tensors\n",
    "    x = torch.randn(3, 5)\n",
    "    y = torch.randn(2, 5)\n",
    "    \n",
    "    if simulated_dtype == 'int4':\n",
    "        # Simulate int4 by clamping values and rounding\n",
    "        x = torch.clamp(x * 7, -8, 7).round()\n",
    "        y = torch.clamp(y * 7, -8, 7).round()\n",
    "    \n",
    "    # Convert to specified dtype\n",
    "    x = x.to(dtype)\n",
    "    y = y.to(dtype)\n",
    "    \n",
    "    # For integer types, we need to convert to float for cdist\n",
    "    if dtype in [torch.int8, torch.int16, torch.int32, torch.int64]:\n",
    "        x_float = x.float()\n",
    "        y_float = y.float()\n",
    "    else:\n",
    "        x_float = x\n",
    "        y_float = y\n",
    "    \n",
    "    # Compute pairwise distance\n",
    "    dist = torch.cdist(x_float, y_float)\n",
    "    \n",
    "    print(f\"Testing cdist with {simulated_dtype or dtype}:\")\n",
    "    print(f\"Input X shape: {x.shape}, dtype: {x.dtype}\")\n",
    "    print(f\"Input Y shape: {y.shape}, dtype: {y.dtype}\")\n",
    "    print(f\"Output shape: {dist.shape}, dtype: {dist.dtype}\")\n",
    "    print(f\"Output:\\n{dist}\\n\")\n",
    "\n",
    "# Test with different dtypes\n",
    "test_cdist(torch.float32)  # fp32\n",
    "#test_cdist(torch.float16)  # fp16\n",
    "test_cdist(torch.int8)     # int8\n",
    "test_cdist(torch.float32, 'int4')  # simulated int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f06f3f-36a4-42e1-ae0e-412212a38ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
