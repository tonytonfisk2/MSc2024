{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d260f6-9f8c-405f-925a-ae5fef490f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4e2c0-dd64-4cc0-9643-b846abf0db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimModel(nn.Module):\n",
    "    def __init__(self, input_size=1, latent_size=1, is_inhibitor=False, signed_inhibitor=True):\n",
    "        super(SimModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.is_inhibitor = is_inhibitor\n",
    "        self.signed_inhibitor = signed_inhibitor\n",
    "        \n",
    "        self.q_lin = nn.Linear(input_size, latent_size)\n",
    "        self.k_lin = nn.Linear(input_size, latent_size)\n",
    "        self.v_lin = nn.Linear(input_size, latent_size)\n",
    "        self.out_lin = nn.Linear(latent_size, latent_size)\n",
    "        \n",
    "    def compute_signed_inhibitor(self, scores: torch.Tensor, v: torch.Tensor, v_t: torch.Tensor) -> torch.Tensor:\n",
    "        pos_v = F.relu(v_t)  # Positive part of V\n",
    "        neg_v = -F.relu(-v_t)  # Negative part of V\n",
    "        v_sum = torch.sum(v, dim=-2, keepdim=True)  # Sum over keys\n",
    "        dist1 = torch.cdist(scores, pos_v, p=1)  # Distance to positive V\n",
    "        dist2 = torch.cdist(scores, neg_v, p=1)  # Distance to negative V\n",
    "        context = 0.5 * (v_sum + dist1 - dist2)\n",
    "        return context\n",
    "\n",
    "    def compute_unsigned_inhibitor(self, scores: torch.Tensor, v: torch.Tensor, v_t: torch.Tensor) -> torch.Tensor:\n",
    "        v_sum = torch.sum(v, dim=-2, keepdim=True)  # Sum over keys\n",
    "        z_sum = torch.sum(scores, dim=-1, keepdim=True)  # Sum of scores\n",
    "        abs_diff = torch.cdist(scores, v_t, p=1)  # Absolute differences\n",
    "        context = 0.5 * (v_sum - z_sum + abs_diff)\n",
    "        return context\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        q = self.q_lin(x)  # (B, S, latent_size)\n",
    "        k = self.k_lin(x)  # (B, S, latent_size)\n",
    "        v = self.v_lin(x)  # (B, S, latent_size)\n",
    "        \n",
    "        if self.is_inhibitor:\n",
    "\n",
    "            scores = torch.cdist(q, k, p=1) / self.latent_size  # (B, S, S)\n",
    "             \n",
    "            scores = F.relu(scores - scores.mean(dim=-1, keepdim=True))  # (B, S, S)           \n",
    "\n",
    "            dropout = nn.Dropout(p=0.1)  \n",
    "            mask_dropped = dropout(mask.to(scores.dtype))  # (B, S)\n",
    "            \n",
    "            mask_expanded = (mask_dropped == 0).unsqueeze(1)  # (B, 1, S)\n",
    "            \n",
    "            M = torch.max(torch.abs(v)) + 1\n",
    "            scores = scores.masked_fill(mask_expanded, M)  # (B, S, S)\n",
    "            \n",
    "           \n",
    "            v_t = v.transpose(-1, -2)  # (B, latent_size, S)\n",
    "            \n",
    "            if self.signed_inhibitor:\n",
    "                context = self.compute_signed_inhibitor(scores, v, v_t)  # (B, S, latent_size)\n",
    "            else:\n",
    "                context = self.compute_unsigned_inhibitor(scores, v, v_t)  # (B, S, latent_size)\n",
    "        else:\n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) / (self.latent_size ** 0.5)  # (B, S, S)\n",
    "            \n",
    "            mask_expanded = (mask == 0).unsqueeze(1)  # (B, 1, S)\n",
    "            \n",
    "            scores = scores.masked_fill(mask_expanded, float('-inf'))  # (B, S, S)\n",
    "            \n",
    "            weights = F.softmax(scores, dim=-1)  # (B, S, S)\n",
    "            \n",
    "\n",
    "            weights = F.dropout(weights, p=0.1, training=self.training)  # (B, S, S)\n",
    "            \n",
    "\n",
    "            context = torch.matmul(weights, v)  \n",
    "        \n",
    "        h = self.out_lin(context)  \n",
    "        \n",
    "        return h, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9e8e9-4705-4526-bd55-77562f16a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineEmbeddingLossWrapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineEmbeddingLossWrapper, self).__init__()\n",
    "        self.cosine_loss = nn.CosineEmbeddingLoss()\n",
    "\n",
    "    def forward(self, student_context, target_context):\n",
    "        # Reshape to (batch_size * seq_length, latent_size)\n",
    "        student = student_context.view(-1, student_context.size(-1))\n",
    "        target = target_context.view(-1, target_context.size(-1))\n",
    "        \n",
    "        # Labels: 1 for similar, since we want to align them\n",
    "        labels = torch.ones(student.size(0)).to(student_context.device)\n",
    "        \n",
    "        loss = self.cosine_loss(student, target, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ba9bb-95e0-48c9-a050-143daaa44eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, seq_length=10, input_size=1, latent_size=1):\n",
    "        super(ToyDataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        # Random input data\n",
    "        self.x = torch.randn(num_samples, seq_length, input_size)\n",
    "        \n",
    "        # Define target context as some function of x, for example, sum over the sequence\n",
    "        # Here, we'll just use random target contexts for demonstration\n",
    "        self.target_h = torch.randn(num_samples, seq_length, latent_size)\n",
    "        \n",
    "        # Attention mask: 1 for valid tokens, 0 for padding\n",
    "        # For simplicity, assume all sequences are fully valid\n",
    "        self.mask = torch.ones(num_samples, seq_length)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.mask[idx], self.target_h[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54ec7e-0449-49cd-b3d9-f697c01dfecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1\n",
    "latent_size = 1  # Increased for better representation\n",
    "is_inhibitor = True\n",
    "signed_inhibitor = True\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Initialize the model\n",
    "model = SimModel(\n",
    "    input_size=input_size,\n",
    "    latent_size=latent_size,\n",
    "    is_inhibitor=is_inhibitor,\n",
    "    signed_inhibitor=signed_inhibitor\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model.to(device)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = CosineEmbeddingLossWrapper()\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "dataset = ToyDataset(num_samples=1000, seq_length=10, input_size=input_size, latent_size=latent_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (x, mask, target_h) in enumerate(dataloader):\n",
    "        x = x.to(device)  # (B, S, input_size)\n",
    "        mask = mask.to(device)  # (B, S)\n",
    "        target_h = target_h.to(device)  # (B, S, latent_size)\n",
    "        \n",
    "        # Forward pass\n",
    "        h, scores = model(x, mask)  # h: (B, S, latent_size)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(h, target_h)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2534f2-4a54-4d43-a91b-2c2f6caf886f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot-product attention output:\n",
      "[[ 3.59216446  1.42199006 -2.87967532]\n",
      " [ 3.77442754  1.49180249 -3.0543858 ]\n",
      " [-1.94609717 -0.86616058  0.38851636]\n",
      " [ 3.91596314  1.5419788  -3.23942727]\n",
      " [-1.97551454 -0.87585477  0.4359626 ]]\n",
      "\n",
      "Inhibitor attention output:\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.92758011  0.30151054 -1.54699329]\n",
      " [-1.77567688 -0.86451925 -0.55321843]\n",
      " [ 2.61997468  0.85162453 -4.36952366]]\n",
      "\n",
      "Cosine similarity between outputs:\n",
      "[        nan         nan -0.67359451 -0.59622408 -0.6873109 ]\n",
      "\n",
      "Euclidean distance between outputs:\n",
      "[4.81852998 5.07947345 3.65618306 6.73807961 6.86989113]\n",
      "\n",
      "Mean Squared Error between outputs:\n",
      "[ 7.73941039  8.60035019  4.45589152 15.13390561 15.73180138]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_702632/2502376817.py:71: RuntimeWarning: invalid value encountered in divide\n",
      "  cosine_similarity = dot_product / (norm_dot * norm_inhibitor)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "class SimpleTransformer:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, attention_type='dot_product'):\n",
    "        self.Wq = np.random.randn(input_dim, hidden_dim)\n",
    "        self.Wk = np.random.randn(input_dim, hidden_dim)\n",
    "        self.Wv = np.random.randn(input_dim, hidden_dim)\n",
    "        self.Wo = np.random.randn(hidden_dim, output_dim)\n",
    "        self.attention_type = attention_type\n",
    "\n",
    "    def dot_product_attention(self, Q, K, V):\n",
    "        attention_scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n",
    "        attention_probs = softmax(attention_scores)\n",
    "        return np.dot(attention_probs, V)\n",
    "\n",
    "    def inhibitor_attention(self, Q, K, V):\n",
    "        Z = np.abs(Q[:, np.newaxis] - K)\n",
    "        return np.maximum(V[:, np.newaxis] - Z, 0).sum(axis=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Q = np.dot(X, self.Wq)\n",
    "        K = np.dot(X, self.Wk)\n",
    "        V = np.dot(X, self.Wv)\n",
    "\n",
    "        if self.attention_type == 'dot_product':\n",
    "            attention_output = self.dot_product_attention(Q, K, V)\n",
    "        else:\n",
    "            attention_output = self.inhibitor_attention(Q, K, V)\n",
    "\n",
    "        return np.dot(attention_output, self.Wo)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 4\n",
    "hidden_dim = 2\n",
    "output_dim = 3\n",
    "seq_length = 5\n",
    "\n",
    "# Create input data\n",
    "X = np.random.randn(seq_length, input_dim)\n",
    "\n",
    "# Initialize models\n",
    "dot_product_model = SimpleTransformer(input_dim, hidden_dim, output_dim, 'dot_product')\n",
    "inhibitor_model = SimpleTransformer(input_dim, hidden_dim, output_dim, 'inhibitor')\n",
    "\n",
    "# Use the same weights for both models to ensure fair comparison\n",
    "inhibitor_model.Wq = dot_product_model.Wq\n",
    "inhibitor_model.Wk = dot_product_model.Wk\n",
    "inhibitor_model.Wv = dot_product_model.Wv\n",
    "inhibitor_model.Wo = dot_product_model.Wo\n",
    "\n",
    "# Forward pass\n",
    "dot_product_output = dot_product_model.forward(X)\n",
    "inhibitor_output = inhibitor_model.forward(X)\n",
    "\n",
    "print(\"Dot-product attention output:\")\n",
    "print(dot_product_output)\n",
    "print(\"\\nInhibitor attention output:\")\n",
    "print(inhibitor_output)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "dot_product = np.sum(dot_product_output * inhibitor_output, axis=1)\n",
    "norm_dot = np.linalg.norm(dot_product_output, axis=1)\n",
    "norm_inhibitor = np.linalg.norm(inhibitor_output, axis=1)\n",
    "cosine_similarity = dot_product / (norm_dot * norm_inhibitor)\n",
    "\n",
    "print(\"\\nCosine similarity between outputs:\")\n",
    "print(cosine_similarity)\n",
    "\n",
    "# Calculate Euclidean distance\n",
    "euclidean_distance = np.linalg.norm(dot_product_output - inhibitor_output, axis=1)\n",
    "print(\"\\nEuclidean distance between outputs:\")\n",
    "print(euclidean_distance)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = np.mean((dot_product_output - inhibitor_output)**2, axis=1)\n",
    "print(\"\\nMean Squared Error between outputs:\")\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c5830-7aab-439f-ac8b-ebaaf45ed77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
